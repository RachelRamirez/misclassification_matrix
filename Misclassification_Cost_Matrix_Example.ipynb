{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYs+KTUnofZtRtPQvDbBDd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RachelRamirez/misclassification_matrix/blob/main/Misclassification_Cost_Matrix_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that I've shown the TB and ISantaro methods are equivalent when seeded properly, I believe what I need to do is train a \"good\" neural network with the same seed 30 times to get an idea of what the typical confusion matrix looks like.  I will run the code within Colab 30 times without restarting because from previous reproducibility studies if I restart and run-all I will get the same results.  This time, I'm interested in the typical variety you can get on a confusion matrix when the neural network is seeded the same way each time, that way when I gve it another cost-matrix to train on, and run that 30 times I can do a more informative comparison of the results.  Since the Isantaro and TB methods were identical I went with the Isantaro method because it was simpler, more efficient, and seemed less time consuming. "
      ],
      "metadata": {
        "id": "xW_9TgRZB0s6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changes from previous Reproducibility notebook:\n",
        "1. Dropout is back in.\n",
        "2. Batch Size is not as large to help with variety.\n",
        "3. Num of Epochs is more than 4 now that I care about achieving good overall accuracy\n",
        "4. Callback for EarlyStop added\n",
        "5. Model Shuffle during Fit is still False (I'm calling it out to see if I need to change that)\n",
        "6. but Model.Fit(use multiprocessors = True)"
      ],
      "metadata": {
        "id": "C0WLMX5eC-Ke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reproducible Seeds"
      ],
      "metadata": {
        "id": "Wn15dbArlsIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For Reproducibility\n",
        "import numpy as np\n",
        "# np.random.seed(1337)  # for reproducibility\n",
        "\n",
        "import tensorflow as tf\n",
        "# tf.random.set_seed(33)\n",
        "\n",
        "import random as python_random\n",
        "# python_random.seed(4)\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/utils/set_random_seed\n",
        "tf.keras.utils.set_random_seed(342) #Possibly use next iteration if the above doesn't work\n",
        "\n",
        "\n",
        "# Running more than once causes variation.  try adding this:\n",
        "# Set seed value\n",
        "seed_value = 56\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "print(\"TF version: \" , tf.__version__ )\n",
        "print(\"Keras version: \" , tf.keras.__version__ )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcjDfFIIbmbo",
        "outputId": "74184103-9312-4366-9734-e0136d20790c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version:  2.9.2\n",
            "Keras version:  2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import rest of Library"
      ],
      "metadata": {
        "id": "mTW-hEgnlp44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# from https://github.com/keras-team/keras/issues/2115#issuecomment-204060456\n",
        "# witha correction on the weighted function in the middle \n",
        "\n",
        "'''Train a simple deep NN on the MNIST dataset.\n",
        "Get to 98.40% test accuracy after 20 epochs\n",
        "(there is *a lot* of margin for parameter tuning).\n",
        "2 seconds per epoch on a K520 GPU.\n",
        "'''\n",
        "\n",
        "from __future__ import print_function  #do i still need this?\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "from keras.utils import np_utils\n",
        "import keras.backend as K\n",
        "from itertools import product\n",
        "import functools\n",
        "from functools import partial\n",
        "\n",
        "from time import sleep\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd \n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "## MORE REPEATABILITY STUFF NEEDED - If theres a way to update this to V2 of Tensorflow great, otherwise I had to use TF 1.0 code\n",
        "# 5. Configure a new global `tensorflow` session (https://stackoverflow.com/questions/50659482/why-cant-i-get-reproducible-results-in-keras-even-though-i-set-the-random-seeds)\n",
        "# from keras import backend as K\n",
        "\n",
        "\n",
        "#I believe thecode below is to help things be repeatable each time different sections in my google colab notebook execute\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)"
      ],
      "metadata": {
        "id": "idfYNyyAgMsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define batch, epochs, and format data"
      ],
      "metadata": {
        "id": "otcbfKF7mY9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256 # I originally had it very  high batch size to reduce the variation in the data each batch and hope it makes the model training more nearly identical which it did, then i bring it back down to something reasonable to get better results training the NN\n",
        "nb_classes = 10\n",
        "nb_epoch = 15\n",
        "\n",
        "# the data, shuffled and split between train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "X_train = X_train.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, nb_classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B59UXDb8i8W5",
        "outputId": "189f1f61-574f-4593-a261-39b67cdb2a68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define weighted_categorical_crossentropy()"
      ],
      "metadata": {
        "id": "Y102hWcfvFyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # https://github.com/keras-team/keras/issues/2115#issuecomment-207765342\n",
        "\n",
        "# def w_categorical_crossentropy(y_true, y_pred, weights):\n",
        "#     nb_cl = len(weights)\n",
        "#     final_mask = K.zeros_like(y_pred[:, 0])\n",
        "#     y_pred_max = K.max(y_pred, axis=1)\n",
        "#     y_pred_max = K.expand_dims(y_pred_max, 1)\n",
        "#     y_pred_max_mat = K.equal(y_pred, y_pred_max)\n",
        " \n",
        "#     for c_t, c_p in product(range(nb_cl), range(nb_cl)):\n",
        "#         final_mask += (K.cast(weights[c_t, c_p],K.floatx()) * K.cast(y_pred_max_mat[:, c_p] ,K.floatx())* K.cast(y_true[:, c_t],K.floatx()))\n",
        "    \n",
        "#     # result = K.categorical_crossentropy(y_true, y_pred)*final_mask\n",
        "#     # tf.print(result, \"Show Result of CE * Final_Mask\")  #this was basically useless to display, and it showed like, 500 lines of print statements each epoch\n",
        "\n",
        "#     return K.categorical_crossentropy(y_true, y_pred)*final_mask   #I changed the order of y_true and y_pred\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fNienjOxgQVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Same Model but use normal Categorical CrossEntropy with no extra cost-matrix of Weights"
      ],
      "metadata": {
        "id": "HtojTVZBvLEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normal_method():\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Dense(512, input_shape=(784,) ,kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(512, kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(10, kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  rms = RMSprop()\n",
        "  # model.compile(loss=ncce, optimizer=rms)\n",
        "  model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=rms, metrics='categorical_accuracy', )\n",
        "\n",
        "  #add early_stop to prevent overfittings\n",
        "  # callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "  model_history = model.fit(X_train, Y_train,\n",
        "            batch_size=batch_size, epochs=nb_epoch, verbose=2,\n",
        "            validation_data=(X_test, Y_test),shuffle=False, use_multiprocessing=True\n",
        "            , callbacks = [callback])\n",
        "            )\n",
        "\n",
        "  \n",
        "  # model.evaluate(X_test, Y_test, verbose=1)  # I know this isn't the typical use of train/val/test sets, please dont' comment on that\n",
        "  \n",
        "  #Predict\n",
        "  y_prediction = model.predict(X_test)\n",
        "  y_prediction  = np.argmax(y_prediction, axis=1)\n",
        "  # Y_prediction = np_utils.to_categorical(y_prediction, nb_classes) #If I want to do SparseCategoricalAccuracy\n",
        "\n",
        "  #Create confusion matrix and normalizes it over predicted (columns)\n",
        "  # result = confusion_matrix(y_test, y_prediction , normalize='pred')  #if I want percentages instead of raw counts\n",
        "\n",
        "  \n",
        "  cm = confusion_matrix(y_test, y_prediction)\n",
        "  cm = pd.DataFrame(cm, range(10),range(10))\n",
        "\n",
        "  #This shows a pretty confusion matrix which I don't neeed to show right now\n",
        "  # plt.figure(figsize = (10,10))\n",
        "  # sns.heatmap(cm, annot=True, annot_kws={\"size\": 12}) # font size\n",
        "  # plt.show()\n",
        "  # cm_normal = cm\n",
        "\n",
        "  return cm\n"
      ],
      "metadata": {
        "id": "InYvpv3kaCxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cw2zBqpvvzi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weighted Categorical Cross Entropy Function"
      ],
      "metadata": {
        "id": "3fHQHrz8MwXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from https://github.com/keras-team/keras/issues/2115#issuecomment-815825633 from Isaranto\n",
        "\n",
        "def weighted_categorical_crossentropy_new(y_true, y_pred, weights):\n",
        "          idx1 = K.argmax(y_pred, axis=1)\n",
        "          idx2 = K.argmax(y_true, axis=1)\n",
        "          mask = tf.gather_nd(weights, tf.stack((idx1, idx2), -1))\n",
        "          return K.categorical_crossentropy(y_true, y_pred) * mask"
      ],
      "metadata": {
        "id": "pUR1sLQ7MvVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #what does ncce stand for?\n",
        "\n",
        " \n",
        "def isaranto_method():\n",
        "  w_array = np.ones((10,10))\n",
        "  # w_array[9, 7] = 1.5\n",
        "  # w_array = w_array - np.eye(10)\n",
        "  # print(\"W_array:  \", w_array)\n",
        "\n",
        "  weighted_list = w_array.tolist()\n",
        "\n",
        "  wcce = partial(weighted_categorical_crossentropy_new, weights=weighted_list)\n",
        "  wcce.__name__ ='w_categorical_crossentropy'\n",
        "\n",
        "  model3 = Sequential()\n",
        "  model3.add(Dense(512, input_shape=(784,), kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "  model3.add(Activation('relu'))\n",
        "  model3.add(Dropout(0.2))\n",
        "  model3.add(Dense(512, kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "  model3.add(Activation('relu'))\n",
        "  model3.add(Dropout(0.2))\n",
        "  model3.add(Dense(10,kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "  model3.add(Activation('softmax'))\n",
        "\n",
        "  rms = RMSprop()\n",
        "\n",
        "  model3.compile(loss=wcce, optimizer=rms,  metrics='categorical_accuracy',)\n",
        "  callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "  model3_history = model3.fit(X_train, Y_train,\n",
        "            batch_size=batch_size, epochs=nb_epoch, verbose=2,\n",
        "            validation_data=(X_test, Y_test), shuffle=False, use_multiprocessing=True\n",
        "            ,callbacks = [callback]\n",
        "            )\n",
        "\n",
        " \n",
        "\n",
        "  #Predict\n",
        "  y_prediction = model3.predict(X_test)\n",
        "  y_prediction  = np.argmax(y_prediction, axis=1)\n",
        "  # Y_prediction = np_utils.to_categorical(y_prediction, nb_classes)\n",
        "\n",
        "  #Create confusion matrix and normalizes it over predicted (columns)\n",
        "  # result = confusion_matrix(y_test, y_prediction , normalize='pred')\n",
        "\n",
        "  \n",
        "\n",
        "  cm3 = confusion_matrix(y_test, y_prediction)\n",
        "  cm3 = pd.DataFrame(cm3, range(10),range(10))\n",
        "  # plt.figure(figsize = (10,10))\n",
        "  # cm3\n",
        "  # sns.heatmap(cm2, annot=True, annot_kws={\"size\": 12}) # font size\n",
        "  # plt.show()\n",
        "\n",
        "  # cm_using_weighted_new = cm3\n",
        "\n",
        "  return cm3"
      ],
      "metadata": {
        "id": "3UWVdmRHNBhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cm_using_weighted_new "
      ],
      "metadata": {
        "id": "OSq7jMYUOF4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cm_normal"
      ],
      "metadata": {
        "id": "auDUR-MjOIZ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}