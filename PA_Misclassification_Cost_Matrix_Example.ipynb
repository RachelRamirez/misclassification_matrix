{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+HhYqDOWUF1NGMWwlBa+Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RachelRamirez/misclassification_matrix/blob/main/PA_Misclassification_Cost_Matrix_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What happens when I use PA method with Weighted_Matrix with w_array[7, 9] = 2"
      ],
      "metadata": {
        "id": "NqL4ADhFLsqG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results of running the  ISantaro appeared counterintuitive, because when I increased the cost of a misclassification, more misclassifications were made. It was hard to see at first because it wasn't consistently happening, it happened 7 out of 30 times, but when it happened it was a very large number of misclassifications.   So now I'm looking to compare another code implementation,  by Phil Alton here:  https://stackoverflow.com/a/61963004 "
      ],
      "metadata": {
        "id": "xW_9TgRZB0s6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember to change the [Admin File stuff]  below and the Weight Matrix before Running"
      ],
      "metadata": {
        "id": "HhaPibIBBS80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reproducible Seeds"
      ],
      "metadata": {
        "id": "Wn15dbArlsIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For Reproducibility\n",
        "import numpy as np\n",
        "# np.random.seed(1337)  # for reproducibility\n",
        "\n",
        "import tensorflow as tf\n",
        "# tf.random.set_seed(33)\n",
        "\n",
        "import random as python_random\n",
        "# python_random.seed(4)\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/utils/set_random_seed\n",
        "tf.keras.utils.set_random_seed(342) #Possibly use next iteration if the above doesn't work\n",
        "\n",
        "\n",
        "# Running more than once causes variation.  try adding this:\n",
        "# Set seed value\n",
        "seed_value = 56\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "print(\"TF version: \" , tf.__version__ )\n",
        "print(\"Keras version: \" , tf.keras.__version__ )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcjDfFIIbmbo",
        "outputId": "5fe9ebda-2986-4d45-dd77-23973f8e8dfb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version:  2.11.0\n",
            "Keras version:  2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import rest of Library"
      ],
      "metadata": {
        "id": "mTW-hEgnlp44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# from https://github.com/keras-team/keras/issues/2115#issuecomment-204060456\n",
        "# witha correction on the weighted function in the middle \n",
        "\n",
        "'''Train a simple deep NN on the MNIST dataset.\n",
        "Get to 98.40% test accuracy after 20 epochs\n",
        "(there is *a lot* of margin for parameter tuning).\n",
        "2 seconds per epoch on a K520 GPU.\n",
        "'''\n",
        "\n",
        "from __future__ import print_function  #do i still need this?\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "from keras.utils import np_utils\n",
        "import keras.backend as K\n",
        "from itertools import product\n",
        "import functools\n",
        "from functools import partial\n",
        "from time import ctime\n",
        "from time import sleep\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd \n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "## MORE REPEATABILITY STUFF NEEDED - If theres a way to update this to V2 of Tensorflow great, otherwise I had to use TF 1.0 code\n",
        "# 5. Configure a new global `tensorflow` session (https://stackoverflow.com/questions/50659482/why-cant-i-get-reproducible-results-in-keras-even-though-i-set-the-random-seeds)\n",
        "# from keras import backend as K\n",
        "\n",
        "\n",
        "#I believe thecode below is to help things be repeatable each time different sections in my google colab notebook execute\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)"
      ],
      "metadata": {
        "id": "idfYNyyAgMsO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define batch, epochs, and format data"
      ],
      "metadata": {
        "id": "otcbfKF7mY9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256 # I originally had it very  high batch size to reduce the variation in the data each batch and hope it makes the model training more nearly identical which it did, then i bring it back down to something reasonable to get better results training the NN\n",
        "nb_classes = 10\n",
        "nb_epoch = 15\n",
        "\n",
        "# the data, shuffled and split between train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "X_train = X_train.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, nb_classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B59UXDb8i8W5",
        "outputId": "9dfd3b99-25b9-4a93-e385-f30b5f92fa22"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Same Model but use normal Categorical CrossEntropy with no extra cost-matrix of Weights"
      ],
      "metadata": {
        "id": "HtojTVZBvLEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def normal_method():\n",
        "\n",
        "#   model = Sequential()\n",
        "#   model.add(Dense(512, input_shape=(784,) ,kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "#   model.add(Activation('relu'))\n",
        "#   model.add(Dropout(0.2))\n",
        "#   model.add(Dense(512, kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "#   model.add(Activation('relu'))\n",
        "#   model.add(Dropout(0.2))\n",
        "#   model.add(Dense(10, kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "#   model.add(Activation('softmax'))\n",
        "\n",
        "#   rms = RMSprop()\n",
        "#   # model.compile(loss=ncce, optimizer=rms)\n",
        "#   model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=rms, metrics='categorical_accuracy', )\n",
        "\n",
        "#   #add early_stop to prevent overfittings\n",
        "#   # callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "#   model_history = model.fit(X_train, Y_train,\n",
        "#             batch_size=batch_size, epochs=nb_epoch, verbose=2,\n",
        "#             validation_data=(X_test, Y_test),shuffle=False, use_multiprocessing=True\n",
        "#             , callbacks = [callback])\n",
        "\n",
        "  \n",
        "#   # model.evaluate(X_test, Y_test, verbose=1)  # I know this isn't the typical use of train/val/test sets, please dont' comment on that\n",
        "  \n",
        "#   #Predict\n",
        "#   y_prediction = model.predict(X_test)\n",
        "#   y_prediction  = np.argmax(y_prediction, axis=1)\n",
        "#   # Y_prediction = np_utils.to_categorical(y_prediction, nb_classes) #If I want to do SparseCategoricalAccuracy\n",
        "\n",
        "#   #Create confusion matrix and normalizes it over predicted (columns)\n",
        "#   # result = confusion_matrix(y_test, y_prediction , normalize='pred')  #if I want percentages instead of raw counts\n",
        "\n",
        "  \n",
        "#   cm = confusion_matrix(y_test, y_prediction)\n",
        "#   cm = pd.DataFrame(cm, range(10),range(10))\n",
        "\n",
        "#   #This shows a pretty confusion matrix which I don't neeed to show right now\n",
        "#   # plt.figure(figsize = (10,10))\n",
        "#   # sns.heatmap(cm, annot=True, annot_kws={\"size\": 12}) # font size\n",
        "#   # plt.show()\n",
        "#   # cm_normal = cm\n",
        "\n",
        "#   return cm\n"
      ],
      "metadata": {
        "id": "InYvpv3kaCxb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cw2zBqpvvzi0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weighted Categorical Cross Entropy Class"
      ],
      "metadata": {
        "id": "3fHQHrz8MwXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightedCategoricalCrossentropy(tf.keras.losses.CategoricalCrossentropy):\n",
        "\n",
        "  def __init__(self, cost_mat, name='weighted_categorical_crossentropy', **kwargs):\n",
        "\n",
        "    cost_mat = np.array(cost_mat)   \n",
        "    ## when loading from config, self.cost_mat returns as a list, rather than an numpy array. \n",
        "    ## Adding the above line fixes this issue, enabling .ndim to call sucessfully. \n",
        "    ## However, this is probably not the best implementation\n",
        "    assert(cost_mat.ndim == 2)\n",
        "    assert(cost_mat.shape[0] == cost_mat.shape[1])\n",
        "    super().__init__(name=name, **kwargs)\n",
        "    self.cost_mat = K.cast_to_floatx(cost_mat)\n",
        "\n",
        "  def __call__(self, y_true, y_pred, sample_weight=None):\n",
        "    assert sample_weight is None, \"should only be derived from the cost matrix\"  \n",
        "    return super().__call__(\n",
        "        y_true=y_true, \n",
        "        y_pred=y_pred, \n",
        "        sample_weight=get_sample_weights(y_true, y_pred, self.cost_mat),\n",
        "    )\n",
        "\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config().copy()\n",
        "    # Calling .update on the line above, during assignment, causes an error with config becoming None-type.\n",
        "    config.update({'cost_mat': (self.cost_mat)})\n",
        "    return config\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "    # something goes wrong here and changes self.cost_mat to a list variable.\n",
        "    # See above for temporary fix\n",
        "    return cls(**config)\n",
        "\n",
        "def get_sample_weights(y_true, y_pred, cost_m):\n",
        "    num_classes = len(cost_m)\n",
        "\n",
        "    y_pred.shape.assert_has_rank(2)\n",
        "    assert(y_pred.shape[1] == num_classes)\n",
        "    y_pred.shape.assert_is_compatible_with(y_true.shape)\n",
        "\n",
        "    y_pred = K.one_hot(K.argmax(y_pred), num_classes)\n",
        "\n",
        "    y_true_nk1 = K.expand_dims(y_true, 2)\n",
        "    y_pred_n1k = K.expand_dims(y_pred, 1)\n",
        "    cost_m_1kk = K.expand_dims(cost_m, 0)\n",
        "\n",
        "    sample_weights_nkk = cost_m_1kk * y_true_nk1 * y_pred_n1k\n",
        "    sample_weights_n = K.sum(sample_weights_nkk, axis=[1, 2])\n",
        "\n",
        "    return sample_weights_n\n",
        "\n",
        "\n",
        "# Register the loss in the Keras namespace to enable loading of the custom object.\n",
        "tf.keras.losses.WeightedCategoricalCrossentropy = WeightedCategoricalCrossentropy\n",
        " "
      ],
      "metadata": {
        "id": "pUR1sLQ7MvVa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WeightedCategoricalCross Entropy Function "
      ],
      "metadata": {
        "id": "-uJmU0t4ANuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def PA_method(cost_matrix):\n",
        "\n",
        "  model3 = Sequential()\n",
        "  model3.add(Dense(512, input_shape=(784,), kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "  model3.add(Activation('relu'))\n",
        "  model3.add(Dropout(0.2))\n",
        "  model3.add(Dense(512, kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "  model3.add(Activation('relu'))\n",
        "  model3.add(Dropout(0.2))\n",
        "  model3.add(Dense(10,kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "  model3.add(Activation('softmax'))\n",
        "\n",
        "  rms = RMSprop()\n",
        "\n",
        "  model3.compile(loss=WeightedCategoricalCrossentropy(cost_matrix), optimizer=rms,  metrics='categorical_accuracy',)\n",
        "  callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "  model3_history = model3.fit(X_train, Y_train,\n",
        "            batch_size=batch_size, epochs=nb_epoch, verbose=2,\n",
        "            validation_data=(X_test, Y_test), shuffle=False, use_multiprocessing=True\n",
        "            ,callbacks = [callback]\n",
        "            )\n",
        "\n",
        " \n",
        "\n",
        "  #Predict\n",
        "  y_prediction = model3.predict(X_test)\n",
        "  y_prediction  = np.argmax(y_prediction, axis=1)\n",
        "  # Y_prediction = np_utils.to_categorical(y_prediction, nb_classes)\n",
        "\n",
        "  #Create confusion matrix and normalizes it over predicted (columns)\n",
        "  # result = confusion_matrix(y_test, y_prediction , normalize='pred')\n",
        "\n",
        "  \n",
        "\n",
        "  cm3 = confusion_matrix(y_test, y_prediction)\n",
        "  cm3 = pd.DataFrame(cm3, range(10),range(10))\n",
        "  # plt.figure(figsize = (10,10))\n",
        "  # cm3\n",
        "  # sns.heatmap(cm2, annot=True, annot_kws={\"size\": 12}) # font size\n",
        "  # plt.show()\n",
        "\n",
        "  # cm_using_weighted_new = cm3\n",
        "\n",
        "  return cm3"
      ],
      "metadata": {
        "id": "3UWVdmRHNBhP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2pvUSOXZNRfs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Keep Track of Experimental Admin Stuff - #Runs and #CostMatrix\n",
        "\n",
        "> Change the cost matrix and number of runs and check the file extension name \n"
      ],
      "metadata": {
        "id": "skXIN6S4npiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Define Cost Matrix and Method"
      ],
      "metadata": {
        "id": "q9YhLRi4NU2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experimental Admin Stuff\n",
        "cost_matrix = np.ones((10,10))\n",
        "\n",
        "### Weight of Misclassification\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "cost_matrix[7, 9] = 2\n",
        "cost_str = str(cost_matrix[7, 9])\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "from datetime import date\n",
        "today = date.today()\n",
        "file_date = today.strftime(\"%Y_%m_%d\")\n",
        "\n",
        "### File Extension to reference in JMP : weights_method_cost\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "file_extension = \"w[7,9]_PA_\" + cost_str\n",
        "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "\n",
        "np.set_printoptions(suppress=True)\n",
        "np.set_printoptions(precision=2)"
      ],
      "metadata": {
        "id": "3bMXTRRBnn3E"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run Experiments"
      ],
      "metadata": {
        "id": "x_EdEdJwOpvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = np.zeros([10,10])\n",
        "combined_cms = np.empty((1,100))\n",
        "\n",
        "## Define the total number of runs\n",
        "### ~~~~~~~~~\n",
        "runs = 30\n",
        "### ~~~~~~~~~~\n",
        "\n",
        "for i in range(0,runs+1):\n",
        "  print(i)\n",
        "  cm2 =  PA_method(cost_matrix)    #Individual CM\n",
        "  print(\"CM: \\n\", cm2)\n",
        "  # cm += cm2                   #Aggregating for an Average\n",
        "  cm2_array = np.asarray(cm2)  #Indiv CM as array for storing\n",
        "  combined_cms = np.vstack((combined_cms,cm2_array.reshape((1,100))))\n",
        "\n",
        "# cm_new = cm/30"
      ],
      "metadata": {
        "id": "OSq7jMYUOF4t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eab7579-b35a-49e0-c7cf-f7f97f9f9bca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch 1/15\n",
            "235/235 - 9s - loss: 0.3090 - categorical_accuracy: 0.9068 - val_loss: 0.2528 - val_categorical_accuracy: 0.9374 - 9s/epoch - 40ms/step\n",
            "Epoch 2/15\n",
            "235/235 - 1s - loss: 0.1196 - categorical_accuracy: 0.9648 - val_loss: 0.1690 - val_categorical_accuracy: 0.9567 - 1s/epoch - 6ms/step\n",
            "Epoch 3/15\n",
            "235/235 - 1s - loss: 0.0821 - categorical_accuracy: 0.9758 - val_loss: 0.0961 - val_categorical_accuracy: 0.9726 - 1s/epoch - 4ms/step\n",
            "Epoch 4/15\n",
            "235/235 - 1s - loss: 0.0630 - categorical_accuracy: 0.9809 - val_loss: 0.0914 - val_categorical_accuracy: 0.9728 - 959ms/epoch - 4ms/step\n",
            "Epoch 5/15\n",
            "235/235 - 1s - loss: 0.0486 - categorical_accuracy: 0.9855 - val_loss: 0.0814 - val_categorical_accuracy: 0.9786 - 922ms/epoch - 4ms/step\n",
            "Epoch 6/15\n",
            "235/235 - 1s - loss: 0.0395 - categorical_accuracy: 0.9876 - val_loss: 0.0779 - val_categorical_accuracy: 0.9805 - 930ms/epoch - 4ms/step\n",
            "Epoch 7/15\n",
            "235/235 - 1s - loss: 0.0350 - categorical_accuracy: 0.9896 - val_loss: 0.0834 - val_categorical_accuracy: 0.9800 - 923ms/epoch - 4ms/step\n",
            "Epoch 8/15\n",
            "235/235 - 1s - loss: 0.0278 - categorical_accuracy: 0.9912 - val_loss: 0.1007 - val_categorical_accuracy: 0.9767 - 872ms/epoch - 4ms/step\n",
            "Epoch 9/15\n",
            "235/235 - 1s - loss: 0.0258 - categorical_accuracy: 0.9918 - val_loss: 0.0862 - val_categorical_accuracy: 0.9798 - 890ms/epoch - 4ms/step\n",
            "313/313 [==============================] - 2s 5ms/step\n",
            "CM: \n",
            "      0     1     2    3    4    5    6    7    8    9\n",
            "0  974     1     0    0    0    1    1    0    2    1\n",
            "1    0  1129     1    1    0    0    2    0    2    0\n",
            "2    1     4  1019    2    1    0    1    1    3    0\n",
            "3    0     0     9  995    0    1    0    2    2    1\n",
            "4    2     1     3    0  966    0    3    1    0    6\n",
            "5    2     0     0   14    1  869    2    0    3    1\n",
            "6    3     3     1    1    3   10  936    0    1    0\n",
            "7    1     6    13    2    8    0    0  983    6    9\n",
            "8    2     1     5    5    4    6    0    2  944    5\n",
            "9    1     2     0    6   13    2    1    1    0  983\n",
            "1\n",
            "Epoch 1/15\n",
            "235/235 - 3s - loss: 0.3090 - categorical_accuracy: 0.9069 - val_loss: 0.2794 - val_categorical_accuracy: 0.9333 - 3s/epoch - 13ms/step\n",
            "Epoch 2/15\n",
            "235/235 - 1s - loss: 0.1201 - categorical_accuracy: 0.9643 - val_loss: 0.1700 - val_categorical_accuracy: 0.9571 - 950ms/epoch - 4ms/step\n",
            "Epoch 3/15\n",
            "235/235 - 1s - loss: 0.0824 - categorical_accuracy: 0.9756 - val_loss: 0.0909 - val_categorical_accuracy: 0.9748 - 922ms/epoch - 4ms/step\n",
            "Epoch 4/15\n",
            "235/235 - 1s - loss: 0.0632 - categorical_accuracy: 0.9810 - val_loss: 0.0901 - val_categorical_accuracy: 0.9740 - 1s/epoch - 5ms/step\n",
            "Epoch 5/15\n",
            "235/235 - 2s - loss: 0.0486 - categorical_accuracy: 0.9853 - val_loss: 0.0842 - val_categorical_accuracy: 0.9769 - 2s/epoch - 8ms/step\n",
            "Epoch 6/15\n",
            "235/235 - 1s - loss: 0.0395 - categorical_accuracy: 0.9876 - val_loss: 0.0769 - val_categorical_accuracy: 0.9790 - 1s/epoch - 6ms/step\n",
            "Epoch 7/15\n",
            "235/235 - 1s - loss: 0.0354 - categorical_accuracy: 0.9895 - val_loss: 0.0872 - val_categorical_accuracy: 0.9788 - 1s/epoch - 6ms/step\n",
            "Epoch 8/15\n",
            "235/235 - 2s - loss: 0.0289 - categorical_accuracy: 0.9901 - val_loss: 0.0942 - val_categorical_accuracy: 0.9766 - 2s/epoch - 8ms/step\n",
            "Epoch 9/15\n",
            "235/235 - 2s - loss: 0.0258 - categorical_accuracy: 0.9919 - val_loss: 0.0797 - val_categorical_accuracy: 0.9815 - 2s/epoch - 7ms/step\n",
            "313/313 [==============================] - 5s 15ms/step\n",
            "CM: \n",
            "      0     1     2    3    4    5    6    7    8    9\n",
            "0  971     1     0    1    1    0    1    1    3    1\n",
            "1    0  1131     1    0    0    0    1    0    2    0\n",
            "2    1     3  1015    1    2    0    2    3    5    0\n",
            "3    0     0     7  992    0    3    0    3    3    2\n",
            "4    1     0     3    0  964    0    3    0    3    8\n",
            "5    2     0     0    3    2  878    2    0    4    1\n",
            "6    2     3     0    1    5    7  939    0    1    0\n",
            "7    1     4    11    1    8    0    0  985    9    9\n",
            "8    1     1     2    3    3    7    1    1  951    4\n",
            "9    1     2     0    2   10    1    1    3    0  989\n",
            "2\n",
            "Epoch 1/15\n",
            "235/235 - 2s - loss: 0.3089 - categorical_accuracy: 0.9064 - val_loss: 0.2482 - val_categorical_accuracy: 0.9382 - 2s/epoch - 8ms/step\n",
            "Epoch 2/15\n",
            "235/235 - 1s - loss: 0.1200 - categorical_accuracy: 0.9645 - val_loss: 0.1613 - val_categorical_accuracy: 0.9580 - 916ms/epoch - 4ms/step\n",
            "Epoch 3/15\n",
            "235/235 - 1s - loss: 0.0824 - categorical_accuracy: 0.9755 - val_loss: 0.0990 - val_categorical_accuracy: 0.9734 - 883ms/epoch - 4ms/step\n",
            "Epoch 4/15\n",
            "235/235 - 1s - loss: 0.0622 - categorical_accuracy: 0.9816 - val_loss: 0.0938 - val_categorical_accuracy: 0.9738 - 1s/epoch - 5ms/step\n",
            "Epoch 5/15\n",
            "235/235 - 1s - loss: 0.0485 - categorical_accuracy: 0.9852 - val_loss: 0.0860 - val_categorical_accuracy: 0.9772 - 1s/epoch - 6ms/step\n",
            "Epoch 6/15\n",
            "235/235 - 2s - loss: 0.0401 - categorical_accuracy: 0.9873 - val_loss: 0.0784 - val_categorical_accuracy: 0.9791 - 2s/epoch - 7ms/step\n",
            "Epoch 7/15\n",
            "235/235 - 1s - loss: 0.0357 - categorical_accuracy: 0.9893 - val_loss: 0.0826 - val_categorical_accuracy: 0.9788 - 878ms/epoch - 4ms/step\n",
            "Epoch 8/15\n",
            "235/235 - 1s - loss: 0.0281 - categorical_accuracy: 0.9908 - val_loss: 0.1008 - val_categorical_accuracy: 0.9751 - 869ms/epoch - 4ms/step\n",
            "Epoch 9/15\n",
            "235/235 - 1s - loss: 0.0263 - categorical_accuracy: 0.9917 - val_loss: 0.0868 - val_categorical_accuracy: 0.9796 - 906ms/epoch - 4ms/step\n",
            "313/313 [==============================] - 2s 5ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save the 30/X confusion matrices"
      ],
      "metadata": {
        "id": "u-MSCXKC48ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Save 30 confusion matrices\n",
        "\n",
        "import pickle\n",
        "\n",
        "str_runs = str(runs)\n",
        "\n",
        "file_name = str_runs + \"CM_\" + file_extension + \"_\" + file_date + \"_.pkl\"\n",
        "print(file_name, \" will be saved with \", combined_cms.shape)\n",
        "\n",
        "with open(file_name, 'wb') as file:\n",
        "      \n",
        "    # A new file will be created\n",
        "    pickle.dump(combined_cms, file)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Open the file in binary mode\n",
        "with open(file_name, 'rb') as file:\n",
        "      \n",
        "    # Call load method to deserialze\n",
        "    var = pickle.load(file)\n",
        "  \n",
        "    print(var)\n",
        "\n",
        "print(file_name, \" was opened with \", var.shape)\n",
        "\n",
        "from google.colab import files\n",
        "files.download( file_name )  \n",
        "\n",
        "print(file_name, \" was saved to Downloads \")\n"
      ],
      "metadata": {
        "id": "9purX_onqXGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many categories are there in the test set?\n",
        "\n",
        "truth_num_per_category = Y_test.sum(axis=0)\n",
        "print(truth_num_per_category)"
      ],
      "metadata": {
        "id": "id_ythTutuo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyze \n",
        "\n",
        "I am now going to load the Average CM and try to get it in a format where I can make it a 1x100 and load all 30 CMs so that we can visualize their distributions in a a big histogram_matrix. At this point the Google Colab variables are gone and I have to reoad them "
      ],
      "metadata": {
        "id": "NP_kxNkhn6Fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I need to remove the first placeholder row of zeros\n",
        "combined_cms = combined_cms[1:(runs+1)]"
      ],
      "metadata": {
        "id": "Z7qMXMuN4kS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "empty_cm = np.zeros((10,10))  \n",
        "empty_cm=pd.DataFrame(empty_cm)\n",
        "\n",
        "empty_cm.columns = ['0p', '1p', '2p', '3p', '4p', '5p', '6p', '7p', '8p', '9p']\n",
        "empty_cm.index = ['0t', '1t', '2t', '3t', '4t', '5t', '6t', '7t', '8t', '9t']\n",
        "\n",
        "# print(myvar_cm_average)\n",
        "\n",
        "empty_cm_array = np.asarray(empty_cm)\n",
        "empty_cm_array_1_100 = np.reshape(empty_cm_array,(1,100))\n",
        "# print(cm_average_array)\n",
        "\n",
        "df = empty_cm\n",
        "df_new = pd.DataFrame(empty_cm_array_1_100,  columns=pd.MultiIndex.from_product([ df.index,df.columns]))\n",
        "df_new.columns.to_flat_index()\n",
        "df_new.columns   = ['_'.join(col) for col in df_new.columns.values]\n",
        "\n",
        "# Now convert combined_cms of size 30x100 to a panda dataframe\n",
        "combined_cms_df = pd.DataFrame(combined_cms, columns=[df_new.columns])\n",
        "\n",
        "combined_cms_df"
      ],
      "metadata": {
        "id": "kLrNJE0s53pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_filename = file_name[:-4] + \".csv\"\n",
        "\n",
        "combined_cms_df.to_csv(csv_filename)\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "files.download(csv_filename )\n",
        "\n",
        "print(\"Downloading \", csv_filename , \" of shape \", combined_cms_df.shape)"
      ],
      "metadata": {
        "id": "ceUFAr_z9xsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(sum(var)/len(var), columns=[\"Values\"]) \n",
        "# print(df)\n",
        "df_sorted = df.sort_values(by=\"Values\", ascending=False)[10:]  #the top 10 are usually diagonal\n",
        "\n",
        "\n",
        "df_sorted.style.format({\n",
        "  'Values': lambda val: f'{val:,.2f}',\n",
        "})\n",
        "\n",
        "print(\"On average...\")\n",
        "print(\"Num 1 misclassifications are misclassifying a 9 as a 4 (\",(df_sorted[\"Values\"].index[0]), \") \", (df_sorted[\"Values\"].values[0]), \" times\" )\n",
        "print(\"Num 2 misclassifications are misclassifying a 6 as a 5 (\", (df_sorted[\"Values\"].index[1]), \") \", (df_sorted[\"Values\"].values[1]), \" times\" )\n",
        "print(\"Num 3 misclassifications are misclassifying a 7 as a 9 (\",(df_sorted[\"Values\"].index[2]), \") \", (df_sorted[\"Values\"].values[2]), \" times\" )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-pGNLuE8gNrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_percents = pd.DataFrame( ((sum(var)*100/len(var)).reshape((10,10))/truth_num_per_category).reshape((100)), columns = [\"Values\"])\n",
        "\n",
        "\n",
        "df_sorted_percents = df_percents.sort_values(by=\"Values\", ascending=False)[10:]  #the top 10 are usually diagonal\n",
        "\n",
        "\n",
        "df_sorted_percents.style.format({\n",
        "  'Values': lambda val: f'{val:,.2f}',\n",
        "})\n",
        "\n",
        "print(\"On average .. \")\n",
        "print(\"Num 1 percent misclassifications are 6 as a 5  (\",(df_sorted_percents[\"Values\"].index[0]), \") \", (df_sorted_percents[\"Values\"].values[0]), \" percent\" )\n",
        "print(\"Num 2 percent misclassifications are 9 as a 4 (\", (df_sorted_percents[\"Values\"].index[1]), \") \", (df_sorted_percents[\"Values\"].values[1]), \" percent\" )\n",
        "print(\"Num 3 percent misclassifications are 7 as a 9 (\",(df_sorted_percents[\"Values\"].index[2]), \") \", (df_sorted_percents[\"Values\"].values[2]), \" percent\" )\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AKYclir2p8wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FrT9iz3dp8rZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extraneous"
      ],
      "metadata": {
        "id": "GZfPCKu_oqrZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To reference later: \n",
        "\n",
        "https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/structured_data/imbalanced_data.ipynb#scrollTo=UJ589fn8ST3x\n",
        "\n",
        "To train a model with class weights:\n",
        "\n",
        "```\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "weighted_model = make_model()\n",
        "weighted_model.load_weights(initial_weights)\n",
        "\n",
        "weighted_history = weighted_model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[early_stopping],\n",
        "    validation_data=(val_features, val_labels),\n",
        "\n",
        "    # The class weights go here\n",
        "    class_weight=class_weight)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "iLEt0OL5ziEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraneous information I am not using at the moment\n",
        "\n",
        "# model.compile(\n",
        "#      optimizer='adam',\n",
        "#      loss=WeightedCategoricalCrossentropy(cost_matrix)\n",
        "#      )\n",
        "\n",
        "## Model Saving\n",
        "\n",
        "# model.save(save_version_dir,save_format='tf')\n",
        "\n",
        "## Model Loading\n",
        "\n",
        "# model = tf.keras.models.load_model(\n",
        "#     save_version_dir,\n",
        "#     compile=True,\n",
        "#     custom_objects={\n",
        "#         'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_matrix)\n",
        "#         }\n",
        "#     )\n",
        " "
      ],
      "metadata": {
        "id": "WRzuOuZeKVpJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}