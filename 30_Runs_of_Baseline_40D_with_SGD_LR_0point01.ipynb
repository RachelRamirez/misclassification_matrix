{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RachelRamirez/misclassification_matrix/blob/main/30_Runs_of_Baseline_40D_with_SGD_LR_0point01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How to change the local time in Google Colab\n",
        "!rm /etc/localtime\n",
        "!ln -s /usr/share/zoneinfo/US/Eastern /etc/localtime\n",
        "!date\n",
        "\n",
        "#If this doesn't show the local time correctly, then you need to restart.\n",
        "import time\n",
        "time.localtime(time.time())"
      ],
      "metadata": {
        "id": "7IIxYa87toxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E_rggtG4FpXO"
      },
      "source": [
        "We'll start by writing a `.py` file which we'll import."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "j2bDdei9Dxzw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "252fda1f-7901-415b-d9b7-3bfbf19ff97d"
      },
      "source": [
        "%%writefile example.py\n",
        "def f():\n",
        "  print ('This is a function defined in a Python source file.')"
      ],
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting example.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ImportLibraries_DefineFunctions.py\n",
        "import time\n",
        "time.localtime(time.time())\n",
        "#For Reproducibility\n",
        "import numpy as np\n",
        "# np.random.seed(1337)  # for reproducibility\n",
        "\n",
        "import tensorflow as tf\n",
        "# tf.random.set_seed(33)\n",
        "\n",
        "import random as python_random\n",
        "# python_random.seed(4)\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/utils/set_random_seed\n",
        "seed = 342\n",
        "tf.keras.utils.set_random_seed(seed) #Possibly use next iteration if the above doesn't work   #This makes everything VERY DETERMINISTIC\n",
        "\n",
        "\n",
        "# Running more than once causes variation.  try adding this:\n",
        "# Set seed value\n",
        "seed_value = 56\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "print(\"TF version: \" , tf.__version__ )\n",
        "print(\"Keras version: \" , tf.keras.__version__ )\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "# from __future__ import print_function  #do i still need this?\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "from keras.utils import np_utils\n",
        "import keras.backend as K\n",
        "from itertools import product\n",
        "import functools\n",
        "from functools import partial\n",
        "from time import ctime\n",
        "from time import sleep\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd \n",
        "from sklearn.metrics import confusion_matrix, classification_report   #CLASSIFICATION REPORT IS FOR F1 SCORE AND BREAKOUT OF AL CATEGORIES ACCURACYS\n",
        "from  sklearn.utils import shuffle\n",
        "from sklearn.metrics import f1_score\n",
        " \n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)\n",
        "\n",
        "print(\"Finished Loading Libraries\")\n",
        "\n",
        "\n",
        "batch_size = 256 \n",
        "\n",
        "# I originally had it very  high batch size to reduce the variation in the data each batch and hope \n",
        "# it makes the model training more nearly identical which it did, then i bring it back down to something reasonable to get better results training the NN\n",
        "\n",
        "nb_classes = 10\n",
        "\n",
        "# the data, shuffled and split between train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "X_train = X_train.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "# print(X_train.shape[0], 'train samples')\n",
        "# print(X_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "#Create a Validation Set\n",
        "X_val = X_test[:7500]   #take the first 7500 for validation\n",
        "Y_val = Y_test[:7500]   #Take the first 7500 for validation\n",
        "y_val = y_test[:7500]\n",
        "\n",
        "X_test = X_test[7500:]  #Keep the last 2500 for test/holdout\n",
        "Y_test = Y_test[7500:]  #Keep the last 2500 for test/holdout\n",
        "y_test = y_test[7500:]\n",
        "\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_val.shape[0], 'validation samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "#Are the sets relatively balanced? Yes each category is between 8% and 11% per category\n",
        "print('Train', Y_train.sum(axis=0)/X_train.shape[0])\n",
        "print('Train # of 9s', Y_train.sum(axis=0)[9])\n",
        "print('Train # of 4s', Y_train.sum(axis=0)[4])\n",
        "\n",
        "print('Val', Y_val.sum(axis=0)/X_val.shape[0])\n",
        "print('Val # of 9s', Y_val.sum(axis=0)[9])\n",
        "print('Val # of 4s', Y_val.sum(axis=0)[4])\n",
        "\n",
        "print('Test', Y_test.sum(axis=0)/X_test.shape[0])\n",
        "print('Test  # of 9s', Y_test.sum(axis=0)[9])\n",
        "print('Test  # of 4s', Y_test.sum(axis=0)[4])\n",
        "\n",
        "#@title\n",
        "class WeightedCategoricalCrossentropy(tf.keras.losses.CategoricalCrossentropy):\n",
        "\n",
        "  def __init__(self, cost_mat, name='weighted_categorical_crossentropy', **kwargs):\n",
        "\n",
        "    cost_mat = np.array(cost_mat)   \n",
        "    ## when loading from config, self.cost_mat returns as a list, rather than an numpy array. \n",
        "    ## Adding the above line fixes this issue, enabling .ndim to call sucessfully. \n",
        "    ## However, this is probably not the best implementation\n",
        "    assert(cost_mat.ndim == 2)\n",
        "    assert(cost_mat.shape[0] == cost_mat.shape[1])\n",
        "    super().__init__(name=name, **kwargs)\n",
        "    self.cost_mat = K.cast_to_floatx(cost_mat)\n",
        "\n",
        "  def __call__(self, y_true, y_pred, sample_weight=None):\n",
        "    assert sample_weight is None, \"should only be derived from the cost matrix\"  \n",
        "    return super().__call__(\n",
        "        y_true=y_true, \n",
        "        y_pred=y_pred, \n",
        "        sample_weight=get_sample_weights(y_true, y_pred, self.cost_mat),\n",
        "    )\n",
        "\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config().copy()\n",
        "    # Calling .update on the line above, during assignment, causes an error with config becoming None-type.\n",
        "    config.update({'cost_mat': (self.cost_mat)})\n",
        "    return config\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "    # something goes wrong here and changes self.cost_mat to a list variable.\n",
        "    # See above for temporary fix\n",
        "    return cls(**config)\n",
        "\n",
        "def get_sample_weights(y_true, y_pred, cost_m):\n",
        "    num_classes = len(cost_m)\n",
        "\n",
        "    y_pred.shape.assert_has_rank(2)\n",
        "    assert(y_pred.shape[1] == num_classes)\n",
        "    y_pred.shape.assert_is_compatible_with(y_true.shape)\n",
        "\n",
        "    y_pred = K.one_hot(K.argmax(y_pred), num_classes)\n",
        "\n",
        "    y_true_nk1 = K.expand_dims(y_true, 2)\n",
        "    y_pred_n1k = K.expand_dims(y_pred, 1)\n",
        "    cost_m_1kk = K.expand_dims(cost_m, 0)\n",
        "\n",
        "    sample_weights_nkk = cost_m_1kk * y_true_nk1 * y_pred_n1k\n",
        "    sample_weights_n = K.sum(sample_weights_nkk, axis=[1, 2])\n",
        "\n",
        "    return sample_weights_n\n",
        "\n",
        "\n",
        "# Register the loss in the Keras namespace to enable loading of the custom object.\n",
        "tf.keras.losses.WeightedCategoricalCrossentropy = WeightedCategoricalCrossentropy\n",
        " \n",
        "#@title\n",
        "def plot_model_history(model_history, nb_epoch, cm3): \n",
        "  # Parameters\n",
        "  # ----------\n",
        "  # model_history : keras.callbacks.History\n",
        "  #     The history object returned by the fit() method of the model.\n",
        "  # cm3 : 10x10 dataframe \n",
        "  #      10x10 dataframe of confusion matrix from predicted X_val categories\n",
        "  # nb_epoch = restored_weights : int\n",
        "  #     The epoch at which the weights were restored.\n",
        "  # tot_epochs : int\n",
        "  #     Calculated Total number of epochs for which the model was trained.\n",
        "  \n",
        "   \n",
        "  tot_epochs = max(model_history.epoch)+1  #if the total epochs ran is 28, it'll show up as 27 in the epoch object so we must add 1\n",
        "  print(\"Total Epochs: \", tot_epochs)\n",
        "\n",
        "  #if tot_epochs is the total number of epochs ran then early stop did not happen, and we need not minus patience\n",
        "  if tot_epochs == nb_epoch:\n",
        "    restored_weights = tot_epochs\n",
        "  else:\n",
        "    restored_weights  = tot_epochs-patience   #when using restore-best-weights and patience, it'll restore the best weights back\n",
        "  print(\"Restored weights at \", restored_weights, \"Patience used: \", patience)\n",
        "\n",
        "  fig = plt.figure(figsize=(20, 10)) # I don't think this works for some reason\n",
        "  fig, ax = plt.subplots(1,3)\n",
        "  ax[0].plot(range(1,tot_epochs+1), model_history.history['categorical_accuracy'], color='blue',             label='Training')\n",
        "  ax[0].plot(range(1,tot_epochs+1), model_history.history['val_categorical_accuracy'] , color='orange',             label='Validation')\n",
        "  ax[0].scatter((restored_weights), model_history.history['val_categorical_accuracy'][restored_weights-1] , color='orange')\n",
        "  ax[0].scatter(restored_weights, model_history.history['categorical_accuracy'][restored_weights-1], color='blue')\n",
        "  ax[0].annotate(text=str(restored_weights),  xy=(restored_weights, model_history.history['val_categorical_accuracy'][restored_weights-1]),\n",
        "                  textcoords=\"offset points\", xytext=(0,10), ha='center', color='black')\n",
        "  ax[0].legend()\n",
        "  ax[0].set_title('Training and Validation Accuracy')\n",
        "\n",
        "  ax[1].plot(range(1,tot_epochs+1), model_history.history['loss'], color= 'blue', label='Training')\n",
        "  ax[1].plot(range(1,tot_epochs+1), model_history.history['val_loss'], color='orange', label='Validation')\n",
        "  ax[1].scatter(restored_weights, model_history.history['loss'][restored_weights-1], color='blue')\n",
        "  ax[1].scatter((restored_weights), model_history.history['val_loss'][restored_weights-1] , color='orange')\n",
        "  ax[1].annotate(text=str(restored_weights),  xy=(restored_weights, model_history.history['val_loss'][restored_weights-1]),\n",
        "                  textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "  ax[1].legend()\n",
        "  ax[1].set_title('Training and Validation Loss')\n",
        "\n",
        "\n",
        "  cm3_wodiag = cm3*(np.ones((10,10)) - np.eye(10))\n",
        "\n",
        "  ax[2] = sns.heatmap(cm3_wodiag, annot=True, annot_kws={\"size\": 7},  fmt='g', cmap=sns.cm.rocket_r) # font size\n",
        "  ax[2].set_xlabel('Predicted Class')\n",
        "  ax[2].set_ylabel('True Class')\n",
        "  ax[2].set_title('# of misclassifications of 9 as 4 is '+str(cm3[4][9]))\n",
        "  cbar = ax[2].collections[0].colorbar\n",
        "  cbar.remove() # Just takes up valuable room and is worthless\n",
        "\n",
        "\n",
        "  plt.gcf().set_size_inches(15, 5)  # this works \n",
        "  # plt.gcf().suptitle(f\"Lambda Value {lambda_val} for {nb_epoch} Epochs and Patience {patience} \" )\n",
        "\n",
        "\n",
        "  #@title\n",
        "def create_model(): #Removed cost-matrix which is called up in the Compile Function and passed to the weighted-loss function\n",
        "  model = Sequential()\n",
        "  model.add(Dense(40, input_shape=(784,), kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(40, kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(10,kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "  model.add(Activation('softmax'))\n",
        "  return model  #I removed Compile\n",
        "\n",
        "\n",
        "\n",
        "  #@title\n",
        "def log_confusion_matrix( epoch, logs):\n",
        "  # Use the model to predict the values from the validation dataset.\n",
        "  y_prediction = model.predict(X_val, verbose=0)     #I call it y_prediction3 because I just want to make sure this is  updated within and not interfering with the other prediction below\n",
        "  y_prediction  = np.argmax(y_prediction, axis=1)\n",
        "\n",
        "  #Create confusion matrix \n",
        "  cm = confusion_matrix(y_val, y_prediction)\n",
        "  cm_array = np.asarray(cm)  #Indiv CM as array for storing\n",
        "  logs['9T_4P'] = cm[9,4]\n",
        "  logs['4T_9P'] = cm[4,9]\n",
        "\n",
        "  logs['0T_Acc'] = cm[0,0]/np.sum(cm[0])\n",
        "  logs['1T_Acc'] = cm[1,1]/np.sum(cm[1])\n",
        "  logs['2T_Acc'] = cm[2,2]/np.sum(cm[2])\n",
        "  logs['3T_Acc'] = cm[3,3]/np.sum(cm[3])\n",
        "  logs['4T_Acc'] = cm[4,4]/np.sum(cm[4])\n",
        "  logs['5T_Acc'] = cm[5,5]/np.sum(cm[5])\n",
        "  logs['6T_Acc'] = cm[6,6]/np.sum(cm[6])\n",
        "  logs['7T_Acc'] = cm[7,7]/np.sum(cm[7])\n",
        "  logs['8T_Acc'] = cm[8,8]/np.sum(cm[8])\n",
        "  logs['9T_Acc'] = cm[9,9]/np.sum(cm[9])\n",
        "\n",
        "\n",
        "  logs['cm_per_epoch'] = cm_array.reshape((1,100))\n",
        "\n",
        "#@title\n",
        "def log_classification_report( epoch, logs):\n",
        "  # Use the model to predict the values from the validation dataset.\n",
        "    y_prediction = model.predict(X_val, verbose=0)     #I call it y_prediction3 because I just want to make sure this is  updated within and not interfering with the other prediction below\n",
        "    y_prediction  = np.argmax(y_prediction, axis=1)\n",
        "\n",
        "    #Create confusion matrix \n",
        "    cr = classification_report(y_val, y_prediction)\n",
        "    # print(cr)\n",
        "    logs['cr_per_epoch'] = cr \n",
        "\n",
        "\n",
        "    #@title\n",
        "def log_f1_score( epoch, logs):\n",
        "  # Use the model to predict the values from the validation dataset.\n",
        "    y_prediction = model.predict(X_val, verbose=0)     #I call it y_prediction3 because I just want to make sure this is  updated within and not interfering with the other prediction below\n",
        "    y_prediction  = np.argmax(y_prediction, axis=1)\n",
        "\n",
        "\n",
        "    logs[\"f1_micro\"] = f1_score(y_val, y_prediction, average='micro')\n",
        "    logs[\"f1_macro\"] = f1_score(y_val, y_prediction, average='macro')\n",
        "    logs[\"f1_weighted\"] = f1_score(y_val, y_prediction, average='weighted')\n",
        "    logs[\"f1_notweighted\"] = f1_score(y_val, y_prediction, average=None)\n",
        "\n",
        "\n",
        "#@title\n",
        "def return_cm(model):\n",
        "  y_prediction = model.predict(X_val, verbose=0)\n",
        "  y_prediction  = np.argmax(y_prediction, axis=1)\n",
        "  # Y_prediction = np_utils.to_categorical(y_prediction, nb_classes)\n",
        "\n",
        "  cm3 = confusion_matrix(y_val, y_prediction)\n",
        "  cm3 = pd.DataFrame(cm3, range(10),range(10))\n",
        "  return cm3\n",
        "\n",
        "  # # plt.figure(figsize = (4,4))\n",
        "  # # cm3\n",
        "  # sns.heatmap(cm3, annot=True, annot_kws={\"size\": 7},  fmt='g') # font size\n",
        "  # plt.show()\n",
        "  # # cm_using_weighted_new = cm3\n",
        " #@title\n",
        "\n",
        "def return_cr(model):\n",
        "  y_prediction = model.predict(X_val, verbose=0)\n",
        "  y_prediction  = np.argmax(y_prediction, axis=1)\n",
        "  # Y_prediction = np_utils.to_categorical(y_prediction, nb_classes)\n",
        "\n",
        "  cr = classification_report(y_val, y_prediction)\n",
        "  print(cr)\n",
        "  return cr\n",
        "\n",
        "\n",
        "def return_f1score(model):\n",
        "  # sklearn.metrics.f1_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')[source] \n",
        "  y_prediction = model.predict(X_val, verbose=0)\n",
        "  y_prediction  = np.argmax(y_prediction, axis=1)\n",
        "  # Y_prediction = np_utils.to_categorical(y_prediction, nb_classes)\n",
        "\n",
        "  f1_notweighted = f1_score(y_val, y_prediction, average=None)\n",
        "  print(f1_notweighted)\n",
        "  f1_micro = f1_score(y_val, y_prediction, average='micro')\n",
        "  f1_macro = f1_score(y_val, y_prediction, average='macro')\n",
        "  f1_weighted = f1_score(y_val, y_prediction, average='weighted')\n",
        "  # print(\"Micro: \", f1_micro, \"Macro: \", f1_macro, \"Weighted: \", f1_weighted)\n",
        "  print(\"Micro: {:.5f} Macro: {:.5f} Weighted: {:.5f}\".format(f1_micro, f1_macro, f1_weighted))\n",
        "  # return f1\n",
        "\n",
        "\n",
        "\n",
        "#@title: plot_model_history_all\n",
        "def plot_model_history_all(model_history, nb_epoch=None, cm3=None): \n",
        "  # Parameters\n",
        "  # ----------\n",
        "  # tot_epochs : int\n",
        "  #     Total number of epochs for which the model was trained.\n",
        "  # model_history : keras.callbacks.History\n",
        "  #     The history object returned by the fit() method of the model.\n",
        "  # cm3 : 10x10 dataframe \n",
        "  #      10x10 dataframe of confusion matrix from predicted X_val categories\n",
        "  # restored_weights : int\n",
        "  #     The epoch at which the weights were restored.\n",
        "\n",
        "  \n",
        "\n",
        "  tot_epochs = max(model_history.epoch)+1  #if the total epochs ran is 28, it'll show up as 27 in the epoch object so we must add 1\n",
        "  # print(\"Total Epochs: \", tot_epochs)\n",
        "\n",
        "  #if tot_epochs is the total number of epochs ran then early stop did not happen, and we need not minus patience\n",
        "  if tot_epochs == nb_epoch:\n",
        "    restored_weights = tot_epochs\n",
        "  else:\n",
        "    restored_weights  = tot_epochs-patience   #when using restore-best-weights and patience, it'll restore the best weights back\n",
        "  # print(\"Restored weights at \", restored_weights, \"Patience used: \", patience)\n",
        "\n",
        "  ax[0].plot(range(1,tot_epochs+1), model_history.history['categorical_accuracy'], color='blue',           )\n",
        "  ax[0].plot(range(1,tot_epochs+1), model_history.history['val_categorical_accuracy'] , color='orange',    )\n",
        "  ax[0].scatter((restored_weights), model_history.history['val_categorical_accuracy'][restored_weights-1] , color='orange')\n",
        "  ax[0].scatter(restored_weights, model_history.history['categorical_accuracy'][restored_weights-1], color='blue')\n",
        "  ax[0].annotate(text=str(restored_weights),  xy=(restored_weights, model_history.history['val_categorical_accuracy'][restored_weights-1]),\n",
        "                  textcoords=\"offset points\", xytext=(0,10), ha='center', color='black')\n",
        "  # ax[0].legend()\n",
        "  ax[0].set_title('Training (Blue) and Validation (Orange) Accuracy', fontsize='8')\n",
        "\n",
        "  ax[1].plot(range(1,tot_epochs+1), model_history.history['loss'], color= 'blue',  )\n",
        "  ax[1].plot(range(1,tot_epochs+1), model_history.history['val_loss'], color='orange',  )\n",
        "  ax[1].scatter(restored_weights, model_history.history['loss'][restored_weights-1], color='blue')\n",
        "  ax[1].scatter((restored_weights), model_history.history['val_loss'][restored_weights-1] , color='orange')\n",
        "  ax[1].annotate(text=str(restored_weights),  xy=(restored_weights, model_history.history['val_loss'][restored_weights-1]),\n",
        "                  textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "  # ax[1].legend()\n",
        "  ax[1].set_title('Training (Blue) and Validation (Orange) Loss' , fontsize='8')\n",
        "\n",
        "\n",
        "  plt.gcf().set_size_inches(10, 5)  # this works \n",
        "  # plt.gcf().suptitle(f\"Lambda Value {lambda_val} for {nb_epoch} Epochs and Patience {patience} \" )\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## ----------------------------------------------------------------------------------------------\n",
        "## Tried to create a Callback to call the Classifcatinreport ever 5 epochs, but it doesnt work \n",
        "## because it cant see the model.  ZGoing back to trying to define the function to only be called every 5th epoch\n",
        "\n",
        "\n",
        "# class cr_callback(tf.keras.callbacks.Callback):\n",
        "\n",
        "#     def on_epoch_end(self, epoch, log=None):\n",
        "\n",
        "#         if epoch % 5 == 0:  # <- add additional condition here\n",
        "#             self._do_the_stuff()\n",
        "            \n",
        "            \n",
        "#     def _do_the_stuff(self, model):\n",
        "#         print('Do the stuff')\n",
        "#         y_prediction = model.predict(X_val, verbose=0)\n",
        "#         y_prediction  = np.argmax(y_prediction, axis=1)\n",
        "#         # Y_prediction = np_utils.to_categorical(y_prediction, nb_classes)\n",
        "#         cr = classification_report(y_val, y_prediction)\n",
        "#         logs['cr'] = cr\n",
        "\n",
        "#           # return cr\n",
        "        \n",
        "#     # def on_training_end(self, logs=None):\n",
        "#     #     self._do_the_stuff()\n",
        "\n",
        "\n",
        "\n",
        "## Saw this example ono Kaggle, but couldn't get it to work, it does give me an idea(if epoch==0)\n",
        "\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# class every10epochCallback(tf.keras.callbacks.Callback):\n",
        "#     def __init__(self, X_val, Y_val):\n",
        "#         super().__init__()\n",
        "#         self.X = X_val\n",
        "#         self.y = Y_val.argmax(axis=1)\n",
        "#     def on_epoch_begin(self, epoch, logs=None):\n",
        "#         if epoch == 0:\n",
        "#             return\n",
        "#         if epoch%10==0:\n",
        "#             pred = (model.predict(self.X))\n",
        "#             print('epoch: ',epoch,'  ,Accuracy:  ',accuracy_score(self.y,pred.argmax(axis=1)),' ')\n",
        "\n",
        "# model.fit(X_train, Y_train,batch_size=batch_size,epochs=10,verbose=0,  validation_data=(X_val, Y_val), shuffle=True, use_multiprocessing=True, \n",
        "#           callbacks=[every10epochCallback(X_val,Y_val)])\n"
      ],
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3NeKdNCLXk1",
        "outputId": "e88080a5-3a16-448b-ee8e-39fa398bd07d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ImportLibraries_DefineFunctions.py\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oLPme2BLEDsz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3d0441b-e97f-41c8-f126-a1a8aa2c777f",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "# Bring the file into the local Python environment.\n",
        "execfile('ImportLibraries_DefineFunctions.py')\n",
        "\n",
        "# Call the function defined in the file.\n",
        "# f()\n",
        "time.localtime(time.time())\n"
      ],
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version:  2.12.0\n",
            "Keras version:  2.12.0\n",
            "Finished Loading Libraries\n",
            "60000 train samples\n",
            "7500 validation samples\n",
            "2500 test samples\n",
            "Train [0.09871667 0.11236667 0.0993     0.10218333 0.09736667 0.09035\n",
            " 0.09863333 0.10441667 0.09751666 0.09915   ]\n",
            "Train # of 9s 5949.0\n",
            "Train # of 4s 5842.0\n",
            "Val [0.09586667 0.1132     0.10453334 0.10066666 0.09986667 0.09013333\n",
            " 0.09413333 0.1016     0.09746667 0.10253333]\n",
            "Val # of 9s 769.0\n",
            "Val # of 4s 749.0\n",
            "Test [0.1044 0.1144 0.0992 0.102  0.0932 0.0864 0.1008 0.1064 0.0972 0.096 ]\n",
            "Test  # of 9s 240.0\n",
            "Test  # of 4s 233.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "time.struct_time(tm_year=2023, tm_mon=5, tm_mday=1, tm_hour=19, tm_min=5, tm_sec=55, tm_wday=0, tm_yday=121, tm_isdst=0)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# #As of 4/18 I am changing this code to save weights initially after five epochs using lambdavalue=1 initially\n",
        "\n",
        "# rms = RMSprop()  #https://keras.io/api/optimizers/rmsprop/ #default learning_rate=0.001\n",
        "sgd = SGD(learning_rate=0.1)\n",
        "\n",
        "patience = 0\n",
        "\n",
        "\n",
        "# SET THE IITIAL LAMBDA VALUE! \n",
        "cost_matrix = np.ones((10,10))\n",
        "lambda_val = 1\n",
        "\n",
        "Truth=9\n",
        "Predicted=4\n",
        "cost_matrix[Truth, Predicted] = lambda_val\n",
        "\n",
        "\n",
        "# # Define the per-epoch callback.\n",
        "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix, )\n",
        "# cr_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_classification_report, )\n",
        "# es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, verbose=1, restore_best_weights = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ## Examples from TDS\n",
        "# from tensorflow.keras.callbacks import LambdaCallback\n",
        "# epoch_callback = LambdaCallback(\n",
        "#     on_epoch_begin=lambda epoch,logs: print('Starting Epoch {}!'.format(epoch+1))\n",
        "# )\n",
        "# batch_loss_callback = LambdaCallback(\n",
        "#     on_batch_end=lambda batch,logs: print('\\n After batch {}, the loss is {:7.2f}.'.format(batch, logs['loss']))\n",
        "# )\n",
        "# train_finish_callback = LambdaCallback(\n",
        "#     on_train_end=lambda logs: print('Training finished!')\n",
        "# )\n",
        "\n",
        "# # Lambda function using if else & else if\n",
        "# min = lambda a, b, c : f\"{a} is smaller\" if(a < b & b < c) \\\n",
        "#      else f\"{b} is smaller\"  if (b < c) else f\"{c} is smaller\" \n",
        "# print(min(40, 30, 10))\n",
        "\n",
        "epoch_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end = lambda epoch,logs: \n",
        "                                                   print(' Epoch {} modulus 5 is {}!'.format(epoch+1, (epoch+1)%5))  )\n",
        "\n",
        "# cr_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end= lambda epoch, logs:\n",
        "# #                                                log_classification_report(epoch, logs) if(epoch+1)%5==0 else print(\" \") )\n",
        "#                                               #  return_cr if(epoch+1)%5==0 else print(\" \") )\n",
        "\n",
        "\n",
        "f1_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_f1_score)\n",
        "\n",
        "\n",
        "model = create_model()\n",
        "\n",
        "# save the model weights\n",
        "model.save_weights('initial_0epochs.h5')\n",
        "\n",
        "\n",
        "\n",
        "model.compile(loss=WeightedCategoricalCrossentropy(cost_matrix), optimizer=sgd,  metrics='categorical_accuracy',)\n",
        "\n",
        "# model_history = model.fit(X_train, Y_train,  batch_size=batch_size, epochs=10, verbose=2,\n",
        "#         validation_data=(X_val, Y_val), shuffle=True, use_multiprocessing=True, callbacks = [ cm_callback, f1_callback])\n",
        "\n",
        "# model.save_weights('initial_10epochs.h5')\n",
        "\n",
        "# model.save_weights('initial_10epochs.h5')\n",
        "# model.save_weights('baseline_for_100epochs.h5')\n",
        "\n",
        "# save the model weights\n",
        "# model.save_weights('initial_150epochs.h5')\n",
        "\n",
        " \n",
        "# cr = return_cr(model)\n",
        "!date\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QcJ3LplOkww",
        "outputId": "ee8d4941-a532-48b1-a14c-3ceade84d2f3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.01      0.03      0.01       719\n",
            "           1       0.00      0.00      0.00       849\n",
            "           2       0.24      0.02      0.04       784\n",
            "           3       0.25      0.03      0.05       755\n",
            "           4       0.04      0.00      0.01       749\n",
            "           5       0.36      0.01      0.02       676\n",
            "           6       0.11      0.78      0.20       706\n",
            "           7       0.00      0.00      0.00       762\n",
            "           8       0.02      0.01      0.01       731\n",
            "           9       0.38      0.06      0.10       769\n",
            "\n",
            "    accuracy                           0.09      7500\n",
            "   macro avg       0.14      0.09      0.04      7500\n",
            "weighted avg       0.14      0.09      0.04      7500\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ImMl-yPWPfT0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below to generate variations of the model training / validation results after training with a cost matrix 30 times did not produce any variability.   Therefore I want to shuffle the training/validation deck randomly between training sessions to see if that helps introduce some randomness.  "
      ],
      "metadata": {
        "id": "RLYZ6gR4WjKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The Training Runs - Commented out to ensure not run again when the notebook is saved with output"
      ],
      "metadata": {
        "id": "V_lUeeXP2Q5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This takes hours to run for about 40 replications so usually after running, I save the out put to GitHub for easy lookup later"
      ],
      "metadata": {
        "id": "owkJgwju2a52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "from google.colab import files\n",
        "\n",
        "## -----------------------------------------------------------------------------------------------------\n",
        "## Now we need to load the weights of the model, and continue training with a different cost matrix\n",
        "## ------------------------------------------------------------------------------------------------------\n",
        "# load the model weights\n",
        "# model.load_weights('initial_5epochs.h5') \n",
        "\n",
        "cost_matrix = np.ones((10,10))\n",
        "\n",
        "model_history_all = []\n",
        "cm_all            = []\n",
        "\n",
        "# cost_list = [10, 100, 1000, 1] # Each one takes about 2 minutes 5*4*2=40 minutes for 5 costs/4 reps -- actually all took 40 reps took almost  1.5 hours\n",
        "cost_list = [1] # Each one takes about 2 minutes 5*4*2=40 minutes for 5 costs/4 reps -- actually all took 40 reps took almost  1.5 hours\n",
        "reps = 30\n",
        "\n",
        "for k in cost_list:\n",
        "  for i in range(reps):\n",
        "    print(\"starting rep \", i, \" for \", k , \"-cost.\")\n",
        "\n",
        "    cost_matrix[9,4] = k\n",
        "    model = create_model()\n",
        "\n",
        "    # model.load_weights('initial_10epochs.h5')\n",
        "\n",
        "    #I may need to re-initiate the optimizer to have a smaller learning rate\n",
        "    model.compile(loss=WeightedCategoricalCrossentropy(cost_matrix), optimizer=sgd,  metrics='categorical_accuracy',)\n",
        "    \n",
        "    nb_epoch = 110\n",
        "    # patience = 20\n",
        "\n",
        "    # es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, verbose=1, restore_best_weights = True)\n",
        "\n",
        "    X_train_shuffled = shuffle(X_train, random_state=42+i)\n",
        "    Y_train_shuffled = shuffle(Y_train, random_state=42+i)\n",
        " \n",
        "\n",
        "    history = model.fit(X_train_shuffled, Y_train_shuffled,          batch_size=batch_size, epochs=nb_epoch, verbose=0,\n",
        "            validation_data=(X_val, Y_val), shuffle=True, use_multiprocessing=True, callbacks = [ cm_callback, f1_callback, epoch_callback])\n",
        "\n",
        "    cm3 = return_cm(model)\n",
        "\n",
        "    del(history.model)\n",
        "    model_history_all.append(history)\n",
        "    cm_all.append(cm3)\n",
        "    ## Now I need to plot all of the \"model_history_all\"\n",
        "\n",
        "    import pickle\n",
        "\n",
        "    # save the variable to a pickle file\n",
        "    with open('baseline_models.pkl', 'wb') as f:\n",
        "        pickle.dump(model_history_all, f)\n",
        "\n",
        "    files.download('baseline_models.pkl')\n",
        "\n",
        "    with open('baseline_cms.pkl', 'wb') as f:\n",
        "        pickle.dump(cm_all, f)\n",
        "\n",
        "    files.download('baseline_cms.pkl')\n",
        "    !date\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # print(model_history_all) #18 items into _ is taking 1 hours and 3 minutes!  perhaps im not usng the right settings - i have no-accelerator on google colab"
      ],
      "metadata": {
        "id": "ZFgEx-T-fg28",
        "outputId": "cd671f73-5847-41f7-821d-d10ff7970743",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting rep  0  for  1 -cost.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_598296f2-9807-4c93-a1b1-adb3f1b098ff\", \"baseline_models.pkl\", 146749)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a647e208-9729-417d-80a9-5f053d75342c\", \"baseline_cms.pkl\", 1384)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting rep  1  for  1 -cost.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download the multiple runs and reps of Model History from training with different lambda values\n",
        "\n",
        "\n",
        "Also commented out because once everything is ran and saved, I don't want to accidentally save additional files"
      ],
      "metadata": {
        "id": "7z0g0vdr2w05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Download the history\n",
        "# from google.colab import files\n",
        "\n",
        "\n",
        "# #  SUPER IMPORTANT ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# for item in model_history_all:\n",
        "#   # item.history\n",
        "#   del item.model\n",
        "# # IF YOU DON'T DELETE THE MODEL YOU WON'T BE ABLE TO UNPICKLE YOUR RESULTS AND THAT WILL SUCK AND YOU WILL CRY\n",
        "# # and no ones wants to see you ugly cry over hours of wasted coding\n",
        "\n",
        "\n",
        "# import pickle\n",
        "\n",
        "# # create a variable\n",
        "# #model_history_all #50 items\n",
        "# #cm_all #50 items\n",
        "\n",
        "# # save the variable to a pickle file\n",
        "# with open('initial_10_secondphase_lambda_history.pkl', 'wb') as f:\n",
        "#     pickle.dump(model_history_all, f)\n",
        "\n",
        "# files.download('initial_10_secondphase_lambda_history.pkl')\n",
        "\n",
        "# with open('initial_10_secondphase_lambda_cm.pkl', 'wb') as f:\n",
        "#     pickle.dump(cm_all, f)\n",
        "\n",
        "# files.download('initial_10_secondphase_lambda_cm.pkl')\n"
      ],
      "metadata": {
        "id": "JLt4gsflmpun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load Model History Variables from Pickle Files - must have weighted-categorical-accuracy defined"
      ],
      "metadata": {
        "id": "WPmV3ALQcsjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #Upload and save the history to variable \n",
        "# import pickle\n",
        "\n",
        "# # #load the pickle file\n",
        "# with open('initial_10_secondphase_lambda_history.pkl', 'rb') as handle:\n",
        "#     model_history_all = pickle.load(handle)\n",
        "\n",
        "# # import keras\n",
        "# # keras.models.load_model('initial_10_secondphase_lambda_history.pkl')\n",
        "# #use the loaded variable\n",
        "# print(model_history_all)\n",
        "\n",
        "# # #load the pickle file\n",
        "# with open('initial_10_secondphase_lambda_cm.pkl', 'rb') as handle:\n",
        "#     cm_all = pickle.load(handle)\n",
        "\n",
        "# # use the loaded variable\n",
        "# print(cm_all)\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "dXFoZGvVZ1oE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example of how to concatenate multiple dataframes without causing an error.  Have to use pandas concatenate function."
      ],
      "metadata": {
        "id": "smchxHuJ3MsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import pandas as pd\n",
        "\n",
        "# Create an empty dataframe\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# Create a list of data\n",
        "data = [1, 2, 3, 4, 5]\n",
        "\n",
        "# Use a for loop to fill the dataframe\n",
        "for x in data:\n",
        "    df = pd.concat([df, pd.DataFrame({'Number':[x], 'Square':[x**2]})], ignore_index=True)\n",
        "\n",
        "# Show the dataframe\n",
        "print(df)"
      ],
      "metadata": {
        "id": "oGYSkVqqk0xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading the data into a Dataframe to be saved as a CSV file"
      ],
      "metadata": {
        "id": "IR2BLyPT3YFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#I want to break out the model_history and cm_history into a dataframe to be referenced by Cost and Rep\n",
        "import pandas as pd\n",
        "df = pd.DataFrame()\n",
        "count=1\n",
        "\n",
        "\n",
        "#It'd be better if i could refer to the \"cost\"/lambda2 as a variable in the model_history, \n",
        "# but i recreated it here\n",
        "# \n",
        "# cost_list = [10, 100, 1000, 1] # Each one takes about 2 minutes 5*4*2=40 minutes for 5 costs/4 reps -- actually all took 40 reps took almost  1.5 hours\n",
        "\n",
        "cost_list = [ 1]\n",
        "\n",
        "for k in cost_list:\n",
        "\n",
        "  \n",
        "#It'd be better if i could refer to the reps a variable in the model_history, \n",
        "# but i recreated it here as the range(10) that was originally used\n",
        "\n",
        "  for i in range(30):\n",
        "    # print(\"k: \", k, \"i: \", i)\n",
        "    df = pd.concat([df, pd.DataFrame({\"cost\": [k], \"rep\": [i],\n",
        "                                      \"model_history\": [model_history_all[count-1]],\n",
        "                                      # \"cm\": [cm_all[count-1]]\n",
        "                                      })] , \n",
        "                   ignore_index=True )\n",
        "    count+=1    \n",
        "\n",
        "\n",
        "# I create a dataframe with CSV but  don't save/download it \n",
        "df.to_csv('dataframe.csv',index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "Vw6G6TnPiVrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #I want to add the 50 epochs i did of lambda1 = 1 using SGD to this list\n",
        "\n",
        "# df = pd.concat([df, pd.DataFrame({\"cost\": 1, \"rep\": 1,\n",
        "#                                       \"model_history\": [model_history],\n",
        "#                                       # \"cm\": [cm_all[count-1]]\n",
        "#                                       })] , \n",
        "#                    ignore_index=True )"
      ],
      "metadata": {
        "id": "lNqjDelK9VLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Example of using df.groupby in a for loop to plot multiple types of data - doesn't include Label\n",
        "```\n",
        "for group in df.groupby(\"cost\"): \n",
        "  fig = plt.figure(figsize=(20, 10))  \n",
        "  fig, ax = plt.subplots(1,2) \n",
        "  ax[0].scatter(group[1][\"cost\"], group[1][\"quantity\"]) \n",
        "  ax[0].set_title(\"cost vs. quantity\") \n",
        "  ax[1].scatter(group[1][\"cost\"], group[1][\"revenue\"]) \n",
        "  ax[1].set_title(\"cost vs. revenue\")\n",
        "  plt.show()\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "zKCVz62dUpBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import time\n",
        "nb_epoch = 50\n",
        "patience = 0\n",
        "df.groupby(\"cost\")\n",
        "\n",
        "for x, group in df.groupby(\"cost\"):\n",
        "  fig = plt.figure(figsize=(20, 10)) \n",
        "  fig, ax = plt.subplots(1,2)\n",
        "  print(\"Group of Lambda-Value:\",   x)\n",
        "\n",
        "  # group\n",
        "  for item in group.model_history:\n",
        "    plot_model_history_all(item)    \n",
        "  plt.show()\n",
        "    # for item, cm3 in zip(group.model_history, group.cm):\n",
        "\n",
        "  #   # cm3_wodiag = cm3*(np.ones((10,10)) - np.eye(10))\n",
        "\n",
        "  #   # ax[2] = sns.heatmap(cm3_wodiag, annot=True, annot_kws={\"size\": 7},  fmt='g', cmap=sns.cm.rocket_r, cbar=False) # font size\n",
        "  #   # ax[2].set_xlabel('Predicted Class')\n",
        "  #   # ax[2].set_ylabel('True Class')\n",
        "  #   # ax[2].set_title('# of misclassifications of 9 as 4 is '+str(cm3[4][9]))\n",
        "  #   # cbar = ax[2].collections[0].colorbar\n",
        "  #   # cbar.remove() # Just takes up valuable room and is worthless\n",
        "  #   plot_model_history_all(item)    \n",
        "\n",
        "  #   #   ax[0].set_title(\"Lambda Value of\"+str(df[\"cost\"]))\n",
        "    \n",
        "  # #   # print(0+r,' to ', r+4+1)  #Correct"
      ],
      "metadata": {
        "id": "DSA5-95WZkj3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}