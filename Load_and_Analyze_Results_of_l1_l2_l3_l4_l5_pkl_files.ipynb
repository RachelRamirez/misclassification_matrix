{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaRsdMuIk+9GgjgTK6103J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RachelRamirez/misclassification_matrix/blob/main/Load_and_Analyze_Results_of_l1_l2_l3_l4_l5_pkl_files.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Results\n",
        "\n",
        "I have a lot of pickle files.  Let's see if I can load them all up and put in a csv format."
      ],
      "metadata": {
        "id": "PaeHBLgDLEbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Qwl3UxhpLECp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4ed40d8-da45-476b-8566-6e8be9139bea"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "tuUF7Fl1VDgI"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After executing the cell above, Drive\n",
        "# files will be present in \"/content/drive/My Drive\".\n",
        "!ls \"/content/drive/My Drive/Con3\"\n",
        "path = \"/content/drive/MyDrive/Con3/\"\n",
        "\n",
        "# file_name = \"delete_later.pkl\"\n",
        "\n",
        "import pickle \n",
        "\n",
        "import os\n",
        "\n",
        "dir = path\n",
        "\n",
        "dict_of_run_files = {}\n",
        "\n",
        "count = 0\n",
        "for file in os.listdir(path):\n",
        "  if file.endswith(\".pkl\"):\n",
        "    # myfunction(file)\n",
        "    with open(path + file, 'rb') as handle:\n",
        "    # Call load method to deserialze\n",
        "      dict_of_run_files[file] = pickle.load(handle)\n",
        "      # list_of_files.extend(value for name, value in sorted(os.listdir(path)).items(), key=lambda item: item[0]) if name.startswith('run')\n",
        "      count+=1 \n",
        "      # print(unpickled_object == variable_to_be_deleted)\n",
        "      print(count, \". \", file, \" was saved to dict_of_run_files. \")\n",
        "\n",
        "      "
      ],
      "metadata": {
        "id": "EgWoXskENjTV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98a441e5-3c92-42d6-a78c-a5cf883516eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 5Lambdas1stPhase\n",
            " delete_later.pkl\n",
            " PreExperiment_PA_Shfl_40D_Lambda1_Lambda2_Lambda3\n",
            "'w[9,4]_L_1000_100_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1720_.pkl'\n",
            "'w[9,4]_L_1000_100_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1721_.pkl'\n",
            "'w[9,4]_L_1000_100_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1722_.pkl'\n",
            "'w[9,4]_L_1000_100_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1723_.pkl'\n",
            "'w[9,4]_L_1000_100_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1725_.pkl'\n",
            "'w[9,4]_L_1000_100_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1257_.pkl'\n",
            "'w[9,4]_L_1000_100_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1259_.pkl'\n",
            "'w[9,4]_L_1000_100_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1300_.pkl'\n",
            "'w[9,4]_L_1000_100_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1302_.pkl'\n",
            "'w[9,4]_L_1000_100_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1304_.pkl'\n",
            "'w[9,4]_L_1000_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1707_.pkl'\n",
            "'w[9,4]_L_1000_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1709_.pkl'\n",
            "'w[9,4]_L_1000_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1711_.pkl'\n",
            "'w[9,4]_L_1000_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1712_.pkl'\n",
            "'w[9,4]_L_1000_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1713_.pkl'\n",
            "'w[9,4]_L_1000_10_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1243_.pkl'\n",
            "'w[9,4]_L_1000_10_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1245_.pkl'\n",
            "'w[9,4]_L_1000_10_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1246_.pkl'\n",
            "'w[9,4]_L_1000_10_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1247_.pkl'\n",
            "'w[9,4]_L_1000_10_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1249_.pkl'\n",
            "'w[9,4]_L_1000_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1701_.pkl'\n",
            "'w[9,4]_L_1000_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1702_.pkl'\n",
            "'w[9,4]_L_1000_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1703_.pkl'\n",
            "'w[9,4]_L_1000_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1705_.pkl'\n",
            "'w[9,4]_L_1000_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1706_.pkl'\n",
            "'w[9,4]_L_1000_1_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1236_.pkl'\n",
            "'w[9,4]_L_1000_1_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1238_.pkl'\n",
            "'w[9,4]_L_1000_1_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1239_.pkl'\n",
            "'w[9,4]_L_1000_1_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1241_.pkl'\n",
            "'w[9,4]_L_1000_1_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1242_.pkl'\n",
            "'w[9,4]_L_1000_200_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1726_.pkl'\n",
            "'w[9,4]_L_1000_200_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1727_.pkl'\n",
            "'w[9,4]_L_1000_200_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1729_.pkl'\n",
            "'w[9,4]_L_1000_200_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1730_.pkl'\n",
            "'w[9,4]_L_1000_200_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1731_.pkl'\n",
            "'w[9,4]_L_1000_200_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1305_.pkl'\n",
            "'w[9,4]_L_1000_200_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1306_.pkl'\n",
            "'w[9,4]_L_1000_200_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1308_.pkl'\n",
            "'w[9,4]_L_1000_200_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1309_.pkl'\n",
            "'w[9,4]_L_1000_200_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1311_.pkl'\n",
            "'w[9,4]_L_1000_500_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1732_.pkl'\n",
            "'w[9,4]_L_1000_500_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1733_.pkl'\n",
            "'w[9,4]_L_1000_500_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1734_.pkl'\n",
            "'w[9,4]_L_1000_500_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1735_.pkl'\n",
            "'w[9,4]_L_1000_500_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1737_.pkl'\n",
            "'w[9,4]_L_1000_500_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1312_.pkl'\n",
            "'w[9,4]_L_1000_500_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1314_.pkl'\n",
            "'w[9,4]_L_1000_500_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1315_.pkl'\n",
            "'w[9,4]_L_1000_500_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1317_.pkl'\n",
            "'w[9,4]_L_1000_500_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1318_.pkl'\n",
            "'w[9,4]_L_1000_50_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1714_.pkl'\n",
            "'w[9,4]_L_1000_50_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1716_.pkl'\n",
            "'w[9,4]_L_1000_50_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1717_.pkl'\n",
            "'w[9,4]_L_1000_50_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1718_.pkl'\n",
            "'w[9,4]_L_1000_50_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1719_.pkl'\n",
            "'w[9,4]_L_1000_50_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1250_.pkl'\n",
            "'w[9,4]_L_1000_50_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1251_.pkl'\n",
            "'w[9,4]_L_1000_50_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1253_.pkl'\n",
            "'w[9,4]_L_1000_50_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1254_.pkl'\n",
            "'w[9,4]_L_1000_50_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1256_.pkl'\n",
            "'w[9,4]_L_100_100_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1641_.pkl'\n",
            "'w[9,4]_L_100_100_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1642_.pkl'\n",
            "'w[9,4]_L_100_100_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1643_.pkl'\n",
            "'w[9,4]_L_100_100_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1644_.pkl'\n",
            "'w[9,4]_L_100_100_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1645_.pkl'\n",
            "'w[9,4]_L_100_100_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1214_.pkl'\n",
            "'w[9,4]_L_100_100_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1215_.pkl'\n",
            "'w[9,4]_L_100_100_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1217_.pkl'\n",
            "'w[9,4]_L_100_100_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1218_.pkl'\n",
            "'w[9,4]_L_100_100_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1220_.pkl'\n",
            "'w[9,4]_L_100_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1626_.pkl'\n",
            "'w[9,4]_L_100_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1628_.pkl'\n",
            "'w[9,4]_L_100_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1629_.pkl'\n",
            "'w[9,4]_L_100_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1631_.pkl'\n",
            "'w[9,4]_L_100_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1632_.pkl'\n",
            "'w[9,4]_L_100_10_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1159_.pkl'\n",
            "'w[9,4]_L_100_10_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1201_.pkl'\n",
            "'w[9,4]_L_100_10_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1202_.pkl'\n",
            "'w[9,4]_L_100_10_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1204_.pkl'\n",
            "'w[9,4]_L_100_10_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1205_.pkl'\n",
            "'w[9,4]_L_100_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1620_.pkl'\n",
            "'w[9,4]_L_100_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1622_.pkl'\n",
            "'w[9,4]_L_100_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1623_.pkl'\n",
            "'w[9,4]_L_100_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1624_.pkl'\n",
            "'w[9,4]_L_100_1_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1151_.pkl'\n",
            "'w[9,4]_L_100_1_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1153_.pkl'\n",
            "'w[9,4]_L_100_1_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1154_.pkl'\n",
            "'w[9,4]_L_100_1_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1156_.pkl'\n",
            "'w[9,4]_L_100_1_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1158_.pkl'\n",
            "'w[9,4]_L_100_200_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1646_.pkl'\n",
            "'w[9,4]_L_100_200_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1647_.pkl'\n",
            "'w[9,4]_L_100_200_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1649_.pkl'\n",
            "'w[9,4]_L_100_200_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1651_.pkl'\n",
            "'w[9,4]_L_100_200_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1652_.pkl'\n",
            "'w[9,4]_L_100_200_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1221_.pkl'\n",
            "'w[9,4]_L_100_200_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1222_.pkl'\n",
            "'w[9,4]_L_100_200_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1224_.pkl'\n",
            "'w[9,4]_L_100_200_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1226_.pkl'\n",
            "'w[9,4]_L_100_200_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1227_.pkl'\n",
            "'w[9,4]_L_100_500_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1653_.pkl'\n",
            "'w[9,4]_L_100_500_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1655_.pkl'\n",
            "'w[9,4]_L_100_500_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1656_.pkl'\n",
            "'w[9,4]_L_100_500_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1657_.pkl'\n",
            "'w[9,4]_L_100_500_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1659_.pkl'\n",
            "'w[9,4]_L_100_500_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1229_.pkl'\n",
            "'w[9,4]_L_100_500_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1231_.pkl'\n",
            "'w[9,4]_L_100_500_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1232_.pkl'\n",
            "'w[9,4]_L_100_500_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1233_.pkl'\n",
            "'w[9,4]_L_100_500_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1235_.pkl'\n",
            "'w[9,4]_L_100_50_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1633_.pkl'\n",
            "'w[9,4]_L_100_50_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1635_.pkl'\n",
            "'w[9,4]_L_100_50_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1637_.pkl'\n",
            "'w[9,4]_L_100_50_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1638_.pkl'\n",
            "'w[9,4]_L_100_50_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1639_.pkl'\n",
            "'w[9,4]_L_100_50_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1207_.pkl'\n",
            "'w[9,4]_L_100_50_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1209_.pkl'\n",
            "'w[9,4]_L_100_50_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1210_.pkl'\n",
            "'w[9,4]_L_100_50_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1212_.pkl'\n",
            "'w[9,4]_L_100_50_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1213_.pkl'\n",
            "'w[9,4]_L_10_100_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1603_.pkl'\n",
            "'w[9,4]_L_10_100_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1604_.pkl'\n",
            "'w[9,4]_L_10_100_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1605_.pkl'\n",
            "'w[9,4]_L_10_100_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1606_.pkl'\n",
            "'w[9,4]_L_10_100_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1607_.pkl'\n",
            "'w[9,4]_L_10_100_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1130_.pkl'\n",
            "'w[9,4]_L_10_100_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1131_.pkl'\n",
            "'w[9,4]_L_10_100_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1133_.pkl'\n",
            "'w[9,4]_L_10_100_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1134_.pkl'\n",
            "'w[9,4]_L_10_100_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1135_.pkl'\n",
            "'w[9,4]_L_10_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1552_.pkl'\n",
            "'w[9,4]_L_10_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1553_.pkl'\n",
            "'w[9,4]_L_10_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1554_.pkl'\n",
            "'w[9,4]_L_10_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1556_.pkl'\n",
            "'w[9,4]_L_10_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1557_.pkl'\n",
            "'w[9,4]_L_10_10_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1117_.pkl'\n",
            "'w[9,4]_L_10_10_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1119_.pkl'\n",
            "'w[9,4]_L_10_10_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1120_.pkl'\n",
            "'w[9,4]_L_10_10_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1122_.pkl'\n",
            "'w[9,4]_L_10_10_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1123_.pkl'\n",
            "'w[9,4]_L_10_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1547_.pkl'\n",
            "'w[9,4]_L_10_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1548_.pkl'\n",
            "'w[9,4]_L_10_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1549_.pkl'\n",
            "'w[9,4]_L_10_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1550_.pkl'\n",
            "'w[9,4]_L_10_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1551_.pkl'\n",
            "'w[9,4]_L_10_1_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1111_.pkl'\n",
            "'w[9,4]_L_10_1_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1113_.pkl'\n",
            "'w[9,4]_L_10_1_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1114_.pkl'\n",
            "'w[9,4]_L_10_1_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1115_.pkl'\n",
            "'w[9,4]_L_10_1_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1116_.pkl'\n",
            "'w[9,4]_L_10_200_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1608_.pkl'\n",
            "'w[9,4]_L_10_200_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1610_.pkl'\n",
            "'w[9,4]_L_10_200_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1612_.pkl'\n",
            "'w[9,4]_L_10_200_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1613_.pkl'\n",
            "'w[9,4]_L_10_200_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1614_.pkl'\n",
            "'w[9,4]_L_10_200_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1137_.pkl'\n",
            "'w[9,4]_L_10_200_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1138_.pkl'\n",
            "'w[9,4]_L_10_200_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1140_.pkl'\n",
            "'w[9,4]_L_10_200_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1141_.pkl'\n",
            "'w[9,4]_L_10_200_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1143_.pkl'\n",
            "'w[9,4]_L_10_500_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1615_.pkl'\n",
            "'w[9,4]_L_10_500_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1616_.pkl'\n",
            "'w[9,4]_L_10_500_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1617_.pkl'\n",
            "'w[9,4]_L_10_500_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1618_.pkl'\n",
            "'w[9,4]_L_10_500_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1619_.pkl'\n",
            "'w[9,4]_L_10_500_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1144_.pkl'\n",
            "'w[9,4]_L_10_500_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1145_.pkl'\n",
            "'w[9,4]_L_10_500_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1146_.pkl'\n",
            "'w[9,4]_L_10_500_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1148_.pkl'\n",
            "'w[9,4]_L_10_500_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1150_.pkl'\n",
            "'w[9,4]_L_10_50_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1558_.pkl'\n",
            "'w[9,4]_L_10_50_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1559_.pkl'\n",
            "'w[9,4]_L_10_50_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1601_.pkl'\n",
            "'w[9,4]_L_10_50_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1602_.pkl'\n",
            "'w[9,4]_L_10_50_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1124_.pkl'\n",
            "'w[9,4]_L_10_50_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1126_.pkl'\n",
            "'w[9,4]_L_10_50_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1127_.pkl'\n",
            "'w[9,4]_L_10_50_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1128_.pkl'\n",
            "'w[9,4]_L_10_50_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1129_.pkl'\n",
            "'w[9,4]_L_1_100_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1529_.pkl'\n",
            "'w[9,4]_L_1_100_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1530_.pkl'\n",
            "'w[9,4]_L_1_100_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1531_.pkl'\n",
            "'w[9,4]_L_1_100_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1532_.pkl'\n",
            "'w[9,4]_L_1_100_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1534_.pkl'\n",
            "'w[9,4]_L_1_100_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1051_.pkl'\n",
            "'w[9,4]_L_1_100_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1052_.pkl'\n",
            "'w[9,4]_L_1_100_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1054_.pkl'\n",
            "'w[9,4]_L_1_100_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1055_.pkl'\n",
            "'w[9,4]_L_1_100_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1056_.pkl'\n",
            "'w[9,4]_L_1_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1519_.pkl'\n",
            "'w[9,4]_L_1_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1520_.pkl'\n",
            "'w[9,4]_L_1_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1521_.pkl'\n",
            "'w[9,4]_L_1_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1522_.pkl'\n",
            "'w[9,4]_L_1_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1523_.pkl'\n",
            "'w[9,4]_L_1_10_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1038_.pkl'\n",
            "'w[9,4]_L_1_10_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1039_.pkl'\n",
            "'w[9,4]_L_1_10_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1041_.pkl'\n",
            "'w[9,4]_L_1_10_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1042_.pkl'\n",
            "'w[9,4]_L_1_10_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1044_.pkl'\n",
            "'w[9,4]_L_1_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1512_.pkl'\n",
            "'w[9,4]_L_1_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1514_.pkl'\n",
            "'w[9,4]_L_1_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1515_.pkl'\n",
            "'w[9,4]_L_1_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1517_.pkl'\n",
            "'w[9,4]_L_1_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1518_.pkl'\n",
            "'w[9,4]_L_1_1_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1032_.pkl'\n",
            "'w[9,4]_L_1_1_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1033_.pkl'\n",
            "'w[9,4]_L_1_1_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1034_.pkl'\n",
            "'w[9,4]_L_1_1_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1035_.pkl'\n",
            "'w[9,4]_L_1_1_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1036_.pkl'\n",
            "'w[9,4]_L_1_200_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1535_.pkl'\n",
            "'w[9,4]_L_1_200_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1536_.pkl'\n",
            "'w[9,4]_L_1_200_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1538_.pkl'\n",
            "'w[9,4]_L_1_200_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1539_.pkl'\n",
            "'w[9,4]_L_1_200_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1540_.pkl'\n",
            "'w[9,4]_L_1_200_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1058_.pkl'\n",
            "'w[9,4]_L_1_200_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1059_.pkl'\n",
            "'w[9,4]_L_1_200_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1100_.pkl'\n",
            "'w[9,4]_L_1_200_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1101_.pkl'\n",
            "'w[9,4]_L_1_200_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1102_.pkl'\n",
            "'w[9,4]_L_1_500_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1541_.pkl'\n",
            "'w[9,4]_L_1_500_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1542_.pkl'\n",
            "'w[9,4]_L_1_500_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1543_.pkl'\n",
            "'w[9,4]_L_1_500_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1544_.pkl'\n",
            "'w[9,4]_L_1_500_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1546_.pkl'\n",
            "'w[9,4]_L_1_500_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1104_.pkl'\n",
            "'w[9,4]_L_1_500_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1105_.pkl'\n",
            "'w[9,4]_L_1_500_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1107_.pkl'\n",
            "'w[9,4]_L_1_500_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1108_.pkl'\n",
            "'w[9,4]_L_1_500_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1109_.pkl'\n",
            "'w[9,4]_L_1_50_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1525_.pkl'\n",
            "'w[9,4]_L_1_50_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1526_.pkl'\n",
            "'w[9,4]_L_1_50_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1527_.pkl'\n",
            "'w[9,4]_L_1_50_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1528_.pkl'\n",
            "'w[9,4]_L_1_50_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1045_.pkl'\n",
            "'w[9,4]_L_1_50_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1046_.pkl'\n",
            "'w[9,4]_L_1_50_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1047_.pkl'\n",
            "'w[9,4]_L_1_50_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1049_.pkl'\n",
            "'w[9,4]_L_1_50_1_1_1_E_1_1_1_1_50_P_0_0_0_0_10__2023_03_22_1050_.pkl'\n",
            "1 .  w[9,4]_L_1_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1512_.pkl  was saved to dict_of_run_files. \n",
            "2 .  w[9,4]_L_1_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1514_.pkl  was saved to dict_of_run_files. \n",
            "3 .  w[9,4]_L_1_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1515_.pkl  was saved to dict_of_run_files. \n",
            "4 .  w[9,4]_L_1_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1517_.pkl  was saved to dict_of_run_files. \n",
            "5 .  w[9,4]_L_1_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1518_.pkl  was saved to dict_of_run_files. \n",
            "6 .  w[9,4]_L_1_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1519_.pkl  was saved to dict_of_run_files. \n",
            "7 .  w[9,4]_L_1_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1520_.pkl  was saved to dict_of_run_files. \n",
            "8 .  w[9,4]_L_1_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1521_.pkl  was saved to dict_of_run_files. \n",
            "9 .  w[9,4]_L_1_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1522_.pkl  was saved to dict_of_run_files. \n",
            "10 .  w[9,4]_L_1_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1523_.pkl  was saved to dict_of_run_files. \n",
            "11 .  w[9,4]_L_1_50_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1525_.pkl  was saved to dict_of_run_files. \n",
            "12 .  w[9,4]_L_1_50_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1526_.pkl  was saved to dict_of_run_files. \n",
            "13 .  w[9,4]_L_1_50_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1527_.pkl  was saved to dict_of_run_files. \n",
            "14 .  w[9,4]_L_1_50_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1528_.pkl  was saved to dict_of_run_files. \n",
            "15 .  w[9,4]_L_1_100_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1529_.pkl  was saved to dict_of_run_files. \n",
            "16 .  w[9,4]_L_1_100_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1530_.pkl  was saved to dict_of_run_files. \n",
            "17 .  w[9,4]_L_1_100_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1531_.pkl  was saved to dict_of_run_files. \n",
            "18 .  w[9,4]_L_1_100_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1532_.pkl  was saved to dict_of_run_files. \n",
            "19 .  w[9,4]_L_1_100_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1534_.pkl  was saved to dict_of_run_files. \n",
            "20 .  w[9,4]_L_1_200_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1535_.pkl  was saved to dict_of_run_files. \n",
            "21 .  w[9,4]_L_1_200_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1536_.pkl  was saved to dict_of_run_files. \n",
            "22 .  w[9,4]_L_1_200_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1538_.pkl  was saved to dict_of_run_files. \n",
            "23 .  w[9,4]_L_1_200_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1539_.pkl  was saved to dict_of_run_files. \n",
            "24 .  w[9,4]_L_1_200_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1540_.pkl  was saved to dict_of_run_files. \n",
            "25 .  w[9,4]_L_1_500_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1541_.pkl  was saved to dict_of_run_files. \n",
            "26 .  w[9,4]_L_1_500_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1542_.pkl  was saved to dict_of_run_files. \n",
            "27 .  w[9,4]_L_1_500_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1543_.pkl  was saved to dict_of_run_files. \n",
            "28 .  w[9,4]_L_1_500_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1544_.pkl  was saved to dict_of_run_files. \n",
            "29 .  w[9,4]_L_1_500_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1546_.pkl  was saved to dict_of_run_files. \n",
            "30 .  w[9,4]_L_10_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1547_.pkl  was saved to dict_of_run_files. \n",
            "31 .  w[9,4]_L_10_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1548_.pkl  was saved to dict_of_run_files. \n",
            "32 .  w[9,4]_L_10_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1549_.pkl  was saved to dict_of_run_files. \n",
            "33 .  w[9,4]_L_10_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1550_.pkl  was saved to dict_of_run_files. \n",
            "34 .  w[9,4]_L_10_1_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1551_.pkl  was saved to dict_of_run_files. \n",
            "35 .  w[9,4]_L_10_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1552_.pkl  was saved to dict_of_run_files. \n",
            "36 .  w[9,4]_L_10_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1553_.pkl  was saved to dict_of_run_files. \n",
            "37 .  w[9,4]_L_10_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1554_.pkl  was saved to dict_of_run_files. \n",
            "38 .  w[9,4]_L_10_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1556_.pkl  was saved to dict_of_run_files. \n",
            "39 .  w[9,4]_L_10_10_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1557_.pkl  was saved to dict_of_run_files. \n",
            "40 .  w[9,4]_L_10_50_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1558_.pkl  was saved to dict_of_run_files. \n",
            "41 .  w[9,4]_L_10_50_1_1_1_E_1_1_1_1_25_P_0_0_0_0_10__2023_03_21_1559_.pkl  was saved to dict_of_run_files. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to get into the \"dict_of_run_files\" variable and dig out each of the contents we need in each pickle file.\n",
        "\n",
        "We need at least the following information in a dataframe:"
      ],
      "metadata": {
        "id": "TMObembs7Ayj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "UniqueID | Date_Time |  Run | Rep | Lambda = (i,j,k) | Num of Epochs = (E1, E2, E3) | EarlyStop Patience = (P1, P2, P3) | Train/Val Accuracy | # of Misclassifications of [9t, 4p] | # Reverse Misclassifications | Final CM | Seed1 | Seed2 \n",
        "-- | -- | --| --| --| --| --| --| -- | --| --| --| --\n",
        "int | string | int | int | tuple | tuple | tuple | float | int | int | (1,100) (1,100), (1,100) | int | int\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VoLDrqp_6wjk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unique ID can come later from just the order of the index or the key-name-of the file since each one has a unique last file_name.\n",
        "\n",
        "Date_Time can come from the last part of the file name string, although in hind sight would have been nice to save to the admin key\n",
        "\n",
        "Run really can't be a trusted Run Number anymore since I had some trouble with it, but can be cleaned up based off the lambda-epochs-patience values.\n",
        "\n",
        "Rep is Rep specified in the Dictionary.\n",
        "\n"
      ],
      "metadata": {
        "id": "lC5sDUf27XqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dict_of_run_files.keys()\n",
        "# An example of a key is 'run1_rep_1_w[9,4]_L_1_1_1_E_5_5_25_P_0_0_25__2023_03_09_1329_.pkl'\n",
        "\n",
        "import pandas as pd\n",
        "data = pd.DataFrame()\n",
        "\n",
        "# I later had the great idea of adding an admin file so some files have it some don't.\n",
        "\n",
        "count = 0\n",
        "for file in os.listdir(path):\n",
        "  if file.endswith(\".pkl\"):\n",
        "    # for key in dict_of_run_files.keys():\n",
        "      try:\n",
        "        dict_of_run_files[file][list(dict_of_run_files[file].keys())[0]]['admin']\n",
        "      except:\n",
        "        print(count, \"No Admin Key\")\n",
        "        data[count] = [file, dict_of_run_files[file][list(dict_of_run_files[file].keys())[0]]]\n",
        "      else:\n",
        "      # for variable in list((\"lambda1\", \"lambda2\", \"lambda3\")):\n",
        "      #   # print(variable)\n",
        "        data[count] = [file, dict_of_run_files[file][list(dict_of_run_files[file].keys())[0]]]\n",
        "      count+=1\n",
        "print(data.head)\n",
        "\n",
        "data_transpose = data.transpose()\n",
        "del(data)\n",
        "file_to_delete = path+ 'delete_later.pkl'\n",
        "del('/content/drive/MyDrive/Con3/delete_later.pkl')\n",
        "\n",
        "# data_transpose.columns\n",
        "\n",
        "# The try block lets you test a block of code for errors.\n",
        "# The except block lets you handle the error.\n",
        "# The else block lets you execute code when there is no error.\n",
        "# The finally block lets you execute code, regardless of the result of the try- and except blocks.\n",
        "\n",
        "# #This file does:\n",
        "# print(\"Run:\" , dict_of_run_files['run23_rep_1_w[9,4]_L_100_1_100_E_5_5_25_P_0_0_0__2023_03_09_1434_.pkl'][(23,1)]['admin']['run'])\n",
        "# print(\"Rep:\" , dict_of_run_files['run23_rep_1_w[9,4]_L_100_1_100_E_5_5_25_P_0_0_0__2023_03_09_1434_.pkl'][(23,1)]['admin']['rep'])\n",
        "\n",
        "#so when filling in my dataframe with information I may need to use a for loop with an try-exception line.\n",
        "\n"
      ],
      "metadata": {
        "id": "2HeQTBi8aWPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # data_transpose[0] #the \"0\" is the first column which is now the filename\n",
        "# data_transpose[1][0] #index 0 of column 1 is all the information saved from the pickle file, minus the \"key\". it still appears to be in dictionary format\n",
        "# data_transpose[1][0][\"First\"]  # this is the \"first\" training phase history\n",
        "data = pd.DataFrame()\n",
        "data['categorical_accuracy'] = np.ones((2,2)).tolist()\n",
        "data['val_categorical_accuracy'] =np.ones((2,2)).tolist()\n",
        "data['loss'] = np.ones((2,2)).tolist()\n",
        "data['val_loss'] = np.ones((2,2)).tolist()\n",
        "data['4T_9P'] = np.ones((2,2)).tolist()\n",
        "data['9T_4P'] =np.ones((2,2)).tolist()\n",
        "data['3ConfusionMatrices'] =np.ones((2,2)).tolist()\n",
        "\n",
        "\n",
        "\n",
        "# Create a list of columns for the data-frame Data where you can no longer subset into smaller keys\n",
        "# list_of_columns_for_data = []\n",
        "#gather all the unique keys in the many nested dictionary files to define a new dataframe with those columns\n",
        "for row in range(0,len(data_transpose)):\n",
        "  data.at[row, 'filename'] = data_transpose[0][row]\n",
        "  for column in list(data_transpose[1][row].keys()):\n",
        "    try:\n",
        "      len(list(data_transpose[1][row][column].keys()))\n",
        "    except AttributeError:\n",
        "      # print(f'data_transpose[1][{row}][{column}] has no keys')\n",
        "      # list_of_columns_for_data.append(column)\n",
        "      if(column=='3ConfusionMatrices'):\n",
        "        data.at[row, column] = pd.DataFrame([data_transpose[1][row][column].tolist()])\n",
        "      else:\n",
        "        data.at[row, column] = data_transpose[1][row][column]\n",
        "\n",
        "\n",
        "    else:\n",
        "      # print(f'data_transpose[1][{row}][{column}] has keys')\n",
        "\n",
        "      for subcolumn in list(data_transpose[1][row][column].keys()):\n",
        "        # list_of_columns_for_data.append(subcolumn)\n",
        "        # print(f'key: subcolumn {subcolumn}')\n",
        "        try:\n",
        "          data.at[row, subcolumn] = data_transpose[1][row][column][subcolumn]\n",
        "        except KeyError:\n",
        "          # print(\"KeyError\")\n",
        "          data.at[row, subcolumn] = pd.DataFrame([data_transpose[1][row][column][subcolumn].tolist()])\n",
        "        except TypeError:\n",
        "          # print(\"TypeError\")\n",
        "          data.at[row, subcolumn] = pd.DataFrame([data_transpose[1][row][column][subcolumn].tolist()])\n",
        "        except ValueError:  #categorical accuracy causes a value error because you can't set the dataframe value to an array\n",
        "          # print(\"ValueError\")\n",
        "          data.at[row, subcolumn] = data_transpose[1][row][column][subcolumn]\n",
        "        # data.at[row, subcolumn] = data_transpose[1][row][column][subcolumn]\n",
        "\n",
        "        #I also want the last value listed in '4T_9P', '9P_4P', 'val_loss',  'val_categorical_accuracy',  'loss',  'val_loss',\n",
        "        if(subcolumn == '4T_9P' ):\n",
        "          # print(str(subcolumn),\" last_value is \" , data_transpose[1][row][column][subcolumn][-1])\n",
        "          new_column_name = str(subcolumn) + \"_last_value\"\n",
        "          data.at[row, new_column_name] = data_transpose[1][row][column][subcolumn][-1]\n",
        "          new_column_name2 = str(subcolumn) + \"_length\"\n",
        "          data.at[row, new_column_name2] = len(data_transpose[1][row][column][subcolumn])\n",
        "          \n",
        "        elif(subcolumn == '9T_4P' ):\n",
        "          # print(str(subcolumn),\" last_value is \" , data_transpose[1][row][column][subcolumn][-1])\n",
        "          new_column_name = str(subcolumn) + \"_last_value\"\n",
        "          data.at[row, new_column_name] = data_transpose[1][row][column][subcolumn][-1]\n",
        "          new_column_name2 = str(subcolumn) + \"_length\"\n",
        "          data.at[row, new_column_name2] = len(data_transpose[1][row][column][subcolumn])\n",
        "\n",
        "        elif(subcolumn == 'val_loss' ):\n",
        "          # print(str(subcolumn),\" last_value is \" , data_transpose[1][row][column][subcolumn][-1])\n",
        "          new_column_name = str(subcolumn) + \"_last_value\"\n",
        "          data.at[row, new_column_name] = data_transpose[1][row][column][subcolumn][-1]\n",
        "          new_column_name2 = str(subcolumn) + \"_length\"\n",
        "          data.at[row, new_column_name2] = len(data_transpose[1][row][column][subcolumn])\n",
        "        elif(subcolumn == 'val_categorical_accuracy' ):\n",
        "          # print(str(subcolumn),\" last_value is \" , data_transpose[1][row][column][subcolumn][-1])\n",
        "          new_column_name = str(subcolumn) + \"_last_value\"\n",
        "          data.at[row, new_column_name] = data_transpose[1][row][column][subcolumn][-1]\n",
        "          new_column_name2 = str(subcolumn) + \"_length\"\n",
        "          data.at[row, new_column_name2] = len(data_transpose[1][row][column][subcolumn])\n",
        "        elif(subcolumn == 'loss' ):\n",
        "          # print(str(subcolumn),\" last_value is \" , data_transpose[1][row][column][subcolumn][-1])\n",
        "          new_column_name = str(subcolumn) + \"_last_value\"\n",
        "          data.at[row, new_column_name] = data_transpose[1][row][column][subcolumn][-1]\n",
        "          new_column_name2 = str(subcolumn) + \"_length\"\n",
        "          data.at[row, new_column_name2] = len(data_transpose[1][row][column][subcolumn])\n",
        "        elif(subcolumn == 'val_loss' ):\n",
        "          # print(str(subcolumn),\" last_value is \" , data_transpose[1][row][column][subcolumn][-1])\n",
        "          new_column_name = str(subcolumn) + \"_last_value\"\n",
        "          data.at[row, new_column_name] = data_transpose[1][row][column][subcolumn][-1]\n",
        "          new_column_name2 = str(subcolumn) + \"_length\"\n",
        "          data.at[row, new_column_name2] = len(data_transpose[1][row][column][subcolumn])\n",
        "\n",
        "        # print(subcolumn)\n",
        "\n",
        "# # data\n",
        "\n",
        "\n",
        "\n",
        "# #Create a column with just the last confusion matrix, the last values of 9t_4p, and 4t_9p\n",
        "# len(data['categorical_accuracy'][0])  #Column 'Categorical Accuracy', Row0, shows 35 values\n",
        "# data['categorical_accuracy'][0][-1]\n",
        "# data['val_categorical_accuracy'][0][-1]\n",
        "# len(data['9T_4P'][0])  #The array of values for '9T_4P' is length 35, and the last value is 12 \n",
        "# data['9T_4P'][0][-1]  # to get the last value for row 0\n",
        " \n",
        "    \n",
        "# data_csv = data.to_csv(\"data.csv\")\n",
        "  \n",
        "# from google.colab import files\n",
        "# files.download('data.csv')\n",
        "\n",
        "# data.drop_duplicates(subset=['3ConfusionMatrices'][0]) #fails TypeError 'unhashable type: numpy.ndarray'\n",
        "# https://stackoverflow.com/questions/43855462/pandas-drop-duplicates-method-not-working-on-dataframe-containing-lists\n",
        "\n",
        "data = data.drop(columns=['filename', '3ConfusionMatrices', 'First', 'Second', 'Third'])\n",
        " \n",
        "data.loc[data.astype(str).drop_duplicates().index]\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "sh4P_lYIyEMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_to_save = pd.DataFrame()\n",
        "data_to_save = data[['lambda1',\t'lambda2',\t'lambda3',\t'epochs1',\t'epochs2',\t'epochs3',\t'es1',\t'es2',\t'es3', '9T_4P_last_value',\t'4T_9P_last_value', 'val_categorical_accuracy_last_value']].copy()\n",
        "\n",
        "for row in range(len(data_to_save)):\n",
        "  # print(row)\n",
        "  try:\n",
        "    data_to_save.loc[row, 'lambdacombo'] = str(int(data_to_save.loc[row, 'lambda1'])) +\"-\"+ str(int(data_to_save.loc[row, 'lambda2'])) +\"-\"+ str(int(data_to_save.loc[row, 'lambda3']))\n",
        "  except ValueError:\n",
        "    data_to_save.loc[row, 'lambdacombo'] = str((data_to_save.loc[row, 'lambda1'])) +\"-\"+ str((data_to_save.loc[row, 'lambda2'])) +\"-\"+ str((data_to_save.loc[row, 'lambda3']))                               \n",
        "\n",
        "data_to_save['lambdacombo'].value_counts()\n",
        "data_csv = data_to_save.to_csv(\"data.csv\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download('data.csv')"
      ],
      "metadata": {
        "id": "YRqszN9qXjVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data_to_save = pd.read_csv('data.csv')"
      ],
      "metadata": {
        "id": "JT_kYs0Bi1_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LambdaCombo Variable Column\n",
        "\n",
        "\"LambdaCombo\" is a column to make all three-phases into a unique string value, so that the 27-unique combos can be easily identified.  These are the unique \"runs\".  I can't trust that each run is numbered to identify the lambdas correctly, so I use the lambda-combo columns. "
      ],
      "metadata": {
        "id": "FgAgDKQprH6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_to_save.head()\n",
        "data_to_save.lambdacombo.value_counts()\n",
        "data_to_save.lambdacombo.value_counts().index"
      ],
      "metadata": {
        "id": "qjT2hqJNJG4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dependent variable of interest is the misclassifications of highest occurence, which in originally running the \"lousy\" neural network of 40 dense connections was misidentifying a 9 as a 4.  Because after 30 runs of just baseline costs (1-1-1) this was the highest misclassification, this became the objective to minimize in creating higher and higher costs for misclassifying a 9 as a 4.  This could also be called the most \"hard\" classification, since it was naturally the most occuring. The dataframe of saved data is sorted by the misclassification rate to enable easier viewing of groups in the following graph "
      ],
      "metadata": {
        "id": "OJaSbgevriec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the required module\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# data_to_save.sort_values('9T_4P_last_value', inplace=True)\n",
        "data_to_save.sort_values('lambdacombo', inplace=True)\n",
        "\n",
        "# # x axis values\n",
        "x = data_to_save['lambdacombo']\n",
        "# corresponding y axis values\n",
        "y = data_to_save['9T_4P_last_value']\n",
        "\n",
        "plt.figure(figsize=(35,5))\n",
        "# plotting the points \n",
        "plt.scatter(x, y)\n",
        "  \n",
        "# naming the x axis\n",
        "plt.xlabel('Type of Training')\n",
        "# naming the y axis\n",
        "plt.ylabel('Misclassifications')\n",
        "  \n",
        "# giving a title to my graph\n",
        "plt.title('Number of Misclassifications versus the Type of Training')\n",
        "  \n",
        "# function to show the plot\n",
        "plt.show() "
      ],
      "metadata": {
        "id": "EyjS_rPkAlKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the lowest misclassifications happen with the highest-cost of misclassification in the first phase.   These average around 2.   The next group of misclassifications happens around when the cost of misclassification is '100' in the first phase.  And finally the last group of misclassifications, which is the highest overall, is in the cost of misclassifications that are '1' in the initial phase.  It doesn't apppear that the second phase makes a large difference overall in any of the groups.  For example 1-1-1, which is a constant rate of '1' cost seems to have the same results   as other runs where the second phase was 1000."
      ],
      "metadata": {
        "id": "WtBMbIRQsOtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2, ax3) = plt.subplots(3, figsize=(15,8), sharex=True)\n",
        "fig.suptitle('Overview of three-variables means and lambda-combos')\n",
        "plt.xticks(rotation=90)\n",
        "  \n",
        "# ax1.scatter(x, y1, c = \"b\", marker='^')\n",
        "ax1.scatter(data_to_save['lambdacombo'], data_to_save['9T_4P_last_value'], marker='.')\n",
        "ax1.set_title('Num of 9 Misclassified as 4s (concern)')\n",
        "\n",
        "# ax2.scatter(x, y2, c=\"orange\")\n",
        "ax2.set_title('Num of 4s Misclassified as 9s (reverse concern)')\n",
        "ax2.scatter(data_to_save['lambdacombo'], data_to_save['4T_9P_last_value'], marker='.')\n",
        " \n",
        "# ax3.scatter(x, y3, c='green')\n",
        "ax3.scatter(data_to_save['lambdacombo'], data_to_save['val_categorical_accuracy_last_value'], marker='.')\n",
        "ax3.set_title('Val Categorical Accuracy')\n",
        "\n",
        "ax1.tick_params(axis='x', labelrotation=90)\n",
        "ax2.tick_params(axis='x', labelrotation=90)\n",
        "ax3.tick_params(axis='x', labelrotation=90)\n",
        "  "
      ],
      "metadata": {
        "id": "awtwxAD0TcN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A look at the above data with just the averages is below.  You can visually see there are no real differences between the averages for each inital phase of the data."
      ],
      "metadata": {
        "id": "mOtJctGAtSRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data_to_save.groupby('lambdacombo')['9T_4P_last_value','val_categorical_accuracy_last_value'].describe()\n",
        "data_combo_summary = data_to_save.groupby('lambdacombo')['9T_4P_last_value','val_categorical_accuracy_last_value', '4T_9P_last_value'].mean()\n",
        "data_combo_summary.sort_values('9T_4P_last_value', inplace=True)\n",
        "\n",
        "# # x axis values\n",
        "x = data_combo_summary.index\n",
        "\n",
        "# corresponding y axis values\n",
        "y1 = data_combo_summary['9T_4P_last_value']\n",
        "y2 = data_combo_summary['4T_9P_last_value']\n",
        "y3 = data_combo_summary['val_categorical_accuracy_last_value']\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(3, figsize=(16,8), sharex=True)\n",
        "fig.suptitle('Overview of three-variables means and lambda-combos')\n",
        "plt.xticks(rotation=90)\n",
        "  \n",
        "# ax1.scatter(x, y1, c = \"b\", marker='^')\n",
        "ax1.scatter(x, y1, marker='.')\n",
        "ax1.set_title('Average Num of 9s Misclassified as 4s (concern)')\n",
        "\n",
        "# ax2.scatter(x, y2, c=\"orange\")\n",
        "ax2.set_title('Average Num of 4s Misclassified as 9s (reverse concern)')\n",
        "ax2.scatter(x, y2, marker='.')\n",
        " \n",
        "# ax3.scatter(x, y3, c='green')\n",
        "ax3.scatter(x,y3, marker='.')\n",
        "ax3.set_title('Average Val Categorical Accuracy')\n",
        "\n",
        "ax1.tick_params(axis='x', labelrotation=90)\n",
        "ax2.tick_params(axis='x', labelrotation=90)\n",
        "ax3.tick_params(axis='x', labelrotation=90)\n",
        "  "
      ],
      "metadata": {
        "id": "F9Jb3WKWt7zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pandas.core.ops import rand_\n",
        "\n",
        "## Now to change this to the Tradeoff Graphs I made but somehow didn't save =O\n",
        "data_to_save.sort_values('lambdacombo', inplace=True, ignore_index=True)\n",
        "\n",
        "# groups = data_to_save.groupby('lambdacombo')\n",
        "\n",
        "def narrow_data(i):\n",
        "    # Define narrow area of dataframe to graph based off \"i\" the phase-1 value\n",
        "    df = data_to_save.where(data_to_save['lambda1']==i).dropna()\n",
        "    # df = df.where(data_to_save['lambda2']==i).dropna()\n",
        "    # df = df.where(data_to_save['lambda3']==i).dropna()\n",
        "    return df\n",
        "\n",
        "def plot_data(df, x,y,ax, lambdas, title):\n",
        "  # ax.scatter(x,y, marker='.', )\n",
        "  # original_lambda_combo_string = 'nan-nan-nan'\n",
        "\n",
        "  for j in (df[lambdas].index):\n",
        "      # print(\"j:\", j)\n",
        "      # colors = ['black', 'orange', 'blue']\n",
        "      # df = data_to_save.where(data_to_save['lambda1']==i).dropna()\n",
        "      # print(\"df[x][j]:\", df[x][j], )\n",
        "      # print(\"df[y][j]:\", df[y][j], )\n",
        "      # print(\"df['lambda2'][j]:\", df['lambda2'][j], )\n",
        "      # print(\"annotate text:\", str(df[lambdas][j]))\n",
        "\n",
        "      # ax.scatter(df[x][j]+0*np.random.random(), df[y][j], marker='.', c=df['lambda2'][j]) \n",
        "      ax.annotate(text=str(df[lambdas][j]),  xy=(df[x][j], df[y][j]), textcoords=\"offset points\", \n",
        "                  xytext=(0+0*np.random.randint(-10,10),10+0*np.random.randint(-10,10)), \n",
        "                  ha='center', size=12)\n",
        "      # original_lambda_combo_string = lambdas[j]\n",
        "  ax.set_xlabel(str(x))\n",
        "  ax.set_ylabel(str(y))\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "  groups = df.groupby('lambdacombo')\n",
        "  for name, group in groups:\n",
        "    # print('name: ', name, 'group: ' , group)  \n",
        "    ax.plot(group[x], group[y], marker='o', linestyle='', markersize=12, label=name)\n",
        "  ax.set_title(title)\n",
        "  ax.legend()\n",
        "\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(25,10))\n",
        "fig.suptitle('Overview of three-variables means and lambda-combos')\n",
        "# plt.xticks(rotation=90)\n",
        "\n",
        "# list_of_phase_1_values = [1, 100, 1000]\n",
        "list_of_phase_1_values = [1]\n",
        "\n",
        "for i in (list_of_phase_1_values):\n",
        "  df = narrow_data(i=i)\n",
        "\n",
        "# df =   data_to_save\n",
        "plot_data(df=df , x='9T_4P_last_value', y='val_categorical_accuracy_last_value', ax=ax1, lambdas='lambdacombo', title='Tradeoff btwn Misclassification and Accuracy')\n",
        "plot_data(df=df, x='4T_9P_last_value',y='val_categorical_accuracy_last_value',ax=ax2, lambdas='lambdacombo', title='Tradeoff btwn Reverse-Misclassification and Accuracy')\n",
        "plot_data(df=df, x='9T_4P_last_value',y='4T_9P_last_value',ax=ax3, lambdas='lambdacombo', title='Tradeoff btwn Misclass and Reverse-Misclass')\n",
        "plt.legend()\n",
        " "
      ],
      "metadata": {
        "id": "qHSU3OVQIyFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It doesn't seem to matter what you do in the middle or later phases according to this experiment.   So more to follow is to see if there is a difference with middle and late phase training.  Sequential testing in JMP does show that lambda1 is most significant.  Now what?"
      ],
      "metadata": {
        "id": "hKoBIeBYxuYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# thoughts\n"
      ],
      "metadata": {
        "id": "Crz9686EaRxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save a file to myDrive/con3"
      ],
      "metadata": {
        "id": "8My0DZuBOBG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyze \n",
        "\n",
        "\\\\\n"
      ],
      "metadata": {
        "id": "NP_kxNkhn6Fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# empty_cm = np.zeros((10,10))  \n",
        "# empty_cm=pd.DataFrame(empty_cm)\n",
        "\n",
        "# empty_cm.columns = ['0p', '1p', '2p', '3p', '4p', '5p', '6p', '7p', '8p', '9p']\n",
        "# empty_cm.index = ['0t', '1t', '2t', '3t', '4t', '5t', '6t', '7t', '8t', '9t']\n",
        "\n",
        "# # print(myvar_cm_average)\n",
        "\n",
        "# empty_cm_array = np.asarray(empty_cm)\n",
        "# empty_cm_array_1_100 = np.reshape(empty_cm_array,(1,100))\n",
        "# # print(cm_average_array)\n",
        "\n",
        "# df = empty_cm\n",
        "# df_new = pd.DataFrame(empty_cm_array_1_100,  columns=pd.MultiIndex.from_product([ df.index,df.columns]))\n",
        "# df_new.columns.to_flat_index()\n",
        "# df_new.columns   = ['_'.join(col) for col in df_new.columns.values]\n",
        "\n",
        "# # Now convert combined_cms of size 30x100 to a panda dataframe\n",
        "# cms_df = pd.DataFrame(combined_cms, columns=[df_new.columns], index=[\"First\", \"Second\", \"Third\"])\n",
        "\n",
        "# cms_df\n",
        "\n",
        "# for run in runs:\n",
        "#   for rep in reps:\n",
        "#     for phase in phases:\n",
        "#       pd.DataFrame(combined_history_dictionary[run,rep][\"3ConfusionMatrices\"], columns=[df_new.columns], index=[\"First\", \"Second\", \"Third\"])"
      ],
      "metadata": {
        "id": "kLrNJE0s53pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combined_cms[0,]"
      ],
      "metadata": {
        "id": "eo_ahxwTVI0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combined_cms_df[\"9t_4p\"]"
      ],
      "metadata": {
        "id": "HgPClcrAUdxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# np.average(combined_cms_df[\"9t_4p\"])"
      ],
      "metadata": {
        "id": "TY59hUTadVyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# csv_filename = file_name[:-4] + \".csv\"\n",
        "\n",
        "# combined_cms_df.to_csv(csv_filename)\n",
        "# # \n",
        "\n",
        "# from google.colab import files\n",
        "# files.download(csv_filename )\n",
        "\n",
        "# print(\"Downloading \", csv_filename , \" of shape \", combined_cms_df.shape)"
      ],
      "metadata": {
        "id": "ceUFAr_z9xsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.DataFrame(sum(var)/len(var), columns=[\"Values\"]) \n",
        "# # print(df)\n",
        "\n",
        "# df.style.format({\n",
        "#   'Values': lambda val: f'{val:,.2f}',\n",
        "# })\n",
        "\n",
        "# (df.sort_values(by=\"Values\", ascending=False)[0:20])\n",
        "\n",
        "\n",
        "# df_sorted = df.sort_values(by=\"Values\", ascending=False)[10:]  #the top 10 are usually diagonal\n",
        "\n",
        "\n",
        "# df_sorted.style.format({\n",
        "#   'Values': lambda val: f'{val:,.2f}',\n",
        "# })\n",
        "\n",
        "# import math\n",
        "\n",
        "# print(\"On average...\")\n",
        "# print(\"Num 1 misclassifications are misclassifying a \", math.floor((df_sorted[\"Values\"].index[0])/10), \" as a \", df_sorted[\"Values\"].index[0]%10, \"  (\", (df_sorted[\"Values\"].values[0]), \" times)\" )\n",
        "# print(\"Num 2 misclassifications are misclassifying a \", math.floor((df_sorted[\"Values\"].index[1])/10), \" as a \", df_sorted[\"Values\"].index[1]%10, \"  (\", (df_sorted[\"Values\"].values[1]), \" times)\" )\n",
        "# print(\"Num 3 misclassifications are misclassifying a \", math.floor((df_sorted[\"Values\"].index[2])/10), \" as a \", df_sorted[\"Values\"].index[2]%10, \"  (\", (df_sorted[\"Values\"].values[2]), \" times)\" )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-pGNLuE8gNrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_percents = pd.DataFrame( ((sum(var)*100/len(var)).reshape((10,10))/truth_num_per_category).reshape((100)), columns = [\"Values\"])\n",
        "\n",
        "\n",
        "# df_sorted_percents = df_percents.sort_values(by=\"Values\", ascending=False)[10:]  #the top 10 are usually diagonal\n",
        "\n",
        "# df_sorted_percents.style.format({\n",
        "#   'Values': lambda val: f'{val:,.2f}',\n",
        "# })\n",
        "\n",
        "# print(\"On average .. \")\n",
        "# print(\"Num 1 percent misclassifications\", math.floor((df_sorted_percents[\"Values\"].index[0])/10), \" as \", df_sorted_percents[\"Values\"].index[0]%10, (df_sorted_percents[\"Values\"].values[0]), \" percent\" )\n",
        "# print(\"Num 2 percent misclassifications\", math.floor((df_sorted_percents[\"Values\"].index[1])/10), \" as \", df_sorted_percents[\"Values\"].index[1]%10,  (df_sorted_percents[\"Values\"].values[1]), \" percent\" )\n",
        "# print(\"Num 3 percent misclassifications\", math.floor((df_sorted_percents[\"Values\"].index[2])/10), \" as \", df_sorted_percents[\"Values\"].index[2]%10, (df_sorted_percents[\"Values\"].values[2]), \" percent\" )\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AKYclir2p8wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "First column name  | Second column name \n",
        "-------------------|------------------\n",
        "Row 1, Col 1       | Row 1, Col 2 \n",
        "Row 2, Col 1       | Row 2, Col 2 "
      ],
      "metadata": {
        "id": "ltYFvZnJuW5v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VZbhS1xhBwUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count  = 0\n",
        "\n",
        "for a in range(1,1000+1, 200):\n",
        "  # print(a, end='')\n",
        "  for b in range(1,1000+1, 200):\n",
        "    # print(b, end='')\n",
        "    for c in range(1,1000+1, 200):\n",
        "      # print(a,b,c)\n",
        "      for d in range(1,1000+1, 200):\n",
        "        print(a, b,c, d)\n",
        "        count+=1\n",
        "\n",
        "\n",
        "print(count) \n",
        "\n",
        "print(count, \"minutes\", count/24, \"hours\")"
      ],
      "metadata": {
        "id": "Paesjh6xjjfl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}