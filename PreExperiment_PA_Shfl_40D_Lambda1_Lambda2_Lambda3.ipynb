{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkrD1a9fxLFZjlz/p6rePv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RachelRamirez/misclassification_matrix/blob/main/PreExperiment_PA_Shfl_40D_Lambda1_Lambda2_Lambda3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PreExperiment PA Shfl 40D Lambda1 Lambda2 Lambda3\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_t4Z4Nq-gQ26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In prior notebooks, I experimented with one variable; lambda, the value the cost-matrix takes on for whatever misclassification we are interested in.  The seeded neural network we trained using a normal cost-matrix of all ones usually had a higher misclassification count on the **T**rue 9s being mislabelled by the model **P**redicting 4s, which you will sometimes see written as \"9T, 4P\" and \"w[9t,4p]\" when referring to the cost of the misclassification.\n",
        "\n",
        "We changed the lambda values and allowed as many epochs of training with  early-stopping and a patience of 10 to conclude once training was beginnnig to overfit the validation data (also the test set).   Lambda values chosen were 1, 10, 100, and 1000.   These values were chosen because it seemed that only very high values like 1000 could drive down the number of misclassifications of 9T,4P to zero.\n",
        "\n",
        "In this notebook, continuing to use a relatively balanced dataset, we're going to also experiment with a second variable, time of training.  This is a little more tricky to define because the variable lambda is thought to have an impact on the number of epochs needed.  Therefore if you have a certain lambda value in the beginning of training, you may only need 10 total epochs of training, but if you use it at the end of training, you may need more epochs of training.  In addition there is the question as to whether we should define   a static number of epochs or to allow early stopping.  \n",
        "\n",
        "As an example:\n",
        "\n",
        "\n",
        "|Lambda | Rep 1 Epochs | Rep 2 Epochs |  Rep 3 | Rep 4 |\tRep 5 |\tRep 6\t| Rep 7\t| Rep 8\t|Rep 9\t| Rep 10 |  Average of 10 Reps |\n",
        "|-- | -- | --- |  --| -- |  --| -- |  --| -- |  --| -- |  --| \n",
        "1\t|17|19|19|\t22|\t17\t|21\t|19\t|22\t|19|\t22|\t... 19*\n",
        "100 |14|14|14|\t11|\t14\t|14\t|14\t|14\t|14|\t14|\t... 13.7*\n",
        "1000:\t|18|23|22|\t35|\t7\t|25\t|21\t|19\t|23|\t24|\t... 21.7\n",
        "\n",
        "*However 1 and 100 were done with patience of 3, whereas 1000 was done with patience of 10 and more epochs.  Rerunning the 1's with the same patience appears to up the number of epochs til early stopping to mid-20s.  \n",
        "\n",
        "\n",
        "We'll test the conditions for using a static training cycle first, because it's easier to code and report on, and then test how to do a dynamic training cycle, \n"
      ],
      "metadata": {
        "id": "5u6fp0kHg4tG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reduced  capacity neural network with two layers of 40 connections\n",
        "\n",
        "  Filename \"PA_Shfl_w[9,4]_2.0_40D_Misclassification_Cost_Matrix_Example\""
      ],
      "metadata": {
        "id": "NqL4ADhFLsqG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check timezone if incorrect restart"
      ],
      "metadata": {
        "id": "l4sBTo67FC9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How to change the local time in Google Colab\n",
        "!rm /etc/localtime\n",
        "!ln -s /usr/share/zoneinfo/US/Eastern /etc/localtime\n",
        "!date\n",
        "\n",
        "#If this doesn't show the local time correctly, then you need to restart.\n",
        "import time\n",
        "time.localtime(time.time())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4I5Wm-6EPYR",
        "outputId": "da49a438-7dad-4394-e271-abcee13ca6ea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu 09 Mar 2023 07:55:32 AM EST\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "time.struct_time(tm_year=2023, tm_mon=3, tm_mday=9, tm_hour=12, tm_min=55, tm_sec=32, tm_wday=3, tm_yday=68, tm_isdst=0)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reproducible Seeds"
      ],
      "metadata": {
        "id": "Wn15dbArlsIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For Reproducibility\n",
        "import numpy as np\n",
        "# np.random.seed(1337)  # for reproducibility\n",
        "\n",
        "import tensorflow as tf\n",
        "# tf.random.set_seed(33)\n",
        "\n",
        "import random as python_random\n",
        "# python_random.seed(4)\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/utils/set_random_seed\n",
        "tf.keras.utils.set_random_seed(342) #Possibly use next iteration if the above doesn't work\n",
        "\n",
        "\n",
        "# Running more than once causes variation.  try adding this:\n",
        "# Set seed value\n",
        "seed_value = 56\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "print(\"TF version: \" , tf.__version__ )\n",
        "print(\"Keras version: \" , tf.keras.__version__ )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcjDfFIIbmbo",
        "outputId": "e06b15bc-c821-4d18-f429-71f300c5795f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version:  2.11.0\n",
            "Keras version:  2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import rest of Library"
      ],
      "metadata": {
        "id": "mTW-hEgnlp44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# from https://github.com/keras-team/keras/issues/2115#issuecomment-204060456\n",
        "# witha correction on the weighted function in the middle \n",
        "\n",
        "'''Train a simple deep NN on the MNIST dataset.\n",
        "Get to 98.40% test accuracy after 20 epochs\n",
        "(there is *a lot* of margin for parameter tuning).\n",
        "2 seconds per epoch on a K520 GPU.\n",
        "'''\n",
        "\n",
        "from __future__ import print_function  #do i still need this?\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "from keras.utils import np_utils\n",
        "import keras.backend as K\n",
        "from itertools import product\n",
        "import functools\n",
        "from functools import partial\n",
        "from time import ctime\n",
        "from time import sleep\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd \n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "## MORE REPEATABILITY STUFF NEEDED - If theres a way to update this to V2 of Tensorflow great, otherwise I had to use TF 1.0 code\n",
        "# 5. Configure a new global `tensorflow` session (https://stackoverflow.com/questions/50659482/why-cant-i-get-reproducible-results-in-keras-even-though-i-set-the-random-seeds)\n",
        "# from keras import backend as K\n",
        "\n",
        "\n",
        "#I believe thecode below is to help things be repeatable each time different sections in my google colab notebook execute\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)"
      ],
      "metadata": {
        "id": "idfYNyyAgMsO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define batch, epochs, and format data"
      ],
      "metadata": {
        "id": "otcbfKF7mY9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256 # I originally had it very  high batch size to reduce the variation in the data each batch and hope it makes the model training more nearly identical which it did, then i bring it back down to something reasonable to get better results training the NN\n",
        "nb_classes = 10\n",
        "nb_epoch = 45\n",
        "\n",
        "# the data, shuffled and split between train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "X_train = X_train.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "# print(X_train.shape[0], 'train samples')\n",
        "# print(X_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "#Create a Validation Set\n",
        "X_val = X_test[:7500]   #take the first 7500 for validation\n",
        "Y_val = Y_test[:7500]   #Take the first 7500 for validation\n",
        "y_val = y_test[:7500]\n",
        "\n",
        "X_test = X_test[7500:]  #Keep the last 2500 for test/holdout\n",
        "Y_test = Y_test[7500:]  #Keep the last 2500 for test/holdout\n",
        "y_test = y_test[7500:]\n",
        "\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_val.shape[0], 'validation samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "#Are the sets relatively balanced? Yes each category is between 8% and 11% per category\n",
        "Y_train.sum(axis=0)/X_train.shape[0]\n",
        "Y_val.sum(axis=0)/X_val.shape[0]\n",
        "Y_test.sum(axis=0)/X_test.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B59UXDb8i8W5",
        "outputId": "8dd638b6-52f7-48d6-b3a0-a7e1dfba3ca8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n",
            "60000 train samples\n",
            "7500 validation samples\n",
            "2500 test samples\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.1044, 0.1144, 0.0992, 0.102 , 0.0932, 0.0864, 0.1008, 0.1064,\n",
              "       0.0972, 0.096 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weighted Categorical Cross Entropy Class"
      ],
      "metadata": {
        "id": "3fHQHrz8MwXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightedCategoricalCrossentropy(tf.keras.losses.CategoricalCrossentropy):\n",
        "\n",
        "  def __init__(self, cost_mat, name='weighted_categorical_crossentropy', **kwargs):\n",
        "\n",
        "    cost_mat = np.array(cost_mat)   \n",
        "    ## when loading from config, self.cost_mat returns as a list, rather than an numpy array. \n",
        "    ## Adding the above line fixes this issue, enabling .ndim to call sucessfully. \n",
        "    ## However, this is probably not the best implementation\n",
        "    assert(cost_mat.ndim == 2)\n",
        "    assert(cost_mat.shape[0] == cost_mat.shape[1])\n",
        "    super().__init__(name=name, **kwargs)\n",
        "    self.cost_mat = K.cast_to_floatx(cost_mat)\n",
        "\n",
        "  def __call__(self, y_true, y_pred, sample_weight=None):\n",
        "    assert sample_weight is None, \"should only be derived from the cost matrix\"  \n",
        "    return super().__call__(\n",
        "        y_true=y_true, \n",
        "        y_pred=y_pred, \n",
        "        sample_weight=get_sample_weights(y_true, y_pred, self.cost_mat),\n",
        "    )\n",
        "\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config().copy()\n",
        "    # Calling .update on the line above, during assignment, causes an error with config becoming None-type.\n",
        "    config.update({'cost_mat': (self.cost_mat)})\n",
        "    return config\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "    # something goes wrong here and changes self.cost_mat to a list variable.\n",
        "    # See above for temporary fix\n",
        "    return cls(**config)\n",
        "\n",
        "def get_sample_weights(y_true, y_pred, cost_m):\n",
        "    num_classes = len(cost_m)\n",
        "\n",
        "    y_pred.shape.assert_has_rank(2)\n",
        "    assert(y_pred.shape[1] == num_classes)\n",
        "    y_pred.shape.assert_is_compatible_with(y_true.shape)\n",
        "\n",
        "    y_pred = K.one_hot(K.argmax(y_pred), num_classes)\n",
        "\n",
        "    y_true_nk1 = K.expand_dims(y_true, 2)\n",
        "    y_pred_n1k = K.expand_dims(y_pred, 1)\n",
        "    cost_m_1kk = K.expand_dims(cost_m, 0)\n",
        "\n",
        "    sample_weights_nkk = cost_m_1kk * y_true_nk1 * y_pred_n1k\n",
        "    sample_weights_n = K.sum(sample_weights_nkk, axis=[1, 2])\n",
        "\n",
        "    return sample_weights_n\n",
        "\n",
        "\n",
        "# Register the loss in the Keras namespace to enable loading of the custom object.\n",
        "tf.keras.losses.WeightedCategoricalCrossentropy = WeightedCategoricalCrossentropy\n",
        " "
      ],
      "metadata": {
        "id": "pUR1sLQ7MvVa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Model with WeightedCategoricalCross Entropy Function "
      ],
      "metadata": {
        "id": "-uJmU0t4ANuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def PA_method_epoch(model3, lambda_val, nb_epoch = 5, patience = 5 , **args):\n",
        "  # print(\"Parameter values specified for this training-cycle.  Number of epochs: \", nb_epoch, \"Number of epochs of Patience: \", patience)\n",
        "  cost_matrix = np.ones((10,10))\n",
        "\n",
        "  Truth=9\n",
        "  Predicted=4\n",
        "\n",
        "  cost_matrix[Truth, Predicted] = lambda_val\n",
        "  # print(\"The cost of a true \", str(Truth), \"  predicted as a \", str(Predicted), \" is \", lambda_val  )\n",
        "\n",
        "\n",
        "  # ---------------------------------------------------------------------------------------------------------------------------------\n",
        "  # Is a Model already created?  \n",
        "  # ---------------------------------------------------------------------------------------------------------------------------------\n",
        "  if model3==None:  #\"No model was passed through so one will be initiated\")\n",
        "\n",
        "      print(\"No model was passed through so one will be initiated\")\n",
        "\n",
        "      model3 = Sequential()\n",
        "      model3.add(Dense(40, input_shape=(784,), kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "      model3.add(Activation('relu'))\n",
        "      model3.add(Dropout(0.2))\n",
        "      model3.add(Dense(40, kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "      model3.add(Activation('relu'))\n",
        "      model3.add(Dropout(0.2))\n",
        "      model3.add(Dense(10,kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "      model3.add(Activation('softmax'))\n",
        "      rms = RMSprop()  #https://keras.io/api/optimizers/rmsprop/\n",
        "      model3.compile(loss=WeightedCategoricalCrossentropy(cost_matrix), optimizer=rms,  metrics='categorical_accuracy',)\n",
        "\n",
        " \n",
        "  # else:  #A model was passed through, so no other model will be created\n",
        "  #   # print(\"A model was passed through\")\n",
        "\n",
        "\n",
        "  def log_confusion_matrix(epoch, logs):\n",
        "    # Use the model to predict the values from the validation dataset.\n",
        "    y_prediction3 = model3.predict(X_val, verbose=0)     #I call it y_prediction3 because I just want to make sure this is  updated within and not interfering with the other prediction below\n",
        "    y_prediction3  = np.argmax(y_prediction3, axis=1)\n",
        "\n",
        "    #Create confusion matrix \n",
        "    cm = confusion_matrix(y_val, y_prediction3)\n",
        "    cm_array = np.asarray(cm)  #Indiv CM as array for storing\n",
        "    logs['9T_4P'] = cm[9,4]\n",
        "    logs['4T_9P'] = cm[4,9]\n",
        "    logs['cm_per_epoch'] = cm_array.reshape((1,100))\n",
        "  # Define the per-epoch callback.\n",
        "  cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix, )\n",
        "\n",
        "\n",
        "  # ---------------------------------------------------------------------------------------------------------------------------------\n",
        "  # Will this be a training session with early-stopping? If so user specifies a patience level, and the appropriate callback is used\n",
        "  # ---------------------------------------------------------------------------------------------------------------------------------\n",
        "  if patience > 0:\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, verbose=1, restore_best_weights = True)\n",
        "    model3_history = model3.fit(X_train, Y_train,\n",
        "          batch_size=batch_size, epochs=nb_epoch, verbose=0,\n",
        "          validation_data=(X_val, Y_val), shuffle=True, use_multiprocessing=True\n",
        "          ,callbacks = [es_callback, cm_callback]\n",
        "          )\n",
        "  #If early-stopping is not needed then the model will be trained without using the ES callback\n",
        "  else:  # no need to do early-stopping if patience = 0\n",
        "    model3_history = model3.fit(X_train, Y_train,\n",
        "          batch_size=batch_size, epochs=nb_epoch, verbose=0,\n",
        "          validation_data=(X_val, Y_val), shuffle=True, use_multiprocessing=True\n",
        "          ,callbacks = [cm_callback]    #<<< -------------- the only thing that changed is no EarlyStop Callback\n",
        "          )\n",
        "\n",
        " \n",
        "\n",
        "    \n",
        "  # ---------------------------------------------------------------------------------------------------------------------------------\n",
        "  # Training complete.  Make Predictions\n",
        "  # ---------------------------------------------------------------------------------------------------------------------------------\n",
        "  #Predict\n",
        "  y_prediction = model3.predict(X_val, verbose=0)\n",
        "  y_prediction  = np.argmax(y_prediction, axis=1)\n",
        "  # Y_prediction = np_utils.to_categorical(y_prediction, nb_classes)\n",
        "\n",
        "  #Create confusion matrix and normalizes it over predicted (columns)\n",
        "  # result = confusion_matrix(y_val, y_prediction , normalize='pred')\n",
        "\n",
        "\n",
        "\n",
        "  cm3 = confusion_matrix(y_val, y_prediction)\n",
        "  cm3 = pd.DataFrame(cm3, range(10),range(10))\n",
        "  \n",
        "  # plt.figure(figsize = (10,10))\n",
        "  # cm3\n",
        "  # sns.heatmap(cm2, annot=True, annot_kws={\"size\": 12}) # font size\n",
        "  # plt.show()\n",
        "  # cm_using_weighted_new = cm3\n",
        "\n",
        "  # print(model3_history.history)\n",
        "  tot_epochs = max(model3_history.epoch)+1  #if the total epochs ran is 28, it'll show up as 27 in the epoch object so we must add 1\n",
        "  print(\"Total Epochs: \", tot_epochs)\n",
        "\n",
        "  #if tot_epochs is the total number of epochs ran then early stop did not happen, and we need not minus patience\n",
        "  if tot_epochs == nb_epoch:\n",
        "    restored_weights = tot_epochs\n",
        "  else:\n",
        "    restored_weights  = tot_epochs-patience   #when using restore-best-weights and patience, it'll restore the best weights back\n",
        "  print(\"Restored weights at \", restored_weights, \"Patience used: \", patience)\n",
        "\n",
        "\n",
        "  return cm3, model3_history, model3, restored_weights\n"
      ],
      "metadata": {
        "id": "3UWVdmRHNBhP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Single Plot Function"
      ],
      "metadata": {
        "id": "7HLUprZ9t9YB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_fx(model3_history, restored_weights):\n",
        "\n",
        "\n",
        "  tot_epochs = max(model3_history.epoch)+1\n",
        "\n",
        "  #Label is the epoch weights are restored \n",
        "  label = f\"{restored_weights}\"\n",
        "  #Label_vale is the value at which the epoch weights are restored \n",
        "  label_value = f\"{ model3_history.history['val_categorical_accuracy'][restored_weights-1]}\"\n",
        "  print(\"Label: \", label, \"Val_Cat_Acc Value: \", label_value)\n",
        "\n",
        "\n",
        "\n",
        "  plt.plot(range(1,tot_epochs+1), model3_history.history['categorical_accuracy'],)\n",
        "  plt.plot(range(1,tot_epochs+1), model3_history.history['val_categorical_accuracy'])\n",
        "  plt.xlim(xmin=1)\n",
        "\n",
        "  plt.scatter((restored_weights), model3_history.history['val_categorical_accuracy'][restored_weights-1] , color='orange')\n",
        "\n",
        "  plt.annotate(text=label,  xy=(restored_weights, model3_history.history['val_categorical_accuracy'][restored_weights-1]),\n",
        "                 textcoords=\"offset points\", \n",
        "                 xytext=(0,10), \n",
        "                 ha='center')\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch (starts at 1)')\n",
        "\n",
        "  \n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()\n",
        "  \n",
        "  \n",
        "\n",
        "  plt.plot(range(1,tot_epochs+1), model3_history.history['loss'])\n",
        "  plt.plot(range(1,tot_epochs+1), model3_history.history['val_loss'])\n",
        "  plt.xlim(xmin=1)\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch (starts at 1)')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.scatter(restored_weights, model3_history.history['val_loss'][restored_weights-1], color='orange')\n",
        "  plt.annotate(text=label,  xy=(restored_weights, model3_history.history['val_loss'][restored_weights-1]),\n",
        "                 textcoords=\"offset points\", \n",
        "                 xytext=(0,10), \n",
        "                 ha='center')\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "lHP7RI6-nQKb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot all three phases together"
      ],
      "metadata": {
        "id": "N7yL66nEOkg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_3_fx(model1_history, model2_history, model3_history, restored_weights1, restored_weights2, restored_weights3):\n",
        "  \n",
        "  tot_epochs = restored_weights1 + restored_weights2 + restored_weights3\n",
        "\n",
        "  #Label is the epoch weights are restored \n",
        "  label1 = f\"{restored_weights1}\"\n",
        "  #Label_vale is the value at which the epoch weights are restored \n",
        "  label_value1 = f\"{ model1_history.history['val_categorical_accuracy'][restored_weights1-1]}\"\n",
        "  # print(\"Label1: \", label1, \"Val_Cat_Acc Value1: \", label_value1)\n",
        "\n",
        "\n",
        "  #Label is the epoch weights are restored in the second phase of training\n",
        "  # if restoredweights are 5 then it'll be after the max number of epochs of the first models training \n",
        "  label2 = f\"{restored_weights2}\"\n",
        "  #Label_vale is the value at which the epoch weights are restored \n",
        "  label_value2 = f\"{ model2_history.history['val_categorical_accuracy'][restored_weights2-1]}\"\n",
        "  # print(\"Label2: \", label2, \"Val_Cat_Acc Value2: \", label_value2)\n",
        "\n",
        "  #Label is the epoch weights are restored in the second phase of training\n",
        "  # if restoredweights are 5 then it'll be after the max number of epochs of the first models training \n",
        "  label3 = f\"{restored_weights3}\"\n",
        "  #Label_vale is the value at which the epoch weights are restored \n",
        "  label_value3 = f\"{ model3_history.history['val_categorical_accuracy'][restored_weights3-1]}\"\n",
        "  # print(\"Label3: \", label3, \"Val_Cat_Acc Value3: \", label_value3)\n",
        "\n",
        " \n",
        "  \n",
        "  epochs = [restored_weights1, restored_weights2, restored_weights3]\n",
        "  models = [history1, history2, history3]\n",
        "\n",
        "  grab_n_epochs_of_model_metric_fun = lambda epoch, model, metric: model.history[metric][:epoch]\n",
        "\n",
        "  model_total_cat_acc =     np.concatenate(list(map(grab_n_epochs_of_model_metric_fun, epochs, models, ['categorical_accuracy']*3)), axis=0)\n",
        "  model_total_val_cat_acc = np.concatenate(list(map(grab_n_epochs_of_model_metric_fun, epochs, models, ['val_categorical_accuracy']*3)), axis=0)\n",
        "  \n",
        "  model_total_loss =        np.concatenate(list(map(grab_n_epochs_of_model_metric_fun, epochs, models, ['loss']*3)), axis=0)\n",
        "  model_total_val_loss =    np.concatenate(list(map(grab_n_epochs_of_model_metric_fun, epochs, models, ['val_loss']*3)), axis=0)\n",
        "\n",
        "  model_total_val_9T_4P =   np.concatenate(list(map(grab_n_epochs_of_model_metric_fun, epochs, models, ['9T_4P']*3)), axis=0)\n",
        "  model_total_val_4T_9P =   np.concatenate(list(map(grab_n_epochs_of_model_metric_fun, epochs, models, ['4T_9P']*3)), axis=0)\n",
        "\n",
        "  \n",
        "  #ScatterPlotPoint and Annotate RestoredWeights1 \n",
        "  offsets = [0,restored_weights1, restored_weights1+restored_weights2]\n",
        "  labels = [label1, label2, label3]\n",
        "\n",
        "  for metric in list(('val_categorical_accuracy', 'val_loss', '9T_4P')):\n",
        "\n",
        "\n",
        "    if(metric=='val_categorical_accuracy'):\n",
        "      plt.plot(range(1,tot_epochs+1), model_total_cat_acc)\n",
        "      plt.plot(range(1,tot_epochs+1), model_total_val_cat_acc)\n",
        "    elif(metric=='val_loss'):\n",
        "      plt.plot(range(1,tot_epochs+1), model_total_loss)\n",
        "      plt.plot(range(1,tot_epochs+1), model_total_val_loss)\n",
        "    else:\n",
        "      plt.plot(range(1,tot_epochs+1), model_total_val_9T_4P, color='orange')\n",
        "      plt.plot(range(1,tot_epochs+1), model_total_val_4T_9P, color='orange', linestyle='dashed')\n",
        "\n",
        "    plt.xlim(xmin=0)\n",
        "\n",
        "\n",
        "    for epoch, model, offset, label in zip(epochs, models, offsets, labels):\n",
        "      # print(f'Epoch of Restored-Weights: {epoch}, Model: {model}, Offset: {offset}')\n",
        "      plt.scatter((epoch+offset), model.history[metric][epoch-1] , color='orange')\n",
        "      plt.annotate(text=label,  xy=((epoch+offset), model.history[metric][epoch-1]),\n",
        "                  textcoords=\"offset points\", \n",
        "                  xytext=(0,10), \n",
        "                  ha='center')\n",
        "    \n",
        "    if(metric=='val_categorical_accuracy'):\n",
        "      plt.ylabel('accuracy')\n",
        "      plt.title('model accuracy')\n",
        "      plt.legend(['train', 'val'], loc='lower right')\n",
        "    \n",
        "\n",
        "    elif(metric=='val_loss'):\n",
        "      plt.ylabel('loss')\n",
        "      plt.title('model loss')\n",
        "      plt.legend(['train', 'val'], loc='upper right')\n",
        "\n",
        "    else:\n",
        "      plt.ylabel('count')\n",
        "      plt.title('Validation misclassifications of 9T as 4P and reverse')\n",
        "      plt.legend(['9t_4p', '4t_9p'], loc='upper right')\n",
        "    \n",
        "\n",
        "    plt.xlabel('epoch (starts at 1)') \n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    history =   {'categorical_accuracy': model_total_cat_acc , \n",
        "               'val_categorical_accuracy': model_total_val_cat_acc  , \n",
        "              'loss': model_total_loss,\n",
        "               'val_loss': model_total_val_loss ,   \n",
        "               '9T_4P': model_total_val_9T_4P ,  \n",
        "               '4T_9P': model_total_val_4T_9P}\n",
        " \n",
        "\n",
        "\n",
        "  return history\n",
        " \n"
      ],
      "metadata": {
        "id": "07WwRSAh6qRW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ZLsEH1ZrqZB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Keep Track of Experimental Admin Stuff - #Reps and #CostMatrix\n",
        "\n",
        "> Change the cost matrix and number of reps and check the file extension name \n"
      ],
      "metadata": {
        "id": "skXIN6S4npiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Define Cost Matrix and Method"
      ],
      "metadata": {
        "id": "q9YhLRi4NU2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experimental Admin Stuff\n",
        "\n",
        "# # ### Weight of Misclassifications for three stages of training\n",
        "# lambda1, lambda2, lambda3 = 1, 1, 1\n",
        "\n",
        "\n",
        "# # ### Number of Epochs for each Stage of Training\n",
        "# train_epochs1, train_epochs2, train_epochs3 = 5, 5, 20\n",
        "\n",
        "# # ### Whether there is EarlyStopping and if so how many Epochs of Patience Patience=0 means no EarlyStopping\n",
        "# es1, es2, es3 = 5, 5, 20\n",
        "\n",
        "# # ### String of weights-of-misclassification for naming files\n",
        "# ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \n",
        "# cost_str = str(lambda1) + \"_\" + str(lambda2) + \"_\"  + str(lambda3)\n",
        "# epochs_str = str(train_epochs1) + \"_\" + str(train_epochs2) + \"_\"  + str(train_epochs3)\n",
        "# patience_str = str(es1) + \"_\" + str(es2) + \"_\"  + str(es3)\n",
        "# # ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \n",
        "\n",
        " \n",
        "\n",
        "# ### File Extension to reference in JMP : weights_method_costs1_2_3\n",
        "# ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# file_extension = \"w[9,4]_L_\" + cost_str + \"_E_\" + epochs_str + \"_P_\" + patience_str + \"_\"\n",
        "# ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# print(len(file_extension))\n",
        "\n",
        "\n",
        "# np.set_printoptions(suppress=True)\n",
        "# np.set_printoptions(precision=2)\n",
        "\n",
        "# print(\"Last run using \", cost_str)\n",
        "# print(\"File Extension \", file_extension)"
      ],
      "metadata": {
        "id": "3bMXTRRBnn3E"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run Experiments"
      ],
      "metadata": {
        "id": "x_EdEdJwOpvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# cm = np.zeros([10,10])\n",
        "# combined_cms = np.empty((1,100))\n",
        "count = 0\n",
        "run = 0\n",
        "rep = 0\n",
        "reps = 5\n",
        "print(\"     | --- Init --- | | --- Mid --- | | --- Late --- | \")\n",
        "for i in [1, 100, 1000]: #lambda1\n",
        "  for j in [1, 100, 1000]:  #lambda2\n",
        "    for k in   [1, 100, 1000]: #lambda3\n",
        "      run+=1\n",
        "      for rep in range(1,reps+1):\n",
        "        count+=1\n",
        "        lambda1 = i\n",
        "        lambda2 =j\n",
        "        lambda3 = k\n",
        "\n",
        "\n",
        "        # ### Weight of Misclassifications for three stages of training\n",
        "        # lambda1, lambda2, lambda3 = 1, 1, 1\n",
        "\n",
        "\n",
        "        # ### Number of Epochs for each Stage of Training\n",
        "        train_epochs1, train_epochs2, train_epochs3 = 5, 5, 25\n",
        "\n",
        "        # ### Whether there is EarlyStopping and if so how many Epochs of Patience Patience=0 means no EarlyStopping\n",
        "        es1, es2, es3 = 0, 0, 25\n",
        "\n",
        "        # ### String of weights-of-misclassification for naming files\n",
        "        ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \n",
        "        cost_str = str(lambda1) + \"_\" + str(lambda2) + \"_\"  + str(lambda3)\n",
        "        epochs_str = str(train_epochs1) + \"_\" + str(train_epochs2) + \"_\"  + str(train_epochs3)\n",
        "        patience_str = str(es1) + \"_\" + str(es2) + \"_\"  + str(es3)\n",
        "        # ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \n",
        "\n",
        "        \n",
        "\n",
        "        ### File Extension to reference in JMP : weights_method_costs1_2_3\n",
        "        ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "        file_extension = \"w[9,4]_L_\" + cost_str + \"_E_\" + epochs_str + \"_P_\" + patience_str + \"_\"\n",
        "        ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "        # print(len(file_extension))\n",
        "\n",
        "\n",
        "        np.set_printoptions(suppress=True)\n",
        "        np.set_printoptions(precision=2)\n",
        "\n",
        "        # print(\"Last run using \", cost_str, \"File Extension \", file_extension)\n",
        "        print(f'{run} '*500)\n",
        "        print(f\"Count:{count}--Run:{run}--Rep:{rep}|Lambdas {i}, {j},{k} | Epochs: {train_epochs1} with {es1} , {train_epochs2} with {es2},  {train_epochs3} with {es3} | \")\n",
        "        print(f'{run} '*500)\n",
        "\n",
        "        combined_history_dictionary = {}  #because there will be a unknown number of epochs a dictionary is probly a better fit\n",
        "        combined_cms       = {}\n",
        "\n",
        "\n",
        "\n",
        "        def display_cm_after_training(training, cm):\n",
        "          print(\"After \", str(training), \" stage CM: \\n\", cm)\n",
        "          print(\"CM[9,4]: \", cm[4][9]) #yes this is the right format dont change! [First Value is the Column][Second Value is the Row]\n",
        " \n",
        "\n",
        "        # Initial Training _________________________________________________________\n",
        "        cm1, history1, model1, restored_weights1 =  PA_method_epoch(None, lambda1, nb_epoch=train_epochs1, patience=es1 )    #Individual CM, and Training/Validation History\n",
        "        # plot_fx(history1, restored_weights1)\n",
        "        # display_cm_after_training(\"first\", cm1)\n",
        "\n",
        "        # Second Phase Training ______________________________________________________\n",
        "        print(\"Run \", run, \"Rep \", rep, \"second-phase training with a  cost_matrix[9, 4] = \",  lambda2)\n",
        "        cm2, history2, model2, restored_weights2 =  PA_method_epoch(model1, lambda2, nb_epoch=train_epochs2, patience=es2 )    #Individual CM, and Training/Validation History\n",
        "        # plot_fx(history2, restored_weights2)\n",
        "        # display_cm_after_training(\"second\", cm2)\n",
        "\n",
        "        \n",
        "        # Third Phase Training ____________________________________________________________\n",
        "        print(\"Run \", run, \"Rep \",rep, \"third-phase training with a  cost_matrix[9, 4] = \",  lambda3)\n",
        "        cm3, history3, model3, restored_weights3 =  PA_method_epoch(model2, lambda3, nb_epoch=train_epochs3, patience=es3 )    #Individual CM, and Training/Validation History\n",
        "        # plot_fx(history3, restored_weights3)\n",
        "        # display_cm_after_training(\"third\", cm3)\n",
        "        \n",
        "\n",
        "\n",
        "        # # Save the results of 1st 2nd and 3rd phase training in one combined_history_list or disctionary\n",
        "        #   combined_history_dictionary[run, i] = {\"First\": history1, \"Second\": history2, \"Third\": history3}\n",
        "        #   combined_history_list.extend((history1, history2, history3))\n",
        "        cm1_array = np.asarray(cm1)  #Indiv CM as array for storing\n",
        "        cm2_array = np.asarray(cm2)\n",
        "        cm3_array = np.asarray(cm3)\n",
        "\n",
        "        # This results in saving the three confusion matrices after the three stages of training\n",
        "        # The shape of the combined Cms will be 1x300 so that all can be saved on one-row\n",
        "      \n",
        "        cms = np.vstack((cm1_array.reshape((1,100)),\n",
        "                                  cm2_array.reshape((1,100)),\n",
        "                                  cm3_array.reshape((1,100)))\n",
        "                                )\n",
        "        \n",
        "\n",
        "        combined_history_4 = plot_3_fx(history1, history2, history3, restored_weights1, restored_weights2, restored_weights3)\n",
        "        combined_history_dictionary[run, rep] = {\"First\": history1, \"Second\": history2, \"Third\": history3, 'Combined': combined_history_4, '3ConfusionMatrices': cms}\n",
        "        combined_cms[run, rep] = cms\n",
        "\n",
        "        print(\"Run: \", run, \" Rep: \", rep, \"  complete \\n\")\n",
        "\n",
        "                    \n",
        "        # To access the dictionary values for all three stages of training:\n",
        "        for phase2 in [\"First\", \"Second\", \"Third\"]:\n",
        "            # print(combined_history_dictionary[run,rep][i].history.keys())\n",
        "            del combined_history_dictionary[run,rep][phase2].model #https://github.com/keras-team/keras/issues/646 from FChollet\n",
        "\n",
        "\n",
        "        import pickle\n",
        "\n",
        "        str_reps = str(reps)\n",
        "\n",
        "\n",
        "        from datetime import date\n",
        "        from datetime import datetime\n",
        "        today = date.today()\n",
        "        file_date = today.strftime(\"%Y_%m_%d\")\n",
        "        now = datetime.now() # current date and time\n",
        "        file_time = now.strftime(\"%H%M\")\n",
        "        print(\"time:\", file_time)\n",
        "\n",
        "        str_runs = str(run)\n",
        "\n",
        "\n",
        "        file_name =  \"run\" + str_runs + \"_\" + \"rep_\" + str_runs + \"_\" + file_extension + \"_\" + file_date + \"_\" + file_time +  \"_.pkl\"\n",
        "        # print(file_name, \" will be saved with \", combined_cms.shape)\n",
        "\n",
        "        with open(file_name, 'wb') as handle:\n",
        "              \n",
        "            # A new file will be created\n",
        "            pickle.dump(combined_history_dictionary, handle,  protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "        # Open the file in binary mode\n",
        "        with open(file_name, 'rb') as handle:\n",
        "              \n",
        "            # Call load method to deserialze\n",
        "            unpickled_object = pickle.load(handle)\n",
        "          \n",
        "            print(unpickled_object == combined_history_dictionary)\n",
        "            \n",
        "        # print(file_name, \" was opened with \", var.shape)\n",
        "\n",
        "        from google.colab import files\n",
        "        files.download( file_name )  \n",
        "\n",
        "        print(file_name, \" was saved to Downloads \")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# "
      ],
      "metadata": {
        "id": "OSq7jMYUOF4t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "outputId": "eed15899-a437-402e-cd55-2e37c8b80850"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     | --- Init --- | | --- Mid --- | | --- Late --- | \n",
            "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
            "Count:1--Run:1--Rep:1|Lambda11, Lambda21, Lambda31 | Epochs15 with Es:0 | Epochs15 with Es20,  Epochs325 with ES3:25 | \n",
            "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
            "No model was passed through so one will be initiated\n",
            "Total Epochs:  5\n",
            "Restored weights at  5 Patience used:  0\n",
            "Run  1 Rep  1 second-phase training with a  cost_matrix[9, 4] =  1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-c3cf6ddcdac6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# Initial Training _________________________________________________________\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mcm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestored_weights1\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mPA_method_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_epochs1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mes1\u001b[0m \u001b[0;34m)\u001b[0m    \u001b[0;31m#Individual CM, and Training/Validation History\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;31m# plot_fx(history1, restored_weights1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# display_cm_after_training(\"first\", cm1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-0973a07c0035>\u001b[0m in \u001b[0;36mPA_method_epoch\u001b[0;34m(model3, lambda_val, nb_epoch, patience, **args)\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0;31m#If early-stopping is not needed then the model will be trained without using the ES callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# no need to do early-stopping if patience = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     model3_history = model3.fit(X_train, Y_train,\n\u001b[0m\u001b[1;32m     64\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-0973a07c0035>\u001b[0m in \u001b[0;36mlog_confusion_matrix\u001b[0;34m(epoch, logs)\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mlog_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Use the model to predict the values from the validation dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0my_prediction3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m#I call it y_prediction3 because I just want to make sure this is  updated within and not interfering with the other prediction below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0my_prediction3\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_prediction3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2348\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2349\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2350\u001b[0;31m                         \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2351\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2352\u001b[0m                             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[0;32m--> 133\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m    134\u001b[0m     return concrete_function._call_flat(\n\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;31m# A new cache key will be built later when saving ConcreteFunction because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;31m# only active captures should be saved.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m     lookup_func_key, _ = function_context.make_cache_key((args, kwargs),\n\u001b[0m\u001b[1;32m    337\u001b[0m                                                          captures)\n\u001b[1;32m    338\u001b[0m     \u001b[0mconcrete_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlookup_func_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/function_context.py\u001b[0m in \u001b[0;36mmake_cache_key\u001b[0;34m(args, captures)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mcaptures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   \u001b[0msignature_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInternalTracingContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m   args_signature = trace_type.from_value(\n\u001b[0m\u001b[1;32m    134\u001b[0m       args, signature_context)\n\u001b[1;32m    135\u001b[0m   captures_dict_tracetype = trace_type.from_value(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/core/function/trace_type/trace_type_builder.py\u001b[0m in \u001b[0;36mfrom_value\u001b[0;34m(value, context)\u001b[0m\n\u001b[1;32m    127\u001b[0m           named_tuple_type, tuple(from_value(c, context) for c in value))\n\u001b[1;32m    128\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/core/function/trace_type/trace_type_builder.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    127\u001b[0m           named_tuple_type, tuple(from_value(c, context) for c in value))\n\u001b[1;32m    128\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/core/function/trace_type/trace_type_builder.py\u001b[0m in \u001b[0;36mfrom_value\u001b[0;34m(value, context)\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_legacy_signature\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTraceType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m   \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSupportsTracingProtocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__tf_tracing_type__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/typing.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m   1151\u001b[0m                     (not callable(getattr(cls, attr, None)) or\n\u001b[1;32m   1152\u001b[0m                      getattr(instance, attr) is not None)\n\u001b[0;32m-> 1153\u001b[0;31m                     for attr in _get_protocol_attrs(cls)):\n\u001b[0m\u001b[1;32m   1154\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__instancecheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/typing.py\u001b[0m in \u001b[0;36m_get_protocol_attrs\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m         \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__annotations__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1077\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_abc_'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mEXCLUDED_ATTRIBUTES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # plot_3_fx(history1, history2, history3, restored_weights1, restored_weights2, restored_weights3)\n"
      ],
      "metadata": {
        "id": "Z66FRDMjuI22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dir(history) and # print(history.__dict__)  are useful for digging into what variables are inside a variable"
      ],
      "metadata": {
        "id": "s6HsboVWsimi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " \n",
        "# # To access the dictionary values for all three stages of training:\n",
        "# for rep in range(1,reps+1):\n",
        "#   for i in [\"First\", \"Second\", \"Third\"]:\n",
        "#     print(combined_history_dictionary[run,rep][i].history.keys())\n",
        "#     del combined_history_dictionary[run,rep][i].model #https://github.com/keras-team/keras/issues/646 from FChollet\n",
        "\n",
        "# # combined_history_dictionary[run,reps]['Third'].history['cm']\n",
        "\n",
        "\n",
        "# combined_history_dictionary[run,reps]['Combined']\n",
        "\n",
        "# for rep in range(1,reps+1):\n",
        "#   print(combined_history_dictionary[run,reps]['3ConfusionMatrices'])\n",
        "\n",
        "\n",
        "# combined_history_dictionary[1,1][\"3ConfusionMatrices\"][2]"
      ],
      "metadata": {
        "id": "JMQNfNee3vWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combined_history_dictionary[run,rep][i].history"
      ],
      "metadata": {
        "id": "fLo5xpsZgR_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combined_history_dictionary[1,1][\"First\"].model"
      ],
      "metadata": {
        "id": "Iy0poQ6ifwOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save the 30/Rep-number of confusion matrices"
      ],
      "metadata": {
        "id": "u-MSCXKC48ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #Save 30 confusion matrices\n",
        "\n",
        "# import pickle\n",
        "\n",
        "# str_reps = str(reps)\n",
        "\n",
        "\n",
        "# from datetime import date\n",
        "# from datetime import datetime\n",
        "# today = date.today()\n",
        "# file_date = today.strftime(\"%Y_%m_%d\")\n",
        "# now = datetime.now() # current date and time\n",
        "# file_time = now.strftime(\"%H%M\")\n",
        "# print(\"time:\", file_time)\n",
        "\n",
        "# str_runs = str(run)\n",
        "\n",
        "\n",
        "# file_name =  \"run\" + str_runs + \"_\" + str_reps + \"reps_\" + file_extension + \"_\" + file_date + \"_\" + file_time +  \"_.pkl\"\n",
        "# # print(file_name, \" will be saved with \", combined_cms.shape)\n",
        "\n",
        "# with open(file_name, 'wb') as handle:\n",
        "      \n",
        "#     # A new file will be created\n",
        "#     pickle.dump(combined_history_dictionary, handle,  protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "# # Open the file in binary mode\n",
        "# with open(file_name, 'rb') as handle:\n",
        "      \n",
        "#     # Call load method to deserialze\n",
        "#     unpickled_object = pickle.load(handle)\n",
        "  \n",
        "#     print(unpickled_object == combined_history_dictionary)\n",
        "    \n",
        "# # print(file_name, \" was opened with \", var.shape)\n",
        "\n",
        "# from google.colab import files\n",
        "# files.download( file_name )  \n",
        "\n",
        "# print(file_name, \".pkl was saved to Downloads \")\n"
      ],
      "metadata": {
        "id": "9purX_onqXGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many categories are there in the test set?\n",
        "\n",
        "truth_num_per_category = Y_val.sum(axis=0)\n",
        "print(truth_num_per_category)"
      ],
      "metadata": {
        "id": "id_ythTutuo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyze \n",
        "\n",
        "I am now going to load the Average CM and try to get it in a format where I can make it a 1x100 and load all 30 CMs so that we can visualize their distributions in a a big histogram_matrix. At this point the Google Colab variables are gone and I have to reoad them "
      ],
      "metadata": {
        "id": "NP_kxNkhn6Fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# empty_cm = np.zeros((10,10))  \n",
        "# empty_cm=pd.DataFrame(empty_cm)\n",
        "\n",
        "# empty_cm.columns = ['0p', '1p', '2p', '3p', '4p', '5p', '6p', '7p', '8p', '9p']\n",
        "# empty_cm.index = ['0t', '1t', '2t', '3t', '4t', '5t', '6t', '7t', '8t', '9t']\n",
        "\n",
        "# # print(myvar_cm_average)\n",
        "\n",
        "# empty_cm_array = np.asarray(empty_cm)\n",
        "# empty_cm_array_1_100 = np.reshape(empty_cm_array,(1,100))\n",
        "# # print(cm_average_array)\n",
        "\n",
        "# df = empty_cm\n",
        "# df_new = pd.DataFrame(empty_cm_array_1_100,  columns=pd.MultiIndex.from_product([ df.index,df.columns]))\n",
        "# df_new.columns.to_flat_index()\n",
        "# df_new.columns   = ['_'.join(col) for col in df_new.columns.values]\n",
        "\n",
        "# # Now convert combined_cms of size 30x100 to a panda dataframe\n",
        "# cms_df = pd.DataFrame(combined_cms, columns=[df_new.columns], index=[\"First\", \"Second\", \"Third\"])\n",
        "\n",
        "# cms_df\n",
        "\n",
        "# for run in runs:\n",
        "#   for rep in reps:\n",
        "#     for phase in phases:\n",
        "#       pd.DataFrame(combined_history_dictionary[run,rep][\"3ConfusionMatrices\"], columns=[df_new.columns], index=[\"First\", \"Second\", \"Third\"])"
      ],
      "metadata": {
        "id": "kLrNJE0s53pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combined_cms[0,]"
      ],
      "metadata": {
        "id": "eo_ahxwTVI0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combined_cms_df[\"9t_4p\"]"
      ],
      "metadata": {
        "id": "HgPClcrAUdxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# np.average(combined_cms_df[\"9t_4p\"])"
      ],
      "metadata": {
        "id": "TY59hUTadVyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# csv_filename = file_name[:-4] + \".csv\"\n",
        "\n",
        "# combined_cms_df.to_csv(csv_filename)\n",
        "# # \n",
        "\n",
        "# from google.colab import files\n",
        "# files.download(csv_filename )\n",
        "\n",
        "# print(\"Downloading \", csv_filename , \" of shape \", combined_cms_df.shape)"
      ],
      "metadata": {
        "id": "ceUFAr_z9xsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.DataFrame(sum(var)/len(var), columns=[\"Values\"]) \n",
        "# # print(df)\n",
        "\n",
        "# df.style.format({\n",
        "#   'Values': lambda val: f'{val:,.2f}',\n",
        "# })\n",
        "\n",
        "# (df.sort_values(by=\"Values\", ascending=False)[0:20])\n",
        "\n",
        "\n",
        "# df_sorted = df.sort_values(by=\"Values\", ascending=False)[10:]  #the top 10 are usually diagonal\n",
        "\n",
        "\n",
        "# df_sorted.style.format({\n",
        "#   'Values': lambda val: f'{val:,.2f}',\n",
        "# })\n",
        "\n",
        "# import math\n",
        "\n",
        "# print(\"On average...\")\n",
        "# print(\"Num 1 misclassifications are misclassifying a \", math.floor((df_sorted[\"Values\"].index[0])/10), \" as a \", df_sorted[\"Values\"].index[0]%10, \"  (\", (df_sorted[\"Values\"].values[0]), \" times)\" )\n",
        "# print(\"Num 2 misclassifications are misclassifying a \", math.floor((df_sorted[\"Values\"].index[1])/10), \" as a \", df_sorted[\"Values\"].index[1]%10, \"  (\", (df_sorted[\"Values\"].values[1]), \" times)\" )\n",
        "# print(\"Num 3 misclassifications are misclassifying a \", math.floor((df_sorted[\"Values\"].index[2])/10), \" as a \", df_sorted[\"Values\"].index[2]%10, \"  (\", (df_sorted[\"Values\"].values[2]), \" times)\" )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-pGNLuE8gNrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_percents = pd.DataFrame( ((sum(var)*100/len(var)).reshape((10,10))/truth_num_per_category).reshape((100)), columns = [\"Values\"])\n",
        "\n",
        "\n",
        "# df_sorted_percents = df_percents.sort_values(by=\"Values\", ascending=False)[10:]  #the top 10 are usually diagonal\n",
        "\n",
        "# df_sorted_percents.style.format({\n",
        "#   'Values': lambda val: f'{val:,.2f}',\n",
        "# })\n",
        "\n",
        "# print(\"On average .. \")\n",
        "# print(\"Num 1 percent misclassifications\", math.floor((df_sorted_percents[\"Values\"].index[0])/10), \" as \", df_sorted_percents[\"Values\"].index[0]%10, (df_sorted_percents[\"Values\"].values[0]), \" percent\" )\n",
        "# print(\"Num 2 percent misclassifications\", math.floor((df_sorted_percents[\"Values\"].index[1])/10), \" as \", df_sorted_percents[\"Values\"].index[1]%10,  (df_sorted_percents[\"Values\"].values[1]), \" percent\" )\n",
        "# print(\"Num 3 percent misclassifications\", math.floor((df_sorted_percents[\"Values\"].index[2])/10), \" as \", df_sorted_percents[\"Values\"].index[2]%10, (df_sorted_percents[\"Values\"].values[2]), \" percent\" )\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AKYclir2p8wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FrT9iz3dp8rZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extraneous"
      ],
      "metadata": {
        "id": "GZfPCKu_oqrZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To reference later: \n",
        "\n",
        "https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/structured_data/imbalanced_data.ipynb#scrollTo=UJ589fn8ST3x\n",
        "\n",
        "To train a model with class weights:\n",
        "\n",
        "```\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "weighted_model = make_model()\n",
        "weighted_model.load_weights(initial_weights)\n",
        "\n",
        "weighted_history = weighted_model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[early_stopping],\n",
        "    validation_data=(val_features, val_labels),\n",
        "\n",
        "    # The class weights go here\n",
        "    class_weight=class_weight)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "iLEt0OL5ziEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraneous information I am not using at the moment\n",
        "\n",
        "# model.compile(\n",
        "#      optimizer='adam',\n",
        "#      loss=WeightedCategoricalCrossentropy(cost_matrix)\n",
        "#      )\n",
        "\n",
        "## Model Saving\n",
        "\n",
        "# model.save(save_version_dir,save_format='tf')\n",
        "\n",
        "## Model Loading\n",
        "\n",
        "# model = tf.keras.models.load_model(\n",
        "#     save_version_dir,\n",
        "#     compile=True,\n",
        "#     custom_objects={\n",
        "#         'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_matrix)\n",
        "#         }\n",
        "#     )\n",
        " "
      ],
      "metadata": {
        "id": "WRzuOuZeKVpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "starting to think about how i would shape the initial, middle and late training experiments.  "
      ],
      "metadata": {
        "id": "GH-yoluLM0sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "run = 0\n",
        "rep = 0\n",
        "\n",
        "print(\"     | --- Init --- | | --- Mid --- | | --- Late --- | \")\n",
        "for i in [\"   1\",\" 100\", \"1000\"]: #lambda1\n",
        "  \n",
        "  for j in  [\"   1\",\" 100\", \"1000\"]:  #lambda2\n",
        "    for k in   [\"   1\",\" 100\", \"1000\"]: #lambda3\n",
        "      run+=1\n",
        "      for rep in range(1,30+1):\n",
        "        count+=1\n",
        "        print(f\"{count}--{run}--{rep}    |     {i}     ,     {j}    ,    {k}     | \")\n",
        "        \n",
        "\n"
      ],
      "metadata": {
        "id": "EisEf8F6MzFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xVGDsrmmNLed"
      }
    }
  ]
}