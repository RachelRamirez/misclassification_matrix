{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfdkZDX61U0F++ZCquWUff",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RachelRamirez/misclassification_matrix/blob/main/Reproducible_Misclassification_Cost_Matrix_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After reading Keras Issues board for ways to include a cost-matrix, I found two ways provided by authors tboquet  and Isaranto.  At first I wasn't getting reproducible results, but then I realized it was because it depended on the order the neural nets were run.  I think this is largely due to the batch order re-cycling through each sequence.  I could not keep the batch order the same.  If I was able to keep it the same I believe I would get the exact same number each time.    Notice for the reproducibility example I had to seed everything, and I took out Dropout, and I set the batch size very high to minimize the variance between training.  Notice that this notebook doesn't immediately make it extremely obvious how it's reproducible.  The key is to change the order of the three functions Normal_Method, Isantro_method, and TB_Method, and you'll see that no matter what method you call first, you'll get the same numerical results each time.   Etc for 2nd and etc for 3rd. The table that follows shows the order of different functions and the first four rounded loss-values of the training data.  all methods produce the same numerical results, it just depends on the order.  "
      ],
      "metadata": {
        "id": "N-YVb1nz9_Ac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Loss Values | is>nor>tb |\tis>tb>nor |\ttb>nor>is\t| nor>is>tb | \n",
        "---------|-----------|-----------|-----------|-----------|\n",
        "1st Method: | \t[1.472, 0.653, 0.49 , 0.344] \t| [1.472, 0.653, 0.49 , 0.344] | \t[1.472, 0.653, 0.49 , 0.344])| \t[1.472, 0.653, 0.49 , 0.344] \n",
        "2nd Method: | \t[1.53 , 0.654, 0.488, 0.389] |\t[1.53 , 0.654, 0.488, 0.389] | \t[1.53 , 0.654, 0.488, 0.389] |\t([1.53 , 0.654, 0.488, 0.389])\n",
        "3rd Method:\t| [1.519, 0.685, 0.498, 0.399] | \t[1.519, 0.685, 0.498, 0.399] | \t[1.519, 0.685, 0.498, 0.399]  | \t[1.519, 0.685, 0.498, 0.399])\n"
      ],
      "metadata": {
        "id": "vjX8aW0c_jSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qCMs9c3-_0IK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reproducible Seeds"
      ],
      "metadata": {
        "id": "Wn15dbArlsIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For Reproducibility\n",
        "import numpy as np\n",
        "# np.random.seed(1337)  # for reproducibility\n",
        "\n",
        "import tensorflow as tf\n",
        "# tf.random.set_seed(33)\n",
        "\n",
        "import random as python_random\n",
        "# python_random.seed(4)\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/utils/set_random_seed\n",
        "tf.keras.utils.set_random_seed(342) #Possibly use next iteration if the above doesn't work\n",
        "\n",
        "\n",
        "# Running more than once causes variation.  try adding this:\n",
        "# Set seed value\n",
        "seed_value = 56\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "print(\"TF version: \" , tf.__version__ )\n",
        "print(\"Keras version: \" , tf.keras.__version__ )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcjDfFIIbmbo",
        "outputId": "d5cee51d-d4f2-4c6e-8bfb-9fd47b4e3eb0"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version:  2.9.2\n",
            "Keras version:  2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import rest of Library"
      ],
      "metadata": {
        "id": "mTW-hEgnlp44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# from https://github.com/keras-team/keras/issues/2115#issuecomment-204060456\n",
        "# witha correction on the weighted function in the middle \n",
        "\n",
        "'''Train a simple deep NN on the MNIST dataset.\n",
        "Get to 98.40% test accuracy after 20 epochs\n",
        "(there is *a lot* of margin for parameter tuning).\n",
        "2 seconds per epoch on a K520 GPU.\n",
        "'''\n",
        "\n",
        "from __future__ import print_function  #do i still need this?\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "from keras.utils import np_utils\n",
        "import keras.backend as K\n",
        "from itertools import product\n",
        "import functools\n",
        "from functools import partial\n",
        "\n",
        "from time import sleep\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd \n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "## MORE REPEATABILITY STUFF NEEDED - If theres a way to update this to V2 of Tensorflow great, otherwise I had to use TF 1.0 code\n",
        "# 5. Configure a new global `tensorflow` session (https://stackoverflow.com/questions/50659482/why-cant-i-get-reproducible-results-in-keras-even-though-i-set-the-random-seeds)\n",
        "# from keras import backend as K\n",
        "\n",
        "\n",
        "#I believe thecode below is to help things be repeatable each time different sections in my google colab notebook execute\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)"
      ],
      "metadata": {
        "id": "idfYNyyAgMsO"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define batch, epochs, and format data"
      ],
      "metadata": {
        "id": "otcbfKF7mY9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 5000 # I originally had it very  high batch size to reduce the variation in the data each batch and hope it makes the model training more nearly identical which it did, then i bring it back down to something reasonable to get better results training the NN\n",
        "nb_classes = 10\n",
        "nb_epoch = 4\n",
        "\n",
        "# the data, shuffled and split between train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "X_train = X_train.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, nb_classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B59UXDb8i8W5",
        "outputId": "d823ffff-0d26-4f7c-8215-28c1cc709ca1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define weighted_categorical_crossentropy()"
      ],
      "metadata": {
        "id": "Y102hWcfvFyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# https://github.com/keras-team/keras/issues/2115#issuecomment-207765342\n",
        "\n",
        "def w_categorical_crossentropy(y_true, y_pred, weights):\n",
        "    nb_cl = len(weights)\n",
        "    final_mask = K.zeros_like(y_pred[:, 0])\n",
        "    y_pred_max = K.max(y_pred, axis=1)\n",
        "    y_pred_max = K.expand_dims(y_pred_max, 1)\n",
        "    y_pred_max_mat = K.equal(y_pred, y_pred_max)\n",
        " \n",
        "    for c_t, c_p in product(range(nb_cl), range(nb_cl)):\n",
        "        final_mask += (K.cast(weights[c_t, c_p],K.floatx()) * K.cast(y_pred_max_mat[:, c_p] ,K.floatx())* K.cast(y_true[:, c_t],K.floatx()))\n",
        "    \n",
        "    # result = K.categorical_crossentropy(y_true, y_pred)*final_mask\n",
        "    # tf.print(result, \"Show Result of CE * Final_Mask\")  #this was basically useless to display, and it showed like, 500 lines of print statements each epoch\n",
        "\n",
        "    return K.categorical_crossentropy(y_true, y_pred)*final_mask   #I changed the order of y_true and y_pred\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fNienjOxgQVq"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Same Model but use normal Categorical CrossEntropy with no extra cost-matrix of Weights"
      ],
      "metadata": {
        "id": "HtojTVZBvLEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normal_method():\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Dense(512, input_shape=(784,) ,kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "  model.add(Activation('relu'))\n",
        "  # model.add(Dropout(0.2))\n",
        "  model.add(Dense(512, kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "  model.add(Activation('relu'))\n",
        "  # model.add(Dropout(0.2))\n",
        "  model.add(Dense(10, kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  rms = RMSprop()\n",
        "  # model.compile(loss=ncce, optimizer=rms)\n",
        "  model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=rms, metrics='categorical_accuracy', )\n",
        "\n",
        "  #add early_stop to prevent overfittings\n",
        "  # callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "  model_history = model.fit(X_train, Y_train,\n",
        "            batch_size=batch_size, epochs=nb_epoch, verbose=0,\n",
        "            validation_data=(X_test, Y_test),shuffle=False, use_multiprocessing=False\n",
        "            # callbacks = [callback])\n",
        "            )\n",
        "\n",
        "  \n",
        "  # model.evaluate(X_test, Y_test, verbose=1)  # I know this isn't the typical use of train/val/test sets, please dont' comment on that\n",
        "  \n",
        "  #Predict\n",
        "  y_prediction = model.predict(X_test)\n",
        "  y_prediction  = np.argmax(y_prediction, axis=1)\n",
        "  # Y_prediction = np_utils.to_categorical(y_prediction, nb_classes) #If I want to do SparseCategoricalAccuracy\n",
        "\n",
        "  #Create confusion matrix and normalizes it over predicted (columns)\n",
        "  # result = confusion_matrix(y_test, y_prediction , normalize='pred')  #if I want percentages instead of raw counts\n",
        "\n",
        "  \n",
        "  cm = confusion_matrix(y_test, y_prediction)\n",
        "  cm = pd.DataFrame(cm, range(10),range(10))\n",
        "\n",
        "  #This shows a pretty confusion matrix which I don't neeed to show right now\n",
        "  # plt.figure(figsize = (10,10))\n",
        "  # sns.heatmap(cm, annot=True, annot_kws={\"size\": 12}) # font size\n",
        "  # plt.show()\n",
        "  cm_normal = cm\n",
        "\n",
        "  return model_history\n"
      ],
      "metadata": {
        "id": "InYvpv3kaCxb"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeatable!\n",
        "\n",
        "Epoch 1/4\n",
        "6/6 [==============================] - 3s 491ms/step - **loss: 1.5304** - categorical_accuracy: 0.5081 - val_loss: 0.7234 - val_categorical_accuracy: 0.8170\n",
        "Epoch 2/4\n",
        "6/6 [==============================] - 3s 467ms/step - loss: **0.6541** - categorical_accuracy: 0.8048 - val_loss: 0.5255 - val_categorical_accuracy: 0.8161\n",
        "Epoch 3/4\n",
        "6/6 [==============================] - 3s 467ms/step - loss: 0.4883 - categorical_accuracy: 0.8496 - val_loss: 0.4127 - val_categorical_accuracy: 0.8734\n",
        "Epoch 4/4\n",
        "6/6 [==============================] - 3s 475ms/step - loss: 0.3893 - categorical_accuracy: 0.8840 - val_loss: 0.3287 - val_categorical_accuracy: 0.9073\n",
        "313/313 [==============================] - 1s 3ms/step - loss: 0.3287 - categorical_accuracy: 0.9073\n",
        "313/313 [==============================] - 1s 2ms/step"
      ],
      "metadata": {
        "id": "Ctb1l69n8F9f"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cw2zBqpvvzi0"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define 2nd Model using loss= Weighted_Cross_Entropy and \"normal\" cost matrix"
      ],
      "metadata": {
        "id": "YbD_6_izamg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #what does ncce stand for? non-uniform cost cross entropy? tboquet on Keras Issues Team named it this way March 31 2016\n",
        "\n",
        "def tb_method():\n",
        "  w_array = np.ones((10,10))\n",
        "  # w_array[9, 7] = 1.5\n",
        "  # w_array = w_array - np.eye(10)\n",
        "  # print(\"W_array:  \", w_array)\n",
        "\n",
        "  # ncce = partial(w_categorical_crossentropy, weights=np.ones((10,10)))\n",
        "\n",
        "\n",
        "  ncce = partial(w_categorical_crossentropy, weights=w_array)\n",
        "  ncce.__name__ ='w_categorical_crossentropy'\n",
        "\n",
        "  model2 = Sequential()\n",
        "  model2.add(Dense(512, input_shape=(784,) ,kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "  model2.add(Activation('relu'))\n",
        "  # model2.add(Dropout(0.2))\n",
        "  model2.add(Dense(512, kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "  model2.add(Activation('relu'))\n",
        "  # model2.add(Dropout(0.2))\n",
        "  model2.add(Dense(10, kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "  model2.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "  rms = RMSprop()\n",
        "\n",
        "  model2.compile(loss=ncce, optimizer=rms,  metrics='categorical_accuracy',)\n",
        "  # callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "  model2_history = model2.fit(X_train, Y_train,\n",
        "            batch_size=batch_size, epochs=nb_epoch, verbose=0,\n",
        "            validation_data=(X_test, Y_test), shuffle=False, use_multiprocessing=False\n",
        "            #callbacks = [callback])\n",
        "  )\n",
        "\n",
        "  # len(history.history['loss'])\n",
        "\n",
        "  # print('Test score:', score[0])\n",
        "  # print('Test accuracy:', score[1])\n",
        "\n",
        "  from sklearn.metrics import confusion_matrix\n",
        "\n",
        "  #Predict\n",
        "  y_prediction = model2.predict(X_test)\n",
        "  y_prediction  = np.argmax(y_prediction, axis=1)\n",
        "  # Y_prediction = np_utils.to_categorical(y_prediction, nb_classes)\n",
        "\n",
        "  #Create confusion matrix and normalizes it over predicted (columns)\n",
        "  # result = confusion_matrix(y_test, y_prediction , normalize='pred')\n",
        "\n",
        "  cm2 = confusion_matrix(y_test, y_prediction)\n",
        "  cm2 = pd.DataFrame(cm2, range(10),range(10))\n",
        "  # plt.figure(figsize = (10,10))\n",
        "  # sns.heatmap(cm2, annot=True, annot_kws={\"size\": 12}) # font size\n",
        "  # plt.show()\n",
        "  \n",
        "  cm_using_weighted = cm2\n",
        "\n",
        "  return model2_history"
      ],
      "metadata": {
        "id": "dlUDblmSUQ4w"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "REPEATABLE - Epoch 1/4\n",
        "6/6 [==============================] - 11s 1s/step - loss: **1.4722** - categorical_accuracy: 0.5274 - val_loss: 0.7470 - val_categorical_accuracy: 0.7868\n",
        "Epoch 2/4\n",
        "6/6 [==============================] - 5s 773ms/step - loss: **0.6526** - categorical_accuracy: 0.7996 - val_loss: 0.5025 - val_categorical_accuracy: 0.8361\n",
        "Epoch 3/4\n",
        "6/6 [==============================] - 4s 627ms/step - loss: 0.4899 - categorical_accuracy: 0.8417 - val_loss: 0.4000 - val_categorical_accuracy: 0.8866\n",
        "Epoch 4/4\n",
        "6/6 [==============================] - 3s 478ms/step - loss: 0.3440 - categorical_accuracy: 0.9026 - val_loss: 0.3035 - val_categorical_accuracy: 0.9118\n",
        "313/313 [==============================] - 1s 3ms/step"
      ],
      "metadata": {
        "id": "7ZF_rtlt-Ppm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not Repeatable upon Re-Running (not Restart+Rerun) \n",
        "\n",
        "Epoch 1/4\n",
        "6/6 [==============================] - 6s 662ms/step - loss: 1.5140 - categorical_accuracy: 0.5150 - val_loss: 0.8487 - val_categorical_accuracy: 0.7004\n",
        "Epoch 2/4\n",
        "6/6 [==============================] - 3s 473ms/step - loss: 0.6816 - categorical_accuracy: 0.7931 - val_loss: 0.4447 - val_categorical_accuracy: 0.8790\n",
        "Epoch 3/4\n",
        "6/6 [==============================] - 3s 475ms/step - loss: 0.4923 - categorical_accuracy: 0.8434 - val_loss: 0.3447 - val_categorical_accuracy: 0.9046\n",
        "Epoch 4/4\n",
        "6/6 [==============================] - 3s 475ms/step - loss: 0.3922 - categorical_accuracy: 0.8845 - val_loss: 0.3259 - val_categorical_accuracy: 0.9045\n",
        "313/313 [==============================] - 1s 3ms/step"
      ],
      "metadata": {
        "id": "s5PiqAVu-cAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not Repeatable upon Re-Running:\n",
        "\n",
        "> Epoch 1/4\n",
        "6/6 [==============================] - 6s 664ms/step - loss: 1.4762 - categorical_accuracy: 0.5231 - val_loss: 0.7467 - val_categorical_accuracy: 0.7548\n",
        "Epoch 2/4\n",
        "6/6 [==============================] - 3s 480ms/step - loss: 0.6470 - categorical_accuracy: 0.7904 - val_loss: 0.4843 - val_categorical_accuracy: 0.8507\n",
        "Epoch 3/4\n",
        "6/6 [==============================] - 3s 483ms/step - loss: 0.4990 - categorical_accuracy: 0.8395 - val_loss: 0.4301 - val_categorical_accuracy: 0.8638\n",
        "Epoch 4/4\n",
        "6/6 [==============================] - 3s 552ms/step - loss: 0.3866 - categorical_accuracy: 0.8798 - val_loss: 0.4996 - val_categorical_accuracy: 0.8318\n",
        "313/313 [==============================] - 1s 4ms/step\n"
      ],
      "metadata": {
        "id": "Laae9XYi-vX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cm_using_weighted"
      ],
      "metadata": {
        "id": "7fefs2xcwgox"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trying a user's suggestion for a more vectorized appraoch Model 3"
      ],
      "metadata": {
        "id": "3fHQHrz8MwXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from https://github.com/keras-team/keras/issues/2115#issuecomment-815825633 from Isaranto\n",
        "\n",
        "def weighted_categorical_crossentropy_new(y_true, y_pred, weights):\n",
        "          idx1 = K.argmax(y_pred, axis=1)\n",
        "          idx2 = K.argmax(y_true, axis=1)\n",
        "          mask = tf.gather_nd(weights, tf.stack((idx1, idx2), -1))\n",
        "          return K.categorical_crossentropy(y_true, y_pred) * mask"
      ],
      "metadata": {
        "id": "pUR1sLQ7MvVa"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #what does ncce stand for?\n",
        "\n",
        " \n",
        "def isaranto_method():\n",
        "  w_array = np.ones((10,10))\n",
        "  # w_array[9, 7] = 1.5\n",
        "  # w_array = w_array - np.eye(10)\n",
        "  # print(\"W_array:  \", w_array)\n",
        "\n",
        "  weighted_list = w_array.tolist()\n",
        "\n",
        "  wcce = partial(weighted_categorical_crossentropy_new, weights=weighted_list)\n",
        "  wcce.__name__ ='w_categorical_crossentropy'\n",
        "\n",
        "  model3 = Sequential()\n",
        "  model3.add(Dense(512, input_shape=(784,), kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "  model3.add(Activation('relu'))\n",
        "  # model3.add(Dropout(0.2))\n",
        "  model3.add(Dense(512, kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "  model3.add(Activation('relu'))\n",
        "  # model3.add(Dropout(0.2))\n",
        "  model3.add(Dense(10,kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "  model3.add(Activation('softmax'))\n",
        "\n",
        "  rms = RMSprop()\n",
        "\n",
        "  model3.compile(loss=wcce, optimizer=rms,  metrics='categorical_accuracy',)\n",
        "  # callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "  model3_history = model3.fit(X_train, Y_train,\n",
        "            batch_size=batch_size, epochs=nb_epoch, verbose=0,\n",
        "            validation_data=(X_test, Y_test), shuffle=False, use_multiprocessing=False\n",
        "            # callbacks = [callback]\n",
        "            )\n",
        "\n",
        "  \n",
        "  # print('Test score:', score[0])\n",
        "  # print('Test accuracy:', score[1])\n",
        "\n",
        "\n",
        "  #Predict\n",
        "  y_prediction = model3.predict(X_test)\n",
        "  y_prediction  = np.argmax(y_prediction, axis=1)\n",
        "  # Y_prediction = np_utils.to_categorical(y_prediction, nb_classes)\n",
        "\n",
        "  #Create confusion matrix and normalizes it over predicted (columns)\n",
        "  # result = confusion_matrix(y_test, y_prediction , normalize='pred')\n",
        "\n",
        "  \n",
        "\n",
        "  cm3 = confusion_matrix(y_test, y_prediction)\n",
        "  cm3 = pd.DataFrame(cm3, range(10),range(10))\n",
        "  # plt.figure(figsize = (10,10))\n",
        "  # cm3\n",
        "  # sns.heatmap(cm2, annot=True, annot_kws={\"size\": 12}) # font size\n",
        "  # plt.show()\n",
        "\n",
        "  # cm_using_weighted_new = cm3\n",
        "\n",
        "  return model3_history"
      ],
      "metadata": {
        "id": "3UWVdmRHNBhP"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeatable, AND This loss function looks exactly like the Normal one without specified costs!\n",
        "Epoch 1/4\n",
        "6/6 [==============================] - 3s 498ms/step - loss: **1.5185** - categorical_accuracy: 0.5047 - val_loss: 0.8206 - val_categorical_accuracy: 0.7422\n",
        "Epoch 2/4\n",
        "6/6 [==============================] - 3s 469ms/step - loss:**0.6851** categorical_accuracy: 0.7891 - val_loss: 0.5890 - val_categorical_accuracy: 0.7968\n",
        "Epoch 3/4\n",
        "6/6 [==============================] - 3s 469ms/step - loss: 0.4974 - categorical_accuracy: 0.8435 - val_loss: 0.3986 - val_categorical_accuracy: 0.8833\n",
        "Epoch 4/4\n",
        "6/6 [==============================] - 3s 461ms/step - loss: 0.3994 - categorical_accuracy: 0.8785 - val_loss: 0.3041 - val_categorical_accuracy: 0.9101\n",
        "313/313 [==============================] - 1s 2ms/step"
      ],
      "metadata": {
        "id": "ClrFpC8V9NZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cm_using_weighted_new "
      ],
      "metadata": {
        "id": "OSq7jMYUOF4t"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cm_normal"
      ],
      "metadata": {
        "id": "auDUR-MjOIZ4"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing the loss history across the three models"
      ],
      "metadata": {
        "id": "96Y-S_zoN5Lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normal_model_history  = normal_method()\n",
        "np.round(normal_model_history.history['loss'],3)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drHZObV9MZh0",
        "outputId": "be1b0e00-de01-4adc-bcf7-d2f85145006b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 4ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.073, 0.452, 0.305, 0.311])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "is_model_history = isaranto_method()\n",
        "np.round(is_model_history.history['loss'],3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-MWWxYtMvV8",
        "outputId": "2ba0dd59-8c4c-4fb1-c2eb-0b0ab49d1049"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.094, 0.435, 0.339, 0.257])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tb_model_history = tb_method()\n",
        "np.round(tb_model_history.history['loss'],3)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7rrACD7MsL-",
        "outputId": "1bfe3f62-0d09-4ced-de28-12cba77d08e2"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.105, 0.448, 0.346, 0.267])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Return the loss history of Normal, TB, Isantro (in that order for putting ni a table to see differences, but the order ran is as given above)"
      ],
      "metadata": {
        "id": "DVdujq70VYyf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iC7SSWCPzU6E"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B5VJHuR8d2EF"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Re-Run First Model with no Weights to see if its still the same"
      ],
      "metadata": {
        "id": "2Vs3be97jmCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # # The results of running the model again as model1 were not identical to model earlier.  So I am trying to put the re-initialized seeds here to see if that makes a difference\n",
        " \n",
        "\n",
        "\n",
        "# model1 = Sequential()\n",
        "# model1.add(Dense(512, input_shape=(784,) ,kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "# model1.add(Activation('relu'))\n",
        "# # model1.add(Dropout(0.2))\n",
        "# model1.add(Dense(512, kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "# model1.add(Activation('relu'))\n",
        "# # model1.add(Dropout(0.2))\n",
        "# model1.add(Dense(10, kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42)))\n",
        "# model1.add(Activation('softmax'))\n",
        "\n",
        "# rms = RMSprop()\n",
        "# # model1.compile(loss=ncce, optimizer=rms)\n",
        "# model1.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=rms, metrics='categorical_accuracy', )\n",
        "\n",
        "# #add early_stop to prevent overfittings\n",
        "# # callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "# model1_history = model1.fit(X_train, Y_train,\n",
        "#           batch_size=batch_size, epochs=nb_epoch, verbose=0,\n",
        "#           validation_data=(X_test, Y_test),shuffle=False, use_multiprocessing=False\n",
        "#           # callbacks = [callback])\n",
        "#           )\n",
        "\n",
        " \n",
        "# model1.evaluate(X_test, Y_test, verbose=1)\n",
        " \n",
        "\n",
        "\n",
        "# # Predict\n",
        "# y_prediction = model1.predict(X_test)\n",
        "# y_prediction  = np.argmax(y_prediction, axis=1)\n",
        "# # Y_prediction = np_utils.to_categorical(y_prediction, nb_classes)\n",
        "\n",
        "# #Create confusion matrix and normalizes it over predicted (columns)\n",
        "# # result = confusion_matrix(y_test, y_prediction , normalize='pred')\n",
        "\n",
        " \n",
        "# cm1 = confusion_matrix(y_test, y_prediction)\n",
        "# cm1 = pd.DataFrame(cm1, range(10),range(10))\n",
        "# # plt.figure(figsize = (10,10))\n",
        "\n",
        "# # sns.heatmap(cm1, annot=True, annot_kws={\"size\": 12}) # font size\n",
        "# # plt.show()\n",
        "\n",
        "# cm_normal_replicate = cm1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_-hBd6sNjmCv"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeatable - Epoch 1/4\n",
        "6/6 [==============================] - 5s 620ms/step - loss: 1.4564 - categorical_accuracy: 0.5507 - val_loss: 0.8112 - val_categorical_accuracy: 0.7332\n",
        "Epoch 2/4\n",
        "6/6 [==============================] - 3s 588ms/step - loss: 0.7296 - categorical_accuracy: 0.7690 - val_loss: 0.4210 - val_categorical_accuracy: 0.8938\n",
        "Epoch 3/4\n",
        "6/6 [==============================] - 4s 777ms/step - loss: 0.4762 - categorical_accuracy: 0.8580 - val_loss: 0.3746 - val_categorical_accuracy: 0.8962\n",
        "Epoch 4/4\n",
        "6/6 [==============================] - 3s 584ms/step - loss: 0.3397 - categorical_accuracy: 0.9036 - val_loss: 0.4469 - val_categorical_accuracy: 0.8487\n",
        "313/313 [==============================] - 1s 3ms/step - loss: 0.4469 - categorical_accuracy: 0.8487\n",
        "313/313 [==============================] - 1s 3ms/step"
      ],
      "metadata": {
        "id": "Z5vcyU3REuon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch 1/4\n",
        "6/6 [==============================] - 3s 501ms/step - loss: **1.5185** - categorical_accuracy: 0.5047 - val_loss: 0.8206 - val_categorical_accuracy: 0.7422\n",
        "Epoch 2/4\n",
        "6/6 [==============================] - 3s 472ms/step - loss: **0.6851** - categorical_accuracy: 0.7891 - val_loss: 0.5890 - val_categorical_accuracy: 0.7968\n",
        "Epoch 3/4\n",
        "6/6 [==============================] - 3s 474ms/step - loss: 0.4974 - categorical_accuracy: 0.8435 - val_loss: 0.3986 - val_categorical_accuracy: 0.8833\n",
        "Epoch 4/4\n",
        "6/6 [==============================] - 3s 477ms/step - loss: 0.3994 - categorical_accuracy: 0.8785 - val_loss: 0.3041 - val_categorical_accuracy: 0.9101\n",
        "313/313 [==============================] - 1s 3ms/step - loss: 0.3041 - categorical_accuracy: 0.9101\n",
        "313/313 [==============================] - 1s 3ms/step"
      ],
      "metadata": {
        "id": "DkDmxY0x87LI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not Repeatable - \n",
        "Epoch 1/4\n",
        "6/6 [==============================] - 3s 496ms/step - loss: **1.4722**- categorical_accuracy: 0.5274 - val_loss: 0.7470 - val_categorical_accuracy: 0.7868\n",
        "Epoch 2/4\n",
        "6/6 [==============================] - 3s 464ms/step - loss: **0.6526** categorical_accuracy: 0.7996 - val_loss: 0.5025 - val_categorical_accuracy: 0.8361\n",
        "Epoch 3/4\n",
        "6/6 [==============================] - 4s 623ms/step - loss: 0.4899 - categorical_accuracy: 0.8417 - val_loss: 0.4000 - val_categorical_accuracy: 0.8866\n",
        "Epoch 4/4\n",
        "6/6 [==============================] - 3s 464ms/step - loss: 0.3440 - categorical_accuracy: 0.9026 - val_loss: 0.3035 - val_categorical_accuracy: 0.9118\n",
        "313/313 [==============================] - 1s 3ms/step - loss: 0.3035 - categorical_accuracy: 0.9118\n",
        "313/313 [==============================] - 1s 3ms/step"
      ],
      "metadata": {
        "id": "5q8d8SCn8wFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not Repeatable - Epoch 1/4\n",
        "6/6 [==============================] - 4s 497ms/step - **loss: 1.4564**- categorical_accuracy: 0.5507 - val_loss: 0.8112 - val_categorical_accuracy: 0.7332\n",
        "Epoch 2/4\n",
        "6/6 [==============================] - 3s 482ms/step - loss: **0.7296** - categorical_accuracy: 0.7690 - val_loss: 0.4210 - val_categorical_accuracy: 0.8939\n",
        "Epoch 3/4\n",
        "6/6 [==============================] - 3s 487ms/step - loss: 0.4762 - categorical_accuracy: 0.8579 - val_loss: 0.3743 - val_categorical_accuracy: 0.8962\n",
        "Epoch 4/4\n",
        "6/6 [==============================] - 3s 473ms/step - loss: 0.3394 - categorical_accuracy: 0.9038 - val_loss: 0.4460 - val_categorical_accuracy: 0.8490\n",
        "313/313 [==============================] - 1s 3ms/step - loss: 0.4460 - categorical_accuracy: 0.8490\n",
        "313/313 [==============================] - 1s 3ms/step"
      ],
      "metadata": {
        "id": "DFDECkwB9-Mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model1_history.history['loss']"
      ],
      "metadata": {
        "id": "fSc1Dh76BwJT"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(0,len(model1_history.history['loss'])):\n",
        "#   print(model1_history.history['loss'][i] - model_history.history['loss'][i])"
      ],
      "metadata": {
        "id": "0hJtI0XAkHwO"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cm1 - cm"
      ],
      "metadata": {
        "id": "j_FjgkCflbn6"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other users scratch work that I haven't incorporated or may not be useful"
      ],
      "metadata": {
        "id": "ddGKWOtKnA9g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "GF4072CD_1L3"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from keras.api._v2 import keras as tk\n",
        "# import tensorflow as tf\n",
        "# from keras.utils import losses_utils\n",
        "# import typing as t\n",
        "\n",
        "\n",
        "# class CostSensitiveLoss(tk.losses.Loss):\n",
        "\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         cost_matrix: t.List, loss: tk.losses.Loss,\n",
        "#     ):\n",
        "#         super().__init__(reduction=loss.reduction, name=loss.name)\n",
        "#         self.loss = loss\n",
        "#         self.cost_matrix = cost_matrix\n",
        "#         self._cost_matrix = tf.constant(cost_matrix, dtype=tf.float32)\n",
        "\n",
        "#     @classmethod\n",
        "#     def from_config(cls, config):\n",
        "#         config['loss'] = tk.losses.deserialize(config['loss'])\n",
        "#         return cls(**config)\n",
        "\n",
        "#     def get_config(self):\n",
        "#         return {\n",
        "#             'cost_matrix': self.cost_matrix,\n",
        "#             'loss': tk.losses.serialize(self.loss),\n",
        "#             'reduction': self.reduction, 'name': self.name\n",
        "#         }\n",
        "    \n",
        "#     def call(self, y_true, y_pred):\n",
        "#         # if y_true is one hot encoded then get integer indices\n",
        "#         if y_true.ndim == 1:\n",
        "#             y_true_index = y_true\n",
        "#         elif y_true.ndim == 2:\n",
        "#             y_true_index = tf.argmax(y_true, axis=1)\n",
        "#         else:\n",
        "#             raise Exception(f\"`y_true.ndim` {y_true.ndim} not supported\")\n",
        "        \n",
        "#         # get cost for batch\n",
        "#         cost_for_batch = tf.nn.embedding_lookup(self._cost_matrix, y_true_index)\n",
        "#         cost_for_batch *= y_pred\n",
        "#         cost_for_batch = tf.reduce_sum(cost_for_batch, axis=1)\n",
        "        \n",
        "#         # get loss\n",
        "#         return self.loss(y_true, y_pred, cost_for_batch)\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     # for debug purpose I have kept 'none' you can\n",
        "#     # safely use other options like 'sum', 'auto'\n",
        "#     _loss = tk.losses.MeanAbsoluteError(reduction='none')\n",
        "    \n",
        "#     # some cost matrices the first cost matrix is the case when you are\n",
        "#     # not using cost sensitive weights\n",
        "#     _cs_loss_1 = CostSensitiveLoss(\n",
        "#         cost_matrix=[[1, 1, 1], [1, 1, 1], [1, 1, 1], ],\n",
        "#         loss=_loss\n",
        "#     )\n",
        "#     _cs_loss_2 = CostSensitiveLoss(\n",
        "#         cost_matrix=[[1, 2, 2], [4, 1, 4], [8, 8, 1], ],\n",
        "#         loss=_loss\n",
        "#     )\n",
        "#     _cs_loss_3 = CostSensitiveLoss(\n",
        "#         cost_matrix=[[1, 4, 8], [2, 1, 8], [2, 4, 1], ],\n",
        "#         loss=_loss\n",
        "#     )\n",
        "#     _y_true = np.asarray(\n",
        "#         [\n",
        "#             [1, 0, 0],\n",
        "#             [0, 1, 0],\n",
        "#             [0, 0, 1],\n",
        "#             [1, 0, 0],\n",
        "#             [0, 1, 0],\n",
        "#             [0, 0, 1],\n",
        "#             [1, 0, 0],\n",
        "#             [0, 1, 0],\n",
        "#             [0, 0, 1],\n",
        "#         ]\n",
        "#     )\n",
        "#     _y_pred = np.asarray(\n",
        "#         [\n",
        "#             [0.8, 0.1, 0.1],\n",
        "#             [0.1, 0.8, 0.1],\n",
        "#             [0.1, 0.1, 0.8],\n",
        "            \n",
        "#             [0.1, 0.8, 0.1],\n",
        "#             [0.1, 0.1, 0.8],\n",
        "#             [0.8, 0.1, 0.1],\n",
        "            \n",
        "#             [0.1, 0.1, 0.8],\n",
        "#             [0.8, 0.1, 0.1],\n",
        "#             [0.1, 0.8, 0.1],\n",
        "#         ]\n",
        "#     )\n",
        "#     print(\"loss ........................\")\n",
        "#     print(_loss(_y_true, _y_pred).numpy())\n",
        "    \n",
        "#     print(\"cs_loss_1 ...................\")\n",
        "#     print(_cs_loss_1(_y_true, _y_pred).numpy())\n",
        "    \n",
        "#     print(\"cs_loss_2 ...................\")\n",
        "#     print(_cs_loss_2(_y_true, _y_pred).numpy())\n",
        "    \n",
        "#     print(\"cs_loss_3 ...................\")\n",
        "#     print(_cs_loss_3(_y_true, _y_pred).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow.keras.backend as K\n",
        "# from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "\n",
        "\n",
        "# class WeightedCategoricalCrossentropy(CategoricalCrossentropy):\n",
        "    \n",
        "#     def __init__(self, cost_mat, name='weighted_categorical_crossentropy', **kwargs):\n",
        "#         assert cost_mat.ndim == 2\n",
        "#         assert cost_mat.shape[0] == cost_mat.shape[1]\n",
        "        \n",
        "#         super().__init__(name=name, **kwargs)\n",
        "#         self.cost_mat = K.cast_to_floatx(cost_mat)\n",
        "    \n",
        "#     def __call__(self, y_true, y_pred, sample_weight=None):\n",
        "#         assert sample_weight is None, \"should only be derived from the cost matrix\"\n",
        "      \n",
        "#         return super().__call__(\n",
        "#             y_true=y_true,\n",
        "#             y_pred=y_pred,\n",
        "#             sample_weight=get_sample_weights(y_true, y_pred, self.cost_mat),\n",
        "#         )\n",
        "\n",
        "\n",
        "# def get_sample_weights(y_true, y_pred, cost_m):\n",
        "#     num_classes = len(cost_m)\n",
        "\n",
        "#     y_pred.shape.assert_has_rank(2)\n",
        "#     y_pred.shape[1:].assert_is_compatible_with(num_classes)\n",
        "#     y_pred.shape.assert_is_compatible_with(y_true.shape)\n",
        "\n",
        "#     y_pred = K.one_hot(K.argmax(y_pred), num_classes)\n",
        "\n",
        "#     y_true_nk1 = K.expand_dims(y_true, 2)\n",
        "#     y_pred_n1k = K.expand_dims(y_pred, 1)\n",
        "#     cost_m_1kk = K.expand_dims(cost_m, 0)\n",
        "\n",
        "#     sample_weights_nkk = cost_m_1kk * y_true_nk1 * y_pred_n1k\n",
        "#     sample_weights_n = K.sum(sample_weights_nkk, axis=[1, 2])\n",
        "\n",
        "#     return sample_weights_n\n",
        "# # Usage:\n",
        "\n",
        "# model.compile(loss=WeightedCategoricalCrossentropy(cost_matrix), ...)"
      ],
      "metadata": {
        "id": "gtMZaFcJa6td"
      },
      "execution_count": 61,
      "outputs": []
    }
  ]
}