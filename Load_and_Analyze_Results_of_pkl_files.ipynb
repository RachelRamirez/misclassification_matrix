{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXIaQ1rzh37e1KfefFaKnQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RachelRamirez/misclassification_matrix/blob/main/Load_and_Analyze_Results_of_pkl_files.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Results\n",
        "\n",
        "I have a lot of pickle files.  Let's see if I can load them all up and put in a csv format."
      ],
      "metadata": {
        "id": "PaeHBLgDLEbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Qwl3UxhpLECp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "649953e5-526e-4581-c86b-5fa93d3d753e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "tuUF7Fl1VDgI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After executing the cell above, Drive\n",
        "# files will be present in \"/content/drive/My Drive\".\n",
        "# !ls \"/content/drive/My Drive/Con3\"\n",
        "\n",
        "path = \"/content/drive/MyDrive/Con3/\"\n",
        "\n",
        "# file_name = \"delete_later.pkl\"\n",
        "\n",
        "import pickle \n",
        "\n",
        "import os\n",
        "\n",
        "dir = path\n",
        "\n",
        "dict_of_run_files = {}\n",
        "\n",
        "count = 0\n",
        "for file in os.listdir(path):\n",
        "  if file.endswith(\".pkl\") and file.startswith(\"run\"):\n",
        "    # myfunction(file)\n",
        "    with open(path + file, 'rb') as handle:\n",
        "    # Call load method to deserialze\n",
        "      dict_of_run_files[file] = pickle.load(handle)\n",
        "      # list_of_files.extend(value for name, value in sorted(os.listdir(path)).items(), key=lambda item: item[0]) if name.startswith('run')\n",
        "      count+=1 \n",
        "      # print(unpickled_object == variable_to_be_deleted)\n",
        "      # print(count, \". \", file, \" was saved to dict_of_run_files. \")\n",
        "\n",
        "      "
      ],
      "metadata": {
        "id": "EgWoXskENjTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to get into the \"dict_of_run_files\" variable and dig out each of the contents we need in each pickle file.\n",
        "\n",
        "We need at least the following information in a dataframe:"
      ],
      "metadata": {
        "id": "TMObembs7Ayj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "UniqueID | Date_Time |  Run | Rep | Lambda = (i,j,k) | Num of Epochs = (E1, E2, E3) | EarlyStop Patience = (P1, P2, P3) | Train/Val Accuracy | # of Misclassifications of [9t, 4p] | # Reverse Misclassifications | Final CM | Seed1 | Seed2 \n",
        "-- | -- | --| --| --| --| --| --| -- | --| --| --| --\n",
        "int | string | int | int | tuple | tuple | tuple | float | int | int | (1,100) (1,100), (1,100) | int | int\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VoLDrqp_6wjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# column_name_list = [\"UniqueID\" , \"Date_Time\" ,  \"Run\" , \"Rep\" , \"Lambda = (i,j,k)\" , \"Epochs = (E1, E2, E3)\" , \"EarlyStop Patience = (P1, P2, P3)\" , \"Val Accuracy\" , \"# of Misclassifications of [9t, 4p]\" , \"# Reverse Misclassifications\" ,\"Final CM\", \"Seed1\" , \"Seed2\" ]\n"
      ],
      "metadata": {
        "id": "8-yeHkWo7TvG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unique ID can come later from just the order of the index or the key-name-of the file since each one has a unique last file_name.\n",
        "\n",
        "Date_Time can come from the last part of the file name string, although in hind sight would have been nice to save to the admin key\n",
        "\n",
        "Run really can't be a trusted Run Number anymore since I had some trouble with it, but can be cleaned up based off the lambda-epochs-patience values.\n",
        "\n",
        "Rep is Rep specified in the Dictionary.\n",
        "\n"
      ],
      "metadata": {
        "id": "lC5sDUf27XqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dict_of_run_files.keys()\n",
        "# An example of a key is 'run1_rep_1_w[9,4]_L_1_1_1_E_5_5_25_P_0_0_25__2023_03_09_1329_.pkl'\n",
        "\n",
        "import pandas as pd\n",
        "data = pd.DataFrame()\n",
        "\n",
        "# I later had the great idea of adding an admin file so some files have it some don't.\n",
        "\n",
        "count = 0\n",
        "for file in os.listdir(path):\n",
        "  if file.endswith(\".pkl\") and file.startswith(\"run\"):\n",
        "    # for key in dict_of_run_files.keys():\n",
        "      try:\n",
        "        dict_of_run_files[file][list(dict_of_run_files[file].keys())[0]]['admin']\n",
        "      except:\n",
        "        print(count, \"No Admin Key\")\n",
        "        data[count] = [file, dict_of_run_files[file][list(dict_of_run_files[file].keys())[0]]]\n",
        "      else:\n",
        "      # for variable in list((\"lambda1\", \"lambda2\", \"lambda3\")):\n",
        "      #   # print(variable)\n",
        "        data[count] = [file, dict_of_run_files[file][list(dict_of_run_files[file].keys())[0]]]\n",
        "      count+=1\n",
        "# print(data.head)\n",
        "\n",
        "data_transpose = data.transpose()\n",
        "del(data)\n",
        "\n",
        "# data_transpose.columns\n",
        "\n",
        "# The try block lets you test a block of code for errors.\n",
        "# The except block lets you handle the error.\n",
        "# The else block lets you execute code when there is no error.\n",
        "# The finally block lets you execute code, regardless of the result of the try- and except blocks.\n",
        "\n",
        "# #This file does:\n",
        "# print(\"Run:\" , dict_of_run_files['run23_rep_1_w[9,4]_L_100_1_100_E_5_5_25_P_0_0_0__2023_03_09_1434_.pkl'][(23,1)]['admin']['run'])\n",
        "# print(\"Rep:\" , dict_of_run_files['run23_rep_1_w[9,4]_L_100_1_100_E_5_5_25_P_0_0_0__2023_03_09_1434_.pkl'][(23,1)]['admin']['rep'])\n",
        "\n",
        "#so when filling in my dataframe with information I may need to use a for loop with an try-exception line.\n",
        "\n"
      ],
      "metadata": {
        "id": "2HeQTBi8aWPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # data_transpose[0] #the \"0\" is the first column which is now the filename\n",
        "# data_transpose[1][0] #index 0 of column 1 is all the information saved from the pickle file, minus the \"key\". it still appears to be in dictionary format\n",
        "# data_transpose[1][0][\"First\"]  # this is the \"first\" training phase history\n",
        "data = pd.DataFrame()\n",
        "data['categorical_accuracy'] = np.ones((2,2)).tolist()\n",
        "data['val_categorical_accuracy'] =np.ones((2,2)).tolist()\n",
        "data['loss'] = np.ones((2,2)).tolist()\n",
        "data['val_loss'] = np.ones((2,2)).tolist()\n",
        "data['4T_9P'] = np.ones((2,2)).tolist()\n",
        "data['9T_4P'] =np.ones((2,2)).tolist()\n",
        "data['3ConfusionMatrices'] =np.ones((2,2)).tolist()\n",
        "\n",
        "\n",
        "\n",
        "# Create a list of columns for the data-frame Data where you can no longer subset into smaller keys\n",
        "# list_of_columns_for_data = []\n",
        "#gather all the unique keys in the many nested dictionary files to define a new dataframe with those columns\n",
        "for row in range(0,len(data_transpose)):\n",
        "  data.at[row, 'filename'] = data_transpose[0][row]\n",
        "  for column in list(data_transpose[1][row].keys()):\n",
        "    try:\n",
        "      len(list(data_transpose[1][row][column].keys()))\n",
        "    except AttributeError:\n",
        "      # print(f'data_transpose[1][{row}][{column}] has no keys')\n",
        "      # list_of_columns_for_data.append(column)\n",
        "      if(column=='3ConfusionMatrices'):\n",
        "        data.at[row, column] = pd.DataFrame([data_transpose[1][row][column].tolist()])\n",
        "      else:\n",
        "        data.at[row, column] = data_transpose[1][row][column]\n",
        "\n",
        "\n",
        "    else:\n",
        "      # print(f'data_transpose[1][{row}][{column}] has keys')\n",
        "\n",
        "      for subcolumn in list(data_transpose[1][row][column].keys()):\n",
        "        # list_of_columns_for_data.append(subcolumn)\n",
        "        # print(f'key: subcolumn {subcolumn}')\n",
        "        try:\n",
        "          data.at[row, subcolumn] = data_transpose[1][row][column][subcolumn]\n",
        "        except KeyError:\n",
        "          # print(\"KeyError\")\n",
        "          data.at[row, subcolumn] = pd.DataFrame([data_transpose[1][row][column][subcolumn].tolist()])\n",
        "        except TypeError:\n",
        "          # print(\"TypeError\")\n",
        "          data.at[row, subcolumn] = pd.DataFrame([data_transpose[1][row][column][subcolumn].tolist()])\n",
        "        except ValueError:  #categorical accuracy causes a value error because you can't set the dataframe value to an array\n",
        "          # print(\"ValueError\")\n",
        "          data.at[row, subcolumn] = data_transpose[1][row][column][subcolumn]\n",
        "        # data.at[row, subcolumn] = data_transpose[1][row][column][subcolumn]\n",
        "\n",
        "        #I also want the last value listed in '4T_9P', '9P_4P', 'val_loss',  'val_categorical_accuracy',  'loss',  'val_loss',\n",
        "        if(subcolumn == '4T_9P' ):\n",
        "          # print(str(subcolumn),\" last_value is \" , data_transpose[1][row][column][subcolumn][-1])\n",
        "          new_column_name = str(subcolumn) + \"_last_value\"\n",
        "          data.at[row, new_column_name] = data_transpose[1][row][column][subcolumn][-1]\n",
        "          new_column_name2 = str(subcolumn) + \"_length\"\n",
        "          data.at[row, new_column_name2] = len(data_transpose[1][row][column][subcolumn])\n",
        "          \n",
        "        elif(subcolumn == '9T_4P' ):\n",
        "          # print(str(subcolumn),\" last_value is \" , data_transpose[1][row][column][subcolumn][-1])\n",
        "          new_column_name = str(subcolumn) + \"_last_value\"\n",
        "          data.at[row, new_column_name] = data_transpose[1][row][column][subcolumn][-1]\n",
        "          new_column_name2 = str(subcolumn) + \"_length\"\n",
        "          data.at[row, new_column_name2] = len(data_transpose[1][row][column][subcolumn])\n",
        "\n",
        "        elif(subcolumn == 'val_loss' ):\n",
        "          # print(str(subcolumn),\" last_value is \" , data_transpose[1][row][column][subcolumn][-1])\n",
        "          new_column_name = str(subcolumn) + \"_last_value\"\n",
        "          data.at[row, new_column_name] = data_transpose[1][row][column][subcolumn][-1]\n",
        "          new_column_name2 = str(subcolumn) + \"_length\"\n",
        "          data.at[row, new_column_name2] = len(data_transpose[1][row][column][subcolumn])\n",
        "        elif(subcolumn == 'val_categorical_accuracy' ):\n",
        "          # print(str(subcolumn),\" last_value is \" , data_transpose[1][row][column][subcolumn][-1])\n",
        "          new_column_name = str(subcolumn) + \"_last_value\"\n",
        "          data.at[row, new_column_name] = data_transpose[1][row][column][subcolumn][-1]\n",
        "          new_column_name2 = str(subcolumn) + \"_length\"\n",
        "          data.at[row, new_column_name2] = len(data_transpose[1][row][column][subcolumn])\n",
        "        elif(subcolumn == 'loss' ):\n",
        "          # print(str(subcolumn),\" last_value is \" , data_transpose[1][row][column][subcolumn][-1])\n",
        "          new_column_name = str(subcolumn) + \"_last_value\"\n",
        "          data.at[row, new_column_name] = data_transpose[1][row][column][subcolumn][-1]\n",
        "          new_column_name2 = str(subcolumn) + \"_length\"\n",
        "          data.at[row, new_column_name2] = len(data_transpose[1][row][column][subcolumn])\n",
        "        elif(subcolumn == 'val_loss' ):\n",
        "          # print(str(subcolumn),\" last_value is \" , data_transpose[1][row][column][subcolumn][-1])\n",
        "          new_column_name = str(subcolumn) + \"_last_value\"\n",
        "          data.at[row, new_column_name] = data_transpose[1][row][column][subcolumn][-1]\n",
        "          new_column_name2 = str(subcolumn) + \"_length\"\n",
        "          data.at[row, new_column_name2] = len(data_transpose[1][row][column][subcolumn])\n",
        "\n",
        "        # print(subcolumn)\n",
        "\n",
        "# data\n",
        "\n",
        "\n",
        "\n",
        "#Create a column with just the last confusion matrix, the last values of 9t_4p, and 4t_9p\n",
        "len(data['categorical_accuracy'][0])  #Column 'Categorical Accuracy', Row0, shows 35 values\n",
        "data['categorical_accuracy'][0][-1]\n",
        "data['val_categorical_accuracy'][0][-1]\n",
        "len(data['9T_4P'][0])  #The array of values for '9T_4P' is length 35, and the last value is 12 \n",
        "data['9T_4P'][0][-1]  # to get the last value for row 0\n",
        " \n",
        "    \n",
        "# data_csv = data.to_csv(\"data.csv\")\n",
        "  \n",
        "# from google.colab import files\n",
        "# files.download('data.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "sh4P_lYIyEMD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b22f00-6cda-45ab-97ac-01c620a9dedc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# len(data_transpose)\n",
        "data['3ConfusionMatrices'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "mvLvT0a0Krql",
        "outputId": "115b1851-3733-49e7-d042-f4866569c42a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   0  \\\n",
              "0  [704, 0, 1, 0, 0, 3, 7, 2, 2, 0, 0, 833, 4, 2,...   \n",
              "\n",
              "                                                   1  \\\n",
              "0  [710, 0, 1, 0, 0, 3, 3, 1, 1, 0, 0, 836, 3, 3,...   \n",
              "\n",
              "                                                   2  \n",
              "0  [706, 0, 2, 1, 0, 4, 2, 3, 1, 0, 0, 837, 4, 3,...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1069259d-b8b0-4d06-a4fa-e4661abe0f8d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[704, 0, 1, 0, 0, 3, 7, 2, 2, 0, 0, 833, 4, 2,...</td>\n",
              "      <td>[710, 0, 1, 0, 0, 3, 3, 1, 1, 0, 0, 836, 3, 3,...</td>\n",
              "      <td>[706, 0, 2, 1, 0, 4, 2, 3, 1, 0, 0, 837, 4, 3,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1069259d-b8b0-4d06-a4fa-e4661abe0f8d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1069259d-b8b0-4d06-a4fa-e4661abe0f8d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1069259d-b8b0-4d06-a4fa-e4661abe0f8d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_to_save = data[['lambda1',\t'lambda2',\t'lambda3',\t'epochs1',\t'epochs2',\t'epochs3',\t'es1',\t'es2',\t'es3', '9T_4P_last_value',\t'4T_9P_last_value', 'val_categorical_accuracy_last_value', '3ConfusionMatrices', 'filename'\t]]\n",
        "data_to_save\n",
        "# list_of_columns_for_data\n",
        "\n",
        "\n",
        "    \n",
        "data_csv = data_to_save.to_csv(\"data.csv\")\n",
        "  \n",
        "from google.colab import files\n",
        "files.download('data.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "YRqszN9qXjVt",
        "outputId": "58564aa2-f7f2-41c1-c9ff-fb2878dba288"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_929594d7-466e-4cac-8ec7-c24b5ea40101\", \"data.csv\", 359902)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save a file to myDrive/con3"
      ],
      "metadata": {
        "id": "8My0DZuBOBG1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyze \n",
        "\n",
        "\\\\\n"
      ],
      "metadata": {
        "id": "NP_kxNkhn6Fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# empty_cm = np.zeros((10,10))  \n",
        "# empty_cm=pd.DataFrame(empty_cm)\n",
        "\n",
        "# empty_cm.columns = ['0p', '1p', '2p', '3p', '4p', '5p', '6p', '7p', '8p', '9p']\n",
        "# empty_cm.index = ['0t', '1t', '2t', '3t', '4t', '5t', '6t', '7t', '8t', '9t']\n",
        "\n",
        "# # print(myvar_cm_average)\n",
        "\n",
        "# empty_cm_array = np.asarray(empty_cm)\n",
        "# empty_cm_array_1_100 = np.reshape(empty_cm_array,(1,100))\n",
        "# # print(cm_average_array)\n",
        "\n",
        "# df = empty_cm\n",
        "# df_new = pd.DataFrame(empty_cm_array_1_100,  columns=pd.MultiIndex.from_product([ df.index,df.columns]))\n",
        "# df_new.columns.to_flat_index()\n",
        "# df_new.columns   = ['_'.join(col) for col in df_new.columns.values]\n",
        "\n",
        "# # Now convert combined_cms of size 30x100 to a panda dataframe\n",
        "# cms_df = pd.DataFrame(combined_cms, columns=[df_new.columns], index=[\"First\", \"Second\", \"Third\"])\n",
        "\n",
        "# cms_df\n",
        "\n",
        "# for run in runs:\n",
        "#   for rep in reps:\n",
        "#     for phase in phases:\n",
        "#       pd.DataFrame(combined_history_dictionary[run,rep][\"3ConfusionMatrices\"], columns=[df_new.columns], index=[\"First\", \"Second\", \"Third\"])"
      ],
      "metadata": {
        "id": "kLrNJE0s53pb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combined_cms[0,]"
      ],
      "metadata": {
        "id": "eo_ahxwTVI0T"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combined_cms_df[\"9t_4p\"]"
      ],
      "metadata": {
        "id": "HgPClcrAUdxu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# np.average(combined_cms_df[\"9t_4p\"])"
      ],
      "metadata": {
        "id": "TY59hUTadVyp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# csv_filename = file_name[:-4] + \".csv\"\n",
        "\n",
        "# combined_cms_df.to_csv(csv_filename)\n",
        "# # \n",
        "\n",
        "# from google.colab import files\n",
        "# files.download(csv_filename )\n",
        "\n",
        "# print(\"Downloading \", csv_filename , \" of shape \", combined_cms_df.shape)"
      ],
      "metadata": {
        "id": "ceUFAr_z9xsk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.DataFrame(sum(var)/len(var), columns=[\"Values\"]) \n",
        "# # print(df)\n",
        "\n",
        "# df.style.format({\n",
        "#   'Values': lambda val: f'{val:,.2f}',\n",
        "# })\n",
        "\n",
        "# (df.sort_values(by=\"Values\", ascending=False)[0:20])\n",
        "\n",
        "\n",
        "# df_sorted = df.sort_values(by=\"Values\", ascending=False)[10:]  #the top 10 are usually diagonal\n",
        "\n",
        "\n",
        "# df_sorted.style.format({\n",
        "#   'Values': lambda val: f'{val:,.2f}',\n",
        "# })\n",
        "\n",
        "# import math\n",
        "\n",
        "# print(\"On average...\")\n",
        "# print(\"Num 1 misclassifications are misclassifying a \", math.floor((df_sorted[\"Values\"].index[0])/10), \" as a \", df_sorted[\"Values\"].index[0]%10, \"  (\", (df_sorted[\"Values\"].values[0]), \" times)\" )\n",
        "# print(\"Num 2 misclassifications are misclassifying a \", math.floor((df_sorted[\"Values\"].index[1])/10), \" as a \", df_sorted[\"Values\"].index[1]%10, \"  (\", (df_sorted[\"Values\"].values[1]), \" times)\" )\n",
        "# print(\"Num 3 misclassifications are misclassifying a \", math.floor((df_sorted[\"Values\"].index[2])/10), \" as a \", df_sorted[\"Values\"].index[2]%10, \"  (\", (df_sorted[\"Values\"].values[2]), \" times)\" )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-pGNLuE8gNrB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_percents = pd.DataFrame( ((sum(var)*100/len(var)).reshape((10,10))/truth_num_per_category).reshape((100)), columns = [\"Values\"])\n",
        "\n",
        "\n",
        "# df_sorted_percents = df_percents.sort_values(by=\"Values\", ascending=False)[10:]  #the top 10 are usually diagonal\n",
        "\n",
        "# df_sorted_percents.style.format({\n",
        "#   'Values': lambda val: f'{val:,.2f}',\n",
        "# })\n",
        "\n",
        "# print(\"On average .. \")\n",
        "# print(\"Num 1 percent misclassifications\", math.floor((df_sorted_percents[\"Values\"].index[0])/10), \" as \", df_sorted_percents[\"Values\"].index[0]%10, (df_sorted_percents[\"Values\"].values[0]), \" percent\" )\n",
        "# print(\"Num 2 percent misclassifications\", math.floor((df_sorted_percents[\"Values\"].index[1])/10), \" as \", df_sorted_percents[\"Values\"].index[1]%10,  (df_sorted_percents[\"Values\"].values[1]), \" percent\" )\n",
        "# print(\"Num 3 percent misclassifications\", math.floor((df_sorted_percents[\"Values\"].index[2])/10), \" as \", df_sorted_percents[\"Values\"].index[2]%10, (df_sorted_percents[\"Values\"].values[2]), \" percent\" )\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AKYclir2p8wk"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}