{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "history_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RachelRamirez/misclassification_matrix/blob/main/Analyzing_Exp_5Bins_11Runs_40D_with_SGD_LR_0point1_DiffInitSeedsperRep_plus_Baselines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "45kERInVLIQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "mydrive_path = \"/content/drive/MyDrive/Con3/Exp_5bins_11Runs_40D_SGD_Lr0point1_diffSeedsPerRep\"\n",
        "\n"
      ],
      "metadata": {
        "id": "O2LZXfDynjLX",
        "outputId": "8f90a553-4d83-4107-cc68-ac54adc7ad68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# How to change the local time in Google Colab\n",
        "!rm /etc/localtime\n",
        "!ln -s /usr/share/zoneinfo/US/Eastern /etc/localtime\n",
        "!date\n",
        "\n",
        "#If this doesn't show the local time correctly, then you need to restart.\n",
        "import time\n",
        "time.localtime(time.time())\n",
        "\n",
        "import datetime\n",
        "\n",
        "# get current date and time\n",
        "now = datetime.datetime.now()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01ccb78d-256c-4871-be0d-716e2e784ee6",
        "id": "bmXq1t0jROYK"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed 17 May 2023 10:21:29 AM EDT\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "0-F4eTC8ROYK"
      },
      "source": [
        "We'll start by writing a `.py` file which we'll import."
      ],
      "cell_type": "markdown"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# import ImportLibraries_DefineFunctions\n",
        "import requests\n",
        "url = 'https://raw.githubusercontent.com/RachelRamirez/misclassification_matrix/main/ImportLibraries_DefineFunctions.py'\n",
        "r = requests.get(url)\n",
        "\n",
        "\n",
        "# make sure your filename is the same as how you want to import \n",
        "with open('ImportLibraries_DefineFunctions.py', 'w') as f:\n",
        "    f.write(r.text)\n",
        "\n",
        "# now we can import\n"
      ],
      "metadata": {
        "id": "GLOmUMNMS133"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92e9a65c-00c5-4266-e22d-eba7bd11e935",
        "id": "G9HBGqzDROYL"
      },
      "source": [
        "#@title\n",
        "# #@title\n",
        "# # Bring the file into the local Python environment.\n",
        "execfile('ImportLibraries_DefineFunctions.py')\n",
        "\n",
        "# Call the function defined in the file.\n",
        "# f()\n",
        "time.localtime(time.time())"
      ],
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version:  2.12.0\n",
            "Keras version:  2.12.0\n",
            "Finished Loading Libraries\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "60000 train samples\n",
            "7500 validation samples\n",
            "2500 test samples\n",
            "Train [0.09871667 0.11236667 0.0993     0.10218333 0.09736667 0.09035\n",
            " 0.09863333 0.10441667 0.09751666 0.09915   ]\n",
            "Train # of 9s 5949.0\n",
            "Train # of 4s 5842.0\n",
            "Val [0.09586667 0.1132     0.10453334 0.10066666 0.09986667 0.09013333\n",
            " 0.09413333 0.1016     0.09746667 0.10253333]\n",
            "Val # of 9s 769.0\n",
            "Val # of 4s 749.0\n",
            "Test [0.1044 0.1144 0.0992 0.102  0.0932 0.0864 0.1008 0.1064 0.0972 0.096 ]\n",
            "Test  # of 9s 240.0\n",
            "Test  # of 4s 233.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "time.struct_time(tm_year=2023, tm_mon=5, tm_mday=17, tm_hour=10, tm_min=21, tm_sec=38, tm_wday=2, tm_yday=137, tm_isdst=1)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# #@title\n",
        "# # #As of 4/18 I am changing this code to save weights initially after five epochs using lambdavalue=1 initially\n",
        "\n",
        "# # rms = RMSprop()  #https://keras.io/api/optimizers/rmsprop/ #default learning_rate=0.001\n",
        "# sgd = SGD(learning_rate=0.1)\n",
        "\n",
        "# patience = 0\n",
        "\n",
        "\n",
        "# # SET THE IITIAL LAMBDA VALUE! \n",
        "# cost_matrix = np.ones((10,10))\n",
        "# lambda_val = 1\n",
        "\n",
        "# Truth=9\n",
        "# Predicted=4\n",
        "# cost_matrix[Truth, Predicted] = lambda_val\n",
        "\n",
        "\n",
        "# # # Define the per-epoch callback.\n",
        "# cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix, )\n",
        "# # cr_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_classification_report, )\n",
        "# # es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, verbose=1, restore_best_weights = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # ## Examples from TDS\n",
        "# # from tensorflow.keras.callbacks import LambdaCallback\n",
        "# # epoch_callback = LambdaCallback(\n",
        "# #     on_epoch_begin=lambda epoch,logs: print('Starting Epoch {}!'.format(epoch+1))\n",
        "# # )\n",
        "# # batch_loss_callback = LambdaCallback(\n",
        "# #     on_batch_end=lambda batch,logs: print('\\n After batch {}, the loss is {:7.2f}.'.format(batch, logs['loss']))\n",
        "# # )\n",
        "# # train_finish_callback = LambdaCallback(\n",
        "# #     on_train_end=lambda logs: print('Training finished!')\n",
        "# # )\n",
        "\n",
        "# # # Lambda function using if else & else if\n",
        "# # min = lambda a, b, c : f\"{a} is smaller\" if(a < b & b < c) \\\n",
        "# #      else f\"{b} is smaller\"  if (b < c) else f\"{c} is smaller\" \n",
        "# # print(min(40, 30, 10))\n",
        "\n",
        "# epoch_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end = lambda epoch,logs: \n",
        "#                                                    print(' Epoch {} complete {}...'.format(epoch+1), end=\" \")  if(epoch+1)%10==0 else print(\"\", end=\" \")  )\n",
        "\n",
        "\n",
        "# # cr_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end= lambda epoch, logs:\n",
        "# # #                                                log_classification_report(epoch, logs) if(epoch+1)%5==0 else print(\" \") )\n",
        "# #                                               #  return_cr if(epoch+1)%5==0 else print(\" \") )\n",
        "\n",
        "\n",
        "# f1_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_f1_score)\n",
        "\n",
        "\n",
        "# # model = create_model()\n",
        "\n",
        "# # # save the model weights\n",
        "# # model.save_weights('initial_0epochs.h5')\n",
        "\n",
        "# # model.compile(loss=WeightedCategoricalCrossentropy(cost_matrix), optimizer=sgd,  metrics='categorical_accuracy',)\n",
        "\n",
        "# # model_history = model.fit(X_train, Y_train,  batch_size=batch_size, epochs=10, verbose=2,\n",
        "# # #         validation_data=(X_val, Y_val), shuffle=True, use_multiprocessing=True, callbacks = [ cm_callback, f1_callback])\n",
        "\n",
        "# # model.save_weights('initial_10epochs.h5')\n",
        "\n",
        " \n",
        "# # cr = return_cr(model)\n",
        "# !date\n"
      ],
      "metadata": {
        "id": "-QcJ3LplOkww",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# model.load_weights('initial_10epochs.h5') \n"
      ],
      "metadata": {
        "id": "ImMl-yPWPfT0",
        "cellView": "form"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below to generate variations of the model training / validation results after training with a cost matrix 30 times did not produce any variability.   Therefore I want to shuffle the training/validation deck randomly between training sessions to see if that helps introduce some randomness.  "
      ],
      "metadata": {
        "id": "RLYZ6gR4WjKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Training Runs - Commented out to ensure not run again when the notebook is saved with output"
      ],
      "metadata": {
        "id": "V_lUeeXP2Q5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This takes hours to run for about 40 replications so usually after running, I save the out put to GitHub for easy lookup later"
      ],
      "metadata": {
        "id": "owkJgwju2a52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #@title\n",
        "# from google.colab import files\n",
        "\n",
        "# ## -----------------------------------------------------------------------------------------------------\n",
        "# ## Now we need to load the weights of the model, and continue training with a different cost matrix\n",
        "# ## ------------------------------------------------------------------------------------------------------\n",
        "# # load the model weights\n",
        "# # model.load_weights('initial_5epochs.h5') \n",
        "\n",
        "# df = pd.DataFrame()\n",
        "# cost_matrix = np.ones((10,10))\n",
        "\n",
        "# model_history_all = []\n",
        "# cm_all            = []\n",
        "\n",
        "# cost_list = [10, 100, 1000] # Each one takes about 2 minutes 5*4*2=40 minutes for 5 costs/4 reps -- actually all took 40 reps took almost  1.5 hours\n",
        "# reps = 1\n",
        "\n",
        "# for k in cost_list:\n",
        "#   for r in range(reps):\n",
        "#     print(\"starting rep \", i, \" for \", k , \"-cost.\")\n",
        "\n",
        "#     cost_matrix[9,4] = k\n",
        "#     model = create_model(r)\n",
        "\n",
        "#     model.load_weights('initial_0epochs.h5')\n",
        "\n",
        "#     #I may need to re-initiate the optimizer to have a smaller learning rate\n",
        "#     model.compile(loss=WeightedCategoricalCrossentropy(cost_matrix), optimizer=sgd,  metrics='categorical_accuracy',)\n",
        "    \n",
        "#     nb_epoch = 110\n",
        "#     # patience = 20\n",
        "\n",
        "#     # es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, verbose=1, restore_best_weights = True)\n",
        "\n",
        "#     X_train_shuffled = shuffle(X_train, random_state=42 + r )\n",
        "#     Y_train_shuffled = shuffle(Y_train, random_state=42 + r)\n",
        " \n",
        "\n",
        "#     history = model.fit(X_train_shuffled, Y_train_shuffled,          batch_size=batch_size, epochs=nb_epoch, verbose=0,\n",
        "#             validation_data=(X_val, Y_val), shuffle=True, use_multiprocessing=True, callbacks = [ cm_callback, f1_callback, epoch_callback])\n",
        "\n",
        "#     cm3 = return_cm(model)\n",
        "\n",
        "#     del(history.model)\n",
        "#     model_history_all.append(history)\n",
        "#     cm_all.append(cm3)\n",
        "#     ## Now I need to plot all of the \"model_history_all\"\n",
        " \n",
        "  \n",
        "#     df = pd.concat([df, pd.DataFrame({\"cost\": k, \"rep\": r ,\n",
        "#                                       \"model_history\": [history],\n",
        "#                                       \"cm\": [cm3]\n",
        "#                                       })] , \n",
        "#                    ignore_index=True )\n",
        "\n",
        "#     import pickle\n",
        "\n",
        "#     # save the variable to a pickle file\n",
        "#     with open('diffcosts_models_lr0point1.pkl', 'wb') as f:\n",
        "#         pickle.dump(model_history_all, f)\n",
        "\n",
        "#     files.download('diffcosts_models_lr0point1.pkl')\n",
        "\n",
        "#     with open('diffcosts_cms_lr0point1.pkl', 'wb') as f:\n",
        "#         pickle.dump(cm_all, f)\n",
        "\n",
        "#     files.download('diffcosts_cms_lr0point1.pkl')\n",
        "#     !date\n",
        "#     df_string = \"df_lr0point1_K\" + str(k) + \"_R\" + str( r ) + \".pkl\" \n",
        "\n",
        "#     #Trying to Work Smarter and Save as Dataframe with Rep Cost History and CM\n",
        "#     with open(df_string, 'wb') as f:\n",
        "#         pickle.dump(df, f)\n",
        "\n",
        "#     files.download(df_string)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#   # print(model_history_all) #18 items into _ is taking 1 hours and 3 minutes!  perhaps im not usng the right settings - i have no-accelerator on google colab\n",
        "# print(df_string, \" is final dataframe file\")"
      ],
      "metadata": {
        "id": "ZFgEx-T-fg28",
        "collapsed": true
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What actually downloaded \n",
        "\n",
        "df_lr0point1_K1000_R29.pkl  is final dataframe file but didn't save to Downloads.  I need to add code to have it download to my Google Drive so this doesn't happen again.  These appear to be the  last files downloaded (its hard to tell because they all download at the same time, instead of the time they are requested to be downloaded.  For example i have 30 diff files that downloaded exactly at 0330:\n",
        "\n",
        "* df_lr0point1_K1000_R9.pkl (20 reps missing? or did this turn out to be 19 reps?)\n",
        "* diffcosts_models_lr0point1 (32).pkl  (because i don't add an extension like the data frame the pickle files just keep downloading with new suffixes.\n",
        "* diffcosts_cms_lr0point1 (23).pkl   \n",
        "\n"
      ],
      "metadata": {
        "id": "es9m50uaxvP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # # save the variable to a pickle file\n",
        "    # with open('baseline_models_lr0point1.pkl', 'wb') as f:\n",
        "    #     pickle.dump(model_history_all, f)\n",
        "\n",
        "    # files.download('baseline_models_lr0point1.pkl')\n",
        "\n",
        "    # with open('baseline_cms_lr0point1.pkl', 'wb') as f:\n",
        "    #     pickle.dump(cm_all, f)\n",
        "\n",
        "    # files.download('baseline_cms_lr0point1.pkl')"
      ],
      "metadata": {
        "id": "APH12P-AQ5bP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download the multiple runs and reps of Model History from training with different lambda values\n",
        "\n",
        "\n",
        "Also commented out because once everything is ran and saved, I don't want to accidentally save additional files"
      ],
      "metadata": {
        "id": "7z0g0vdr2w05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load Model History Variables from Pickle Files - must have weighted-categorical-accuracy defined"
      ],
      "metadata": {
        "id": "WPmV3ALQcsjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #Upload and save the history to variable \n",
        "# import pickle\n",
        "\n",
        "# # # #load the pickle file\n",
        "# # with open('baseline_models_lr0point1.pkl', 'rb') as handle:\n",
        "# #     model_history_all = pickle.load(handle)\n",
        "\n",
        "# # # import keras\n",
        "# # # keras.models.load_model('initial_10_secondphase_lambda_history.pkl')\n",
        "# # #use the loaded variable\n",
        "# # print(model_history_all)\n",
        "\n",
        "# # # #load the pickle file\n",
        "# # with open('baseline_cms_lr0point1.pkl', 'rb') as handle:\n",
        "# #     cm_all = pickle.load(handle)\n",
        "\n",
        "# # # use the loaded variable\n",
        "# # print(cm_all)\n",
        "\n",
        "# # df_string = 'df_lr0point1_K1000_R9.pkl'\n",
        "\n",
        "# # #load the pickle file\n",
        "# with open(df_string, 'rb') as handle:\n",
        "#     df = pickle.load(handle)\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "#   # save the variable to a pickle file\n",
        "#   with open('diffcosts_models_lr0point1 (31).pkl', 'rb') as handle:\n",
        "#     model_history_all  = pickle.load(handle)\n",
        "\n",
        " \n",
        "#   with open('diffcosts_cms_lr0point1.pkl',  'rb') as handle:\n",
        "#    cm_all  = pickle.load(handle)\n",
        "\n",
        " \n",
        "\n",
        "# # use the loaded variable\n",
        "# print(df)\n",
        " "
      ],
      "metadata": {
        "id": "dXFoZGvVZ1oE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # #I want to break out the model_history and cm_history into a dataframe to be referenced by Cost and Rep\n",
        "# # import pandas as pd\n",
        "# # df = pd.DataFrame()\n",
        "# # count=1\n",
        "\n",
        "\n",
        "# # #It'd be better if i could refer to the \"cost\"/lambda2 as a variable in the model_history, \n",
        "# # # but i recreated it here\n",
        "# # # \n",
        "# # # cost_list = [10, 100, 1000, 1] # Each one takes about 2 minutes 5*4*2=40 minutes for 5 costs/4 reps -- actually all took 40 reps took almost  1.5 hours\n",
        "\n",
        "# # cost_list = [ 1]\n",
        "\n",
        "# # for k in cost_list:\n",
        "\n",
        "  \n",
        "# # #It'd be better if i could refer to the reps a variable in the model_history, \n",
        "# # # but i recreated it here as the range(10) that was originally used\n",
        "\n",
        "# #   for i in range(30):\n",
        "# #     # print(\"k: \", k, \"i: \", i)\n",
        "# #     df = pd.concat([df, pd.DataFrame({\"cost\": [k], \"rep\": [i],\n",
        "# #                                       \"model_history\": [model_history_all[count-1]],\n",
        "# #                                       # \"cm\": [cm_all[count-1]]\n",
        "# #                                       })] , \n",
        "# #                    ignore_index=True )\n",
        "# #     count+=1    \n",
        "\n",
        "\n",
        "# # I create a dataframe with CSV but  don't save/download it \n",
        "# df.to_csv('dataframe.csv',index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "JgxsbrcEPiEa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# import seaborn as sns\n",
        "\n",
        "# import time\n",
        "# nb_epoch = 110\n",
        "# patience = 0\n",
        "# df.groupby(\"cost\")\n",
        "\n",
        "# for x, group in df.groupby(\"cost\"):\n",
        "#   # fig = plt.figure(figsize=(20, 10)) \n",
        "#   # fig, ax = plt.subplots(1,2)\n",
        "#   print(\"Group of Lambda-Value:\",   x)\n",
        "\n",
        "#   # group\n",
        "#   for item in group.model_history:\n",
        "#     plot_model_history_all(item)    \n",
        "#   plt.show()\n",
        "  "
      ],
      "metadata": {
        "id": "S8nf629kPiEb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df[\"model_history\"][0].history.keys()\n",
        "# # "
      ],
      "metadata": {
        "id": "ocYFlDuP9qbL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plots  look very similar.  The loss values, even with shuffling, are nearly identical at least within 0.01 for the first three epochs of runs 1 and 2 (haven't checked all, but the plot makes it look like the runs are nearly identical)"
      ],
      "metadata": {
        "id": "cnKubJqKCXOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df[\"model_history\"][2].history[\"val_loss\"][0] - df[\"model_history\"][1].history[\"val_loss\"][0]\n",
        "# df[\"model_history\"][2].history[\"val_loss\"][1] - df[\"model_history\"][1].history[\"val_loss\"][1]\n",
        "# df[\"model_history\"][2].history[\"val_loss\"][2] - df[\"model_history\"][1].history[\"val_loss\"][2]"
      ],
      "metadata": {
        "id": "xRS1JJpl_yOq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n"
      ],
      "metadata": {
        "id": "5orz3_QVDJws"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the highest misclassification?\n",
        "\n"
      ],
      "metadata": {
        "id": "HFEuhKt4DST1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(len(df[\"model_history\"])):\n",
        "#   print(i)\n",
        "#   cm = df[\"model_history\"][i].history[\"cm_per_epoch\"][-1].reshape((10,10))\n",
        "#   # cm = cm.reshape((10,10))\n",
        "  \n",
        "\n",
        "#   # Create figure and axes\n",
        "#   fig, ax = plt.subplots(figsize=(7, 7))\n",
        "\n",
        "\n",
        "#   cm3_wodiag = pd.DataFrame(cm*(np.ones((10,10)) - np.eye(10)))\n",
        "#   plt.xlabel('Predicted Class')\n",
        "#   plt.ylabel('True Class')\n",
        "#   plt.title('# of misclassifications of 9 as 4 is '+str(cm[9][4]) + ' reverse misclass is ' + str(cm[4][9])  ) \n",
        "#   sns.heatmap(cm3_wodiag, annot=True, annot_kws={\"size\": 7},  fmt='g', cmap=sns.cm.rocket_r, cbar=False) # font size\n",
        "#   plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  "
      ],
      "metadata": {
        "id": "KnjvWQjxDYAC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average Misclassification Confusion Plot"
      ],
      "metadata": {
        "id": "fmbcFQcfhdyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cm_sum = 0\n",
        "\n",
        "# for i in range(len(df[\"model_history\"])):\n",
        "#   # print(i)\n",
        "#   cm_sum += df[\"model_history\"][i].history[\"cm_per_epoch\"][-1].reshape((10,10))\n",
        "#   # print(cm_sum)\n",
        "\n",
        "# cm_average = cm_sum/(len(df[\"model_history\"]))\n",
        "# # print(cm_average)\n",
        "\n",
        "# # Create figure and axes\n",
        "# fig, ax = plt.subplots(figsize=(7, 7))\n",
        "\n",
        "\n",
        "# cm3_wodiag = pd.DataFrame(cm_average*(np.ones((10,10)) - np.eye(10)))\n",
        "# plt.xlabel('Predicted Class')\n",
        "# plt.ylabel('True Class')\n",
        "# plt.title('# of Average misclassifications of 9 as 4 is '+str(cm3_wodiag[4][9]) + ' reverse misclass is ' + str(cm3_wodiag[9][4])  ) \n",
        "# sns.heatmap(cm3_wodiag, annot=True, annot_kws={\"size\": 7},  fmt='.1f', cmap=sns.cm.rocket_r, cbar=False) # font size\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# plt.xlabel('Predicted Class')\n",
        "# plt.ylabel('True Class')\n",
        "# plt.title('# of Average misclassifications of 9 as 4 is '+str(cm_average[9][4]) + ' reverse misclass is ' + str(cm_average[4][9]) ) \n",
        "# sns.heatmap(cm_average, annot=True, annot_kws={\"size\": 7},  fmt='.1f', cmap=sns.cm.rocket_r, cbar=False) # font size\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# #The notation for digging into a dataframe versus a numpy array is reversed / its horribly confusing/ but it is  listed above correctly\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fX4B3dPAhdhn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(len(df[\"model_history\"])):\n",
        "#   plot_model_history(df[\"model_history\"][i], 110, df[\"model_history\"][i].history[\"cm_per_epoch\"][-1].reshape((10,10)))\n",
        "\n",
        "#   plt.gcf().set_size_inches(10, 5)  # this works \n",
        "#   # plt.gcf().suptitle(f\"Lambda Value {lambda_val} for {nb_epoch} Epochs and Patience {patience} \" )\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xkH6R_LyMOWN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df[\"model_history\"][0].history[\"cm_per_epoch\"][-1].reshape((10,10))\n",
        "\n",
        "# # \n",
        "# #The 30th rep has 10 misclassified 9s as 4s \n",
        "# # df[\"model_history\"][29].history[\"cm_per_epoch\"][-1].reshape((10,10))[9][4]\n"
      ],
      "metadata": {
        "id": "PPlXeHqyVvNT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistics"
      ],
      "metadata": {
        "id": "75g_dXG5go_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Needs to be re-done for multiple costs!\n",
        "\n",
        "# for x, group in df.groupby(\"cost\"):\n",
        "  \n",
        "#   print(\" -------------- \")\n",
        "\n",
        "#   print(\"Group of Lambda-Value:\",   x)\n",
        "\n",
        "#   print(\"The minimum Validation Loss occurred at the following epochs for the reps\")\n",
        "\n",
        "\n",
        "#   for index in group.index:  \n",
        "#     print(\"index\", index)\n",
        "#     sum_9T_4P = 0\n",
        "#     sum_4T_9P = 0\n",
        "#     for rep in range(1,len(group)+1):\n",
        "#       print(\" -------------- \")\n",
        "\n",
        "#       # for epoch in range(len(group[\"model_history\"][index].history[\"val_loss\"])):\n",
        "#       #   print(\"Epoch:\", epoch)\n",
        "#       # print(\"The minimum Validation Loss occurred at the following epochs for the reps\")\n",
        "\n",
        "#       # for epoch_num in range(len(group[\"model_history\"][index].history[\"val_loss\"])):  #This tells you how many epochs there are\n",
        "#       print(\"Cost: \", k,  \"Rep:\", rep,  end=\"\\t\")\n",
        "#       print(\"Epoch: \", np.argmin(group[\"model_history\"][index].history[\"val_loss\"]), end=\"\\t\")\n",
        "#       print(\"CM_9T_4P at that epoch: \",  group[\"model_history\"][index].history[\"cm_per_epoch\"][np.argmin(group[\"model_history\"][index].history[\"val_loss\"])].reshape((10,10))[9][4]     , end=\"\\t\")\n",
        "#       print(\"CM_4T_9P at that epoch: \",  group[\"model_history\"][index].history[\"cm_per_epoch\"][np.argmin(group[\"model_history\"][index].history[\"val_loss\"])].reshape((10,10))[4][9]     )\n",
        "\n",
        "\n",
        "\n",
        "#       sum_9T_4P += group[\"model_history\"][index].history[\"cm_per_epoch\"][np.argmin(group[\"model_history\"][index].history[\"val_loss\"])].reshape((10,10))[9][4]  \n",
        "#       sum_4T_9P += group[\"model_history\"][index].history[\"cm_per_epoch\"][np.argmin(group[\"model_history\"][index].history[\"val_loss\"])].reshape((10,10))[4][9]  \n",
        "\n",
        "\n",
        "#     print(\"The average misclassifications of 9 as 4 if Early Stopping Took place: \", sum_9T_4P/len(group[\"model_history\"]),   \"\\t \", ((sum_9T_4P/len(group[\"model_history\"]))/np.sum(cm_all, axis=2)[0][9])*100 , \"percent\"   )\n",
        "#     print(\"The average reverse misclassifications of 4 as 9 if Early Stopping Took place: \", sum_4T_9P/len(group[\"model_history\"]),   \"\\t \", ((sum_4T_9P/len(group[\"model_history\"]))/np.sum(cm_all, axis=2)[0][4])*100 , \"percent\"   )\n",
        "\n",
        "\n",
        "#       # sum_94 = 0\n",
        "#       # sum_49 = 0\n",
        "\n",
        "#       # # for epoch_num in range(len(group[\"model_history\"][rep].history[\"val_loss\"])):  #This tells you how many epochs there are\n",
        "#       # #     # print(cm_all[j][9][4])\n",
        "#       # sum_49 += group[\"model_history\"][epoch].history[\"cm_per_epoch\"][-1].reshape((10,10))[4][9]\n",
        "#       # sum_94 += group[\"model_history\"][rep].history[\"cm_per_epoch\"][-1].reshape((10,10))[9][4]\n",
        "\n",
        "#       #     # print(sum_cm)\n",
        "\n",
        "#     print(\" -------------- \")\n",
        "#       # print(\"The average misclassifications if taken out to the last training epoch: \", sum_94/len(cm_all), \"\\t \", (sum_94/np.sum(cm_all, axis=2)[0][9])*100 , \"percent\"   )\n",
        "#       # print(\"The average reverse misclassifications if taken out to the last training epoch: \", sum_49/len(cm_all), \"\\t \", (sum_49/len(cm_all)/np.sum(cm_all, axis=2)[0][4])*100 , \"percent\"   )\n",
        "\n",
        "#       # # Val # of 9s 769.0\n",
        "#       # #Get the original total number of 9s in the Val Set\n",
        "#       # np.sum(cm_all, axis=2)[0][9]\n",
        "\n",
        "#       # # Val # of 4s 749.0\n",
        "#       # np.sum(cm_all, axis=2)[0][4]\n"
      ],
      "metadata": {
        "id": "MUOJz-RNWB3P"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taking the Average Confusion Matrix using the last confusion matrix per epoch, (per cost), divided by reps"
      ],
      "metadata": {
        "id": "YWFb0D2a-2Dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for k, group in df.groupby(\"cost\"):  \n",
        "#   print(\" -------------- \")\n",
        "#   print(\"Group of Lambda-Value:\",   k)\n",
        "\n",
        "#   # print(group.index)\n",
        "#   # print(group) \n",
        "  \n",
        "#   total_cm_per_rep = np.zeros((10,10))\n",
        "#   for index in group.index:\n",
        "#     print(\"Index:\",   index) \n",
        "\n",
        "\n",
        "#     # for epoch in range(len(group[\"model_history\"][index].history[\"val_loss\"])):\n",
        "#     #   # print(\"index: \", index, \"cost: \", k, \"rep: \", rep)\n",
        "#     total_cm_per_rep += (group[\"model_history\"][index].history[\"cm_per_epoch\"][-1].reshape((10,10)))\n",
        "#     # print(total_cm_per_rep)\n",
        "#   print(pd.DataFrame(total_cm_per_rep/len(group.index)))"
      ],
      "metadata": {
        "id": "eynAYI4I0sMK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# for k, group in df.groupby(\"cost\"):\n",
        "  \n",
        "#   print(\" -------------- \")\n",
        "\n",
        "#   print(\"Group of Lambda-Value:\",   k)\n",
        "\n",
        "#   # print(group.index)\n",
        "#   # print(group)\n",
        "#   for index in group.index:\n",
        "#     print(index)\n",
        "#     for rep in range(1,len(group)+1):\n",
        "#       print(\"Rep: \", rep)\n",
        "       \n",
        "\n",
        "\n",
        "#       for epoch in range(len(group[\"model_history\"][index].history[\"cm_per_epoch\"])):\n",
        "#         # print(\"index: \", index, \"cost: \", k, \"rep: \", rep, \"epochs: \", epoch) #\"last cm:\", group[\"model_history\"][index].history[\"cm_per_epoch\"][rep].reshape((10,10)))\n",
        "#         # print(\"epoch: \", epoch)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UxyaZCMSqDvE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scratch"
      ],
      "metadata": {
        "id": "-4sFk2MGQmc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # list1 = [0,0,0,0,1]\n",
        "# list1 = [0,1,0,0,0]\n",
        "\n",
        "\n",
        "# count = 1\n",
        "# prev_lambda_value = list1[0]\n",
        "# print(\"Initialize model with lambda: \", prev_lambda_value)\n",
        "\n",
        "\n",
        "# for lambda_value in list1:\n",
        "\n",
        "#     print(\"Bin: \", count, \" \", end=\"\")\n",
        "\n",
        "#     if lambda_value != prev_lambda_value:\n",
        "#         print(f\"lambda_value {lambda_value} is different than previous lambda_value {prev_lambda_value} so save current weights and then compile with weight {lambda_value}\")\n",
        "\n",
        "#     else:\n",
        "#       print(f\"lambda_value {lambda_value} is equal to previous lambda_value {prev_lambda_value} so continue training {lambda_value}\")\n",
        "      \n",
        "\n",
        "#     count+=1\n",
        "#     prev_lambda_value = lambda_value"
      ],
      "metadata": {
        "id": "xE5h97BhQlOI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If I want to continue training with the current cost_matrix this seems to work\n",
        "\n",
        "```\n",
        "\n",
        "model = create_model(0)\n",
        "# save the model weights\n",
        "model.save_weights('initial_0epochs.h5')\n",
        "model.compile(loss=WeightedCategoricalCrossentropy(cost_matrix), optimizer=sgd,  metrics='categorical_accuracy',)\n",
        "\n",
        "model_history = model.fit(X_train, Y_train,  batch_size=batch_size, epochs=2, verbose=2,\n",
        "         validation_data=(X_val, Y_val), shuffle=True, use_multiprocessing=True, callbacks = [ cm_callback, f1_callback])\n",
        "\n",
        "model.save_weights('initial_2epochs.h5')\n",
        "\n",
        " \n",
        "#Continue Training with no change\n",
        "model_history2 = model.fit(X_train, Y_train,  batch_size=batch_size, epochs=2, verbose=2,\n",
        "         validation_data=(X_val, Y_val), shuffle=True, use_multiprocessing=True, callbacks = [ cm_callback, f1_callback])\n",
        "\n",
        "model.save_weights('initial2_lambda_next2_lambda.h5')```\n",
        "\n"
      ],
      "metadata": {
        "id": "k3HuHB9JSg3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To switch the cost-matrix and then continue training thsi seems to work\n",
        "\n",
        "```\n",
        "model = create_model(0)\n",
        "# save the model weights\n",
        "model.save_weights('initial_0epochs.h5')\n",
        "model.compile(loss=WeightedCategoricalCrossentropy(cost_matrix), optimizer=sgd,  metrics='categorical_accuracy',)\n",
        "\n",
        "model_history = model.fit(X_train, Y_train,  batch_size=batch_size, epochs=2, verbose=2,\n",
        "         validation_data=(X_val, Y_val), shuffle=True, use_multiprocessing=True, callbacks = [ cm_callback, f1_callback])\n",
        "\n",
        "model.save_weights('initial_2epochs.h5')\n",
        "\n",
        "model = create_model(0)\n",
        "cost_matrix[Truth, Predicted] = 100\n",
        "model.compile(loss=WeightedCategoricalCrossentropy(cost_matrix), optimizer=sgd,  metrics='categorical_accuracy',)\n",
        "model.load_weights('initial_2epochs.h5')\n",
        " \n",
        "#Continue Training with no change\n",
        "model_history2 = model.fit(X_train, Y_train,  batch_size=batch_size, epochs=2, verbose=2,\n",
        "         validation_data=(X_val, Y_val), shuffle=True, use_multiprocessing=True, callbacks = [ cm_callback, f1_callback])\n",
        "\n",
        "model.save_weights('initial2_1_next2_100.h5')\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "BkNBqaTFTMWw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K9pov0y1SlgQ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scratch work"
      ],
      "metadata": {
        "id": "RfzPcxGZZVlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "mydrive_path = \"/content/drive/MyDrive/Con3/Exp_5bins_11Runs_40D_SGD_Lr0point1_diffSeedsPerRep/\"\n",
        "\n",
        "\n",
        "from google.colab import files\n"
      ],
      "metadata": {
        "id": "eI0JDKbpoEWa",
        "outputId": "4691ee17-6299-4d6b-bcd8-9364fe6a4fd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_tf_model_weights(model, file_string):\n",
        "\n",
        "    final_weights = file_string + \".h5\"\n",
        "    model.save_weights(final_weights)\n",
        "\n",
        "    weights = model.get_weights()\n",
        "\n",
        "    # df = pd.DataFrame(weights)\n",
        "\n",
        "    # csv_file_name = file_string + \".csv\"\n",
        "    pkl_file_name = file_string + \".pkl\"\n",
        "\n",
        "\n",
        "    # df.to_csv(csv_file_name, index=False)\n",
        "\n",
        "\n",
        "    # with open(pkl_file_name, 'wb') as f:\n",
        "        # pickle.dump(weights, f)\n",
        "    # print('Model weights saved to CSV and Pickle files.')\n",
        "\n",
        "    with open(str(mydrive_path) + str(pkl_file_name), 'wb') as f:\n",
        "        pickle.dump(weights, f)\n",
        "    print('Model Pickle file saved to: ', mydrive_path)\n",
        "\n",
        "\n",
        "    model.save_weights(str(mydrive_path) + str(final_weights))\n",
        "    print('Model weights saved to: ', mydrive_path)\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    \n"
      ],
      "metadata": {
        "id": "GKZ7NHnR1ujM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SET THE IITIAL LAMBDA VALUE! \n",
        "cost_matrix = np.ones((10,10))\n",
        "lambda_val = 1\n",
        "\n",
        "Truth=9\n",
        "Predicted=4\n",
        "cost_matrix[Truth, Predicted] = lambda_val"
      ],
      "metadata": {
        "id": "VLdJgBNiqxF8"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Import\n",
        "# import pandas as pd \n",
        "# import pickle\n",
        "\n",
        "\n",
        "# #Define 11 Test Runs:\n",
        "# list1=[1,1,1,1,1]\n",
        "# list2=[1,1,1,1,10]\n",
        "# list3=[1,1,1,1,100]\n",
        "# list4=[1,1,1,10,1]\n",
        "# list5=[1,1,1,100,1]\n",
        "# list6=[1,1,10,1,1]\n",
        "# list7=[1,1,100,1,1]\n",
        "# list8=[1,10,1,1,1]\n",
        "# list9=[1,100,1,1,1]\n",
        "# list10=[10,1,1,1,1]\n",
        "# list11=[100,1,1,1,1]\n",
        "\n",
        "\n",
        "# #List all 11 Test Runs in Array:\n",
        "# # list_of_test_runs = [list2]\n",
        "\n",
        "# list_of_test_runs = [list1,list2,list3,list4,list5,list6,list7,list8,list9,list10,list11]          #<<<<< CHANGE BACK\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# #Create dataframe to store all the information\n",
        "# data = {'rep': [], 'bins': [], 'lambdas': [], 'model_history': [], 'final_saved_weights_string': []}\n",
        "# scratch_df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "\n",
        "# #Start For-Loop by looking at the individual lists of lambda-values inside the test run lists:\n",
        "# for test_run in list_of_test_runs:\n",
        "\n",
        "#   print(f\"Starting with test-run of Lambdas {test_run} :\")\n",
        "\n",
        "  \n",
        "#   for rep in [0,1,2]:                                                                             #<<<<< CHANGE BACK\n",
        "#   # for rep in [0]:\n",
        "#     model_history = {}\n",
        "\n",
        "#     model_weights_string = \"rep\" + str(rep) + \"_\"\n",
        "#     print(\" ------------------------------------- \")\n",
        "#     count = 1                #This is the BIN COUNT\n",
        "#     print(f\"Rep {rep} : \", end=\"\")\n",
        "    \n",
        "    \n",
        "#     prev_lambda_value = test_run[0]\n",
        "\n",
        "#     # print(\"Initialize model with lambda: \", prev_lambda_value)\n",
        "#     # print(f\"     Call function INITIALIZE MODEL with parameters REP {rep} and test_run[0] LambdaValue {test_run[0]}  \")\n",
        "\n",
        "#     #\"Initialize Model Function\" for variable MODEL\n",
        "#     model = create_model(i=rep)\n",
        "\n",
        "    \n",
        "#     #Compile the initial model with the first-lambda-value of the current-test-run\n",
        "#     cost_matrix[Truth, Predicted] = prev_lambda_value\n",
        "#     model.compile(loss=WeightedCategoricalCrossentropy(cost_matrix), optimizer=sgd,  metrics='categorical_accuracy',)\n",
        "\n",
        "#     #I dont call \"training\" yet, I've compiled with the initial weight, and then I allow my for-loop to start/continue the training everytime the lambdavalue is the same \n",
        "\n",
        "\n",
        "#     #There are currently \"5-lambda_values\" in each of the test-run-arrays, which means there are 5-equally-sized-bins.  The lambda_value is the lambda-values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#     for lambda_value in test_run:\n",
        "#         N_EPOCHS = 110//len(test_run)                                                             #<<<<< CHANGE BACK\n",
        "#         # N_EPOCHS = 1\n",
        "#         print(\"Bin: \", count, \" \", end=\"\")\n",
        "        \n",
        "#         #If current lambda_value is different then the previous lambda value, save weights, re-compile with different lambda, and training for 110/5 epochs:\n",
        "#         #\n",
        "#         if lambda_value != prev_lambda_value:\n",
        "#             # print(f\"lambda_value {lambda_value} is different than previous lambda_value {prev_lambda_value} so save current weights as model_weights: {model_weights_string} and then compile with weight {lambda_value}\")\n",
        "#             # print(f\"   Call function CHANGE MODEL with parameters REP {rep} and NEXT LambdaValue in List {lambda_value} to train for epochs __{N_EPOCHS}___ \")\n",
        "#             weights = model.get_weights()\n",
        "#             temp_var = model_weights_string + \".h5\"\n",
        "#             model.save_weights(temp_var)\n",
        "\n",
        "#             #\"Create new model\" with same name MODEL\n",
        "#             model = create_model(i=rep)\n",
        "#             model.set_weights(weights)\n",
        "#             #Compile the \"new\" model with the next-lambda-value of the current-test-run\n",
        "#             cost_matrix[Truth, Predicted] = lambda_val\n",
        "#             model.compile(loss=WeightedCategoricalCrossentropy(cost_matrix), optimizer=sgd,  metrics='categorical_accuracy',)\n",
        "\n",
        "#             #Continue training for 110/5 epochs\n",
        "#             model_history2 = model.fit(X_train, Y_train,  batch_size=batch_size, epochs=N_EPOCHS, verbose=0, validation_data=(X_val, Y_val), shuffle=True, use_multiprocessing=True, callbacks = [ cm_callback, f1_callback])\n",
        "#             # print(\"Model History 2 Loss so far\", model_history2.history['loss'],\"\\n\\n\")\n",
        "          \n",
        "#             try:\n",
        "#               # print(\"trying\")\n",
        "#               model_history = {key: model_history[key] + model_history2.history[key] for key in model_history2.history.keys()}\n",
        "#             except KeyError:\n",
        "#               # print(\"Key Error\")  \n",
        "#               model_history = {key: model_history2.history[key] for key in model_history2.history.keys()}\n",
        "#             #finally:\n",
        "#               # print(\"Finaly\")  \n",
        "\n",
        "#             # print(\"Model History (New) Loss so far\", model_history['loss'],\"\\n\\n\")\n",
        "\n",
        "\n",
        "#         #If current lambda_value is the same as previous lambda value, continue training for 110/5 epochs:\n",
        "#         #\n",
        "#         else:\n",
        "#           # print(f\"lambda_value {lambda_value} is equal to previous lambda_value {prev_lambda_value} so continue training {lambda_value} for epochs __{110/5}\")\n",
        "#           # print(f\"     Call function CONTINUE TRAIING \")\n",
        "#           model_history1 = model.fit(X_train, Y_train,  batch_size=batch_size, epochs=N_EPOCHS, verbose=0, validation_data=(X_val, Y_val), shuffle=True, use_multiprocessing=True, callbacks = [ cm_callback, f1_callback])\n",
        "#           # print(\"Model History 1 Loss so far\", model_history1.history['loss'],\"\\n\\n\")\n",
        "\n",
        "#           try:\n",
        "#             # print(\"Trying\")\n",
        "#             model_history = {key: model_history[key] + model_history1.history[key] for key in model_history1.history.keys()}\n",
        "#           except KeyError:\n",
        "#             # print(\"Key Error\")\n",
        "#             model_history = {key: model_history1.history[key] for key in model_history1.history.keys()}\n",
        "#           #finally:\n",
        "#             # print(\"Finally\")\n",
        "#             # model_history = {key: model_history1.history[key] for key in model_history1.history.keys()}\n",
        "\n",
        "\n",
        "#           # print( model_history.history[key] for key in model_history.history.keys())\n",
        "#           # print(\"Model History (New) Loss so far\", model_history['loss'],\"\\n\\n\")\n",
        "\n",
        "#         count+=1\n",
        "#         prev_lambda_value = lambda_value         \n",
        "#         model_weights_string = model_weights_string + str(lambda_value) + \"_\"\n",
        "\n",
        "    \n",
        "#     print(f\"Save final weights as {model_weights_string}\")\n",
        "#     save_tf_model_weights(model, model_weights_string)\n",
        " \n",
        "#     now = datetime.datetime.now()\n",
        "\n",
        "#     #Saving to DataFrame\n",
        "#     scratch_df = pd.concat([scratch_df, pd.DataFrame({'rep': rep, 'bins': 5, 'lambdas': [test_run], 'model_history': [model_history], 'final_saved_weights_string': model_weights_string ,  \n",
        "#       'loss': [model_history['loss']],\n",
        "#       'categorical_accuracy': [model_history['categorical_accuracy']],\n",
        "#       'val_loss': [model_history['val_loss']],\n",
        "#       'val_categorical_accuracy': [model_history['val_categorical_accuracy']],\n",
        "#       '9T_4P': [model_history['9T_4P']],\n",
        "#       '4T_9P': [model_history['4T_9P']],\n",
        "#       '0T_Acc': [model_history['0T_Acc']],\n",
        "#       '1T_Acc': [model_history['1T_Acc']],\n",
        "#       '2T_Acc': [model_history['2T_Acc']],\n",
        "#       '3T_Acc': [model_history['3T_Acc']],\n",
        "#       '4T_Acc': [model_history['4T_Acc']],\n",
        "#       '5T_Acc': [model_history['5T_Acc']],\n",
        "#       '6T_Acc': [model_history['6T_Acc']],\n",
        "#       '7T_Acc': [model_history['7T_Acc']],\n",
        "#       '8T_Acc': [model_history['8T_Acc']],\n",
        "#       '9T_Acc': [model_history['9T_Acc']],\n",
        "#       'cm_per_epoch': [model_history['cm_per_epoch']],\n",
        "#       'f1_micro': [model_history['f1_micro']],\n",
        "#       'f1_macro': [model_history['f1_macro']],\n",
        "#       'f1_weighted': [model_history['f1_weighted']],\n",
        "#       'f1_notweighted': [model_history['f1_notweighted']],\n",
        "#                    \"datetime\": now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "#                                     })], ignore_index=True)\n",
        "\n",
        "\n",
        "#     print(\"Deleting previous Model_history\")\n",
        "#     del(model_history)\n",
        "#   print(\" ------------------------------------- \")\n",
        "\n",
        "#   # print(scratch_df)"
      ],
      "metadata": {
        "id": "Pwke5a9uZW6q"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If I need to load weights later and evaluate this appears to work\n",
        "# tempfilename= str('/content/drive/MyDrive/Con3/Exp_5bins_11Runs_40D_SGD_Lr0point1_diffSeedsPerRep/')+str(model_weights_string)+str(\".h5\")\n",
        "# model.load_weights(tempfilename)\n",
        "# model.evaluate(X_test, Y_test)"
      ],
      "metadata": {
        "id": "a2eRG44ktjNp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # model_history['loss']\n",
        "# scratch_df\n",
        "\n",
        "# # scratch_df[(scratch_df[\"rep\"] == 0) & (scratch_df[\"bins\"] == 5)]"
      ],
      "metadata": {
        "id": "Y3klb0l28Fig"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for k in model_history.history.keys():\n",
        "#   print(\"'\", end=\"\")\n",
        "#   # print(\"model_history_\", end=\"\")\n",
        "#   print(k, end=\"\")\n",
        "#   print(\"'\", end=\"\")\n",
        "#   print(\": \", end=\"\")\n",
        "#   print( \"[model_history.history\", end=\"\")\n",
        "#   print(\"['\", end=\"\")\n",
        "#   # print(\"model_history_\", end=\"\")\n",
        "#   print(k, end=\"\")\n",
        "#   print(\"']],\", end=\"\")\n",
        "#   print()\n",
        "\n",
        "# model_history['loss']"
      ],
      "metadata": {
        "id": "FIObuLz100qL"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scratch_df.iloc[0][\"model_history\"]['loss']"
      ],
      "metadata": {
        "id": "SPGHR9YxcFbY"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scratch_df.to_pickle('df.pkl')\n",
        "# files.download(\"df.pkl\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8AIVC-IsmJ-Z"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scratch_df.to_csv('df.csv', index=False)\n",
        "# files.download(\"df.csv\")"
      ],
      "metadata": {
        "id": "h3FYbrq9sBqh"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title: plot_model_history_all\n",
        "def plot_model_history_all(model_history, nb_epoch=None, cm3=None, label=None): \n",
        "  # Parameters\n",
        "  # ----------\n",
        "  # tot_epochs : int\n",
        "  #     Total number of epochs for which the model was trained.\n",
        "  # model_history : keras.callbacks.History\n",
        "  #     The history object returned by the fit() method of the model.\n",
        "  # cm3 : 10x10 dataframe \n",
        "  #      10x10 dataframe of confusion matrix from predicted X_val categories\n",
        "  # restored_weights : int\n",
        "  #     The epoch at which the weights were restored.\n",
        "\n",
        "\n",
        "  tot_epochs = len(model_history['loss'])  #if the total epochs ran is 28, it'll show up as 27 in the epoch object so we must add 1\n",
        "  # print(\"Total Epochs: \", tot_epochs)\n",
        "\n",
        "  #No Early Stop so not using this\n",
        "  #if tot_epochs is the total number of epochs ran then early stop did not happen, and we need not minus patience\n",
        "  # if tot_epochs == nb_epoch:\n",
        "  #   restored_weights = tot_epochs\n",
        "  # else:\n",
        "  #   restored_weights  = tot_epochs-patience   #when using restore-best-weights and patience, it'll restore the best weights back\n",
        "  # # print(\"Restored weights at \", restored_weights, \"Patience used: \", patience)\n",
        "\n",
        "  # ax[0].plot(range(1,tot_epochs+1), model_history['categorical_accuracy'], label=label+str(\"Train\") , linestyle=\"--\"    )\n",
        "  ax[0].plot(range(1,tot_epochs+1), model_history['val_categorical_accuracy'] , label=label+str(\"Val\"), linestyle=':' )\n",
        "  # ax[0].scatter((restored_weights), model_history['val_categorical_accuracy'][restored_weights-1] , color='orange', label=\"Val\")\n",
        "  # ax[0].scatter(restored_weights, model_history['categorical_accuracy'][restored_weights-1], color='blue', label=\"Train\")\n",
        "  # ax[0].annotate(text=str(restored_weights),  xy=(restored_weights, model_history['val_categorical_accuracy'][restored_weights-1]),\n",
        "  #                 textcoords=\"offset points\", xytext=(0,10), ha='center', color='black')\n",
        "  ax[0].legend()\n",
        "  ax[0].set_title('Training (Blue) and Validation (Orange) Accuracy', fontsize='8')\n",
        "\n",
        "\n",
        "\n",
        "  # ax[1].plot(range(1,tot_epochs+1), model_history['loss'],  label=label+str(\"Train\") , linestyle=\"--\"  )\n",
        "  ax[1].plot(range(1,tot_epochs+1), model_history['val_loss'],  label=label+str(\"Val\"), linestyle=':' )\n",
        "  # ax[1].scatter(restored_weights, model_history['loss'][restored_weights-1], color='blue')\n",
        "  # ax[1].scatter((restored_weights), model_history['val_loss'][restored_weights-1] , color='orange')\n",
        "  # ax[1].annotate(text=str(restored_weights),  xy=(restored_weights, model_history['val_loss'][restored_weights-1]),\n",
        "  #                 textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "  ax[1].legend()\n",
        "  ax[1].set_title('Training (Blue) and Validation (Orange) Loss' , fontsize='8')\n",
        "\n",
        "\n",
        "  ax[2].plot(range(1,tot_epochs+1), model_history['9T_4P'],  label=label  )\n",
        "  # ax[2].plot(range(1,tot_epochs+1), model_history['9T_4P'], color='orange',  )\n",
        "  # ax[1].scatter(restored_weights, model_history['loss'][restored_weights-1], color='blue')\n",
        "  # ax[1].scatter((restored_weights), model_history['val_loss'][restored_weights-1] , color='orange')\n",
        "  # ax[1].annotate(text=str(restored_weights),  xy=(restored_weights, model_history['val_loss'][restored_weights-1]),\n",
        "  #                 textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "  ax[2].legend()\n",
        "  ax[2].set_title('Validation Misclassifications of 9T_4P' , fontsize='8')\n",
        "\n",
        "\n",
        "  plt.gcf().set_size_inches(10, 5)  # this works \n",
        "  # plt.gcf().suptitle(f\"Lambda Value {lambda_val} for {nb_epoch} Epochs and Patience {patience} \" )\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UGXnD-CEdeUP"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fig = plt.figure(figsize=(20, 10))\n",
        "# fig, ax = plt.subplots(1,3)\n",
        "\n",
        "# # for index, row in scratch_df.iterrows():\n",
        "\n",
        "# for index, row in scratch_df.iterrows():\n",
        "#   # print(index)\n",
        "#   plot_model_history_all(scratch_df.iloc[index][\"model_history\"], label=str(row[\"lambdas\"]))\n",
        "    "
      ],
      "metadata": {
        "id": "QDOE-VIObokY"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "\n",
        "# Digging into a Dataframe by Column:\n",
        "\n",
        "#Importing pandas\n",
        "import pandas as pd\n",
        "\n",
        "#Creating a dataframe\n",
        "df = pd.DataFrame({\"A\": [1,2,3], \"B\": [4,5,6], \"C\": [7,8,9]})\n",
        "\n",
        "#Accessing column data by label\n",
        "column_a = df[\"A\"]\n",
        "\n",
        "#Printing the column data\n",
        "print(column_a)\n",
        "\n",
        "# Digging into a Dataframe by Row:\n",
        "  \n",
        "#Accessing row data by index\n",
        "row_1 = df.iloc[1]\n",
        "\n",
        "#Printing the row data\n",
        "print(row_1)\n",
        "\n",
        "# Digging into a Dataframe by Row and Column: \n",
        "\n",
        "#Accessing row and column data by index and label\n",
        "cell_1_a = df.iloc[1][\"A\"]\n",
        "\n",
        "#Printing the row and column data\n",
        "print(cell_1_a)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "hrf7MJVbjK4i"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eyBv8rlojM44"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "\n",
        "df.groupby(\"cost\")\n",
        "\n",
        "for x, group in df.groupby(\"cost\"):\n",
        "  # fig = plt.figure(figsize=(20, 10)) \n",
        "  # fig, ax = plt.subplots(1,2)\n",
        "  print(\"Group of Lambda-Value:\",   x)\n",
        "\n",
        "  # group\n",
        "  for item in group.model_history:\n",
        "    plot_model_history_all(item)    \n",
        "  plt.show()\n",
        "  \n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "XdhEBb6Gj-za"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Analysis as of Completing all 11 Runs and 2 Baselines with 3 reps each"
      ],
      "metadata": {
        "id": "xo2za1EMlmIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import pandas as pd\n",
        "\n",
        "# df1 = pd.read_pickle('df (4).pkl')\n",
        "# df2 = pd.read_pickle('df (5).pkl')\n",
        "\n",
        "# df = pd.concat([df1, df2])\n",
        "# df = df.reset_index(drop=True)\n",
        " \n",
        "#I already combined the runs and saved as a pickle file into MyDrive so now I recover that file\n",
        "import pickle\n",
        "\n",
        "# Define the file path and name\n",
        "mypath = \"/content/drive/MyDrive/Con3/Exp_5bins_11Runs_40D_SGD_Lr0point1_diffSeedsPerRep/\"\n",
        "pickle_file = mypath + \"all_runs_as_dataframe.pkl\"\n",
        "\n",
        "# Load the pickle file\n",
        "with open(pickle_file, 'rb') as f:\n",
        "    df = pickle.load(f)\n",
        "\n",
        "# Now you can work with the loaded dataframe\n",
        "# For example, you can show the first five rows of the dataframe\n",
        "# df.head()"
      ],
      "metadata": {
        "id": "O_qTth58Gcxk"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We dont need the model_history column anymore because we pulled out all the objects within that dictionary and made them their own columns.  So we delete / drop that column."
      ],
      "metadata": {
        "id": "10wrFh53jSWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# Delete the 'model_history' column because the information is already contained in all the other columns\n",
        "df = df.drop('model_history', axis=1)\n",
        "\n",
        "# Display the updated dataframe which is half the memory of the previous dataframe\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "S9GTPWRIF_K8",
        "outputId": "898a2302-d1c9-4d71-fa25-e3bf4d07583c"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   rep  bins           lambdas final_saved_weights_string  \\\n",
              "0  0.0   5.0   [1, 1, 1, 1, 1]            rep0_1_1_1_1_1_   \n",
              "1  1.0   5.0   [1, 1, 1, 1, 1]            rep1_1_1_1_1_1_   \n",
              "2  2.0   5.0   [1, 1, 1, 1, 1]            rep2_1_1_1_1_1_   \n",
              "3  0.0   5.0  [1, 1, 1, 1, 10]           rep0_1_1_1_1_10_   \n",
              "4  1.0   5.0  [1, 1, 1, 1, 10]           rep1_1_1_1_1_10_   \n",
              "\n",
              "                                                loss  \\\n",
              "0  [0.9456429481506348, 0.4860853850841522, 0.407...   \n",
              "1  [0.9782470464706421, 0.5124708414077759, 0.427...   \n",
              "2  [0.9994550347328186, 0.5300266742706299, 0.438...   \n",
              "3  [0.9456429481506348, 0.486085444688797, 0.4073...   \n",
              "4  [0.9782470464706421, 0.5124708414077759, 0.427...   \n",
              "\n",
              "                                categorical_accuracy  \\\n",
              "0  [0.6949666738510132, 0.854449987411499, 0.8777...   \n",
              "1  [0.685366690158844, 0.843833327293396, 0.87231...   \n",
              "2  [0.6777166724205017, 0.8369500041007996, 0.867...   \n",
              "3  [0.6949666738510132, 0.854449987411499, 0.8777...   \n",
              "4  [0.685366690158844, 0.843833327293396, 0.87231...   \n",
              "\n",
              "                                            val_loss  \\\n",
              "0  [0.3900302052497864, 0.31284916400909424, 0.26...   \n",
              "1  [0.4170288145542145, 0.32572802901268005, 0.28...   \n",
              "2  [0.4193570911884308, 0.33019378781318665, 0.28...   \n",
              "3  [0.3900302052497864, 0.31284916400909424, 0.26...   \n",
              "4  [0.41702884435653687, 0.32572802901268005, 0.2...   \n",
              "\n",
              "                            val_categorical_accuracy  \\\n",
              "0  [0.8893333077430725, 0.9082666635513306, 0.920...   \n",
              "1  [0.8845333456993103, 0.9050666689872742, 0.917...   \n",
              "2  [0.887066662311554, 0.9058666825294495, 0.9170...   \n",
              "3  [0.8893333077430725, 0.9082666635513306, 0.920...   \n",
              "4  [0.8845333456993103, 0.9050666689872742, 0.917...   \n",
              "\n",
              "                                               9T_4P  \\\n",
              "0  [40, 32, 34, 27, 34, 14, 11, 18, 17, 12, 16, 1...   \n",
              "1  [64, 66, 46, 22, 37, 15, 9, 12, 9, 22, 12, 16,...   \n",
              "2  [36, 42, 41, 18, 31, 21, 10, 17, 12, 17, 25, 3...   \n",
              "3  [40, 32, 34, 27, 34, 14, 11, 18, 17, 12, 16, 1...   \n",
              "4  [64, 66, 46, 22, 37, 15, 9, 12, 9, 18, 13, 15,...   \n",
              "\n",
              "                                               4T_9P  ...  \\\n",
              "0  [43, 35, 20, 27, 16, 33, 27, 19, 19, 22, 16, 1...  ...   \n",
              "1  [32, 20, 21, 38, 21, 31, 46, 28, 27, 16, 26, 2...  ...   \n",
              "2  [54, 31, 24, 42, 19, 29, 50, 28, 38, 32, 16, 1...  ...   \n",
              "3  [43, 35, 20, 27, 16, 33, 27, 19, 19, 22, 16, 1...  ...   \n",
              "4  [32, 20, 21, 38, 21, 31, 46, 28, 27, 17, 26, 2...  ...   \n",
              "\n",
              "                                              6T_Acc  \\\n",
              "0  [0.9263456090651558, 0.9348441926345609, 0.944...   \n",
              "1  [0.9305949008498584, 0.93342776203966, 0.93909...   \n",
              "2  [0.93342776203966, 0.9376770538243626, 0.92776...   \n",
              "3  [0.9263456090651558, 0.9348441926345609, 0.944...   \n",
              "4  [0.9305949008498584, 0.93342776203966, 0.93909...   \n",
              "\n",
              "                                              7T_Acc  \\\n",
              "0  [0.8569553805774278, 0.8976377952755905, 0.902...   \n",
              "1  [0.884514435695538, 0.8910761154855643, 0.9146...   \n",
              "2  [0.863517060367454, 0.910761154855643, 0.90288...   \n",
              "3  [0.8569553805774278, 0.8976377952755905, 0.902...   \n",
              "4  [0.884514435695538, 0.8910761154855643, 0.9146...   \n",
              "\n",
              "                                              8T_Acc  \\\n",
              "0  [0.8590971272229823, 0.8549931600547196, 0.868...   \n",
              "1  [0.8248974008207934, 0.8714090287277702, 0.857...   \n",
              "2  [0.8604651162790697, 0.8549931600547196, 0.859...   \n",
              "3  [0.8590971272229823, 0.8549931600547196, 0.868...   \n",
              "4  [0.8248974008207934, 0.8714090287277702, 0.857...   \n",
              "\n",
              "                                              9T_Acc  \\\n",
              "0  [0.8608582574772432, 0.8842652795838751, 0.880...   \n",
              "1  [0.7958387516254877, 0.8218465539661899, 0.850...   \n",
              "2  [0.8621586475942783, 0.8660598179453837, 0.879...   \n",
              "3  [0.8608582574772432, 0.8842652795838751, 0.880...   \n",
              "4  [0.7958387516254877, 0.8218465539661899, 0.850...   \n",
              "\n",
              "                                        cm_per_epoch  \\\n",
              "0  [[[692, 0, 2, 0, 0, 8, 12, 1, 4, 0, 0, 821, 2,...   \n",
              "1  [[[695, 0, 1, 0, 0, 7, 12, 1, 3, 0, 0, 825, 6,...   \n",
              "2  [[[696, 0, 0, 0, 1, 7, 10, 1, 4, 0, 0, 833, 2,...   \n",
              "3  [[[692, 0, 2, 0, 0, 8, 12, 1, 4, 0, 0, 821, 2,...   \n",
              "4  [[[695, 0, 1, 0, 0, 7, 12, 1, 3, 0, 0, 825, 6,...   \n",
              "\n",
              "                                            f1_micro  \\\n",
              "0  [0.8893333333333333, 0.9082666666666667, 0.920...   \n",
              "1  [0.8845333333333333, 0.9050666666666667, 0.917...   \n",
              "2  [0.8870666666666667, 0.9058666666666667, 0.917...   \n",
              "3  [0.8893333333333333, 0.9082666666666667, 0.920...   \n",
              "4  [0.8845333333333333, 0.9050666666666667, 0.917...   \n",
              "\n",
              "                                            f1_macro  \\\n",
              "0  [0.8882316895423983, 0.9072518662116483, 0.919...   \n",
              "1  [0.8823194816144022, 0.9039453346304072, 0.916...   \n",
              "2  [0.8850595642681671, 0.9044252079409356, 0.915...   \n",
              "3  [0.8882316895423983, 0.9072518662116483, 0.919...   \n",
              "4  [0.8823194816144022, 0.9039453346304072, 0.916...   \n",
              "\n",
              "                                         f1_weighted  \\\n",
              "0  [0.8891085314112255, 0.9079025892372772, 0.920...   \n",
              "1  [0.8834613877897937, 0.9047057870312513, 0.917...   \n",
              "2  [0.88651391511855, 0.9055499596598852, 0.91678...   \n",
              "3  [0.8891085314112255, 0.9079025892372772, 0.920...   \n",
              "4  [0.8834613877897937, 0.9047057870312513, 0.917...   \n",
              "\n",
              "                                      f1_notweighted             datetime  \n",
              "0  [[0.9434219495569189, 0.9469434832756631, 0.88...  2023-05-04 22:31:34  \n",
              "1  [[0.943652410047522, 0.9515570934256055, 0.878...  2023-05-04 22:34:24  \n",
              "2  [[0.9475833900612662, 0.9602305475504322, 0.88...  2023-05-04 22:37:12  \n",
              "3  [[0.9434219495569189, 0.9469434832756631, 0.88...  2023-05-04 22:40:01  \n",
              "4  [[0.943652410047522, 0.9515570934256055, 0.878...  2023-05-04 22:42:49  \n",
              "\n",
              "[5 rows x 26 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5dc5623f-5db8-4fc8-a1ec-875a09d55787\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rep</th>\n",
              "      <th>bins</th>\n",
              "      <th>lambdas</th>\n",
              "      <th>final_saved_weights_string</th>\n",
              "      <th>loss</th>\n",
              "      <th>categorical_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_categorical_accuracy</th>\n",
              "      <th>9T_4P</th>\n",
              "      <th>4T_9P</th>\n",
              "      <th>...</th>\n",
              "      <th>6T_Acc</th>\n",
              "      <th>7T_Acc</th>\n",
              "      <th>8T_Acc</th>\n",
              "      <th>9T_Acc</th>\n",
              "      <th>cm_per_epoch</th>\n",
              "      <th>f1_micro</th>\n",
              "      <th>f1_macro</th>\n",
              "      <th>f1_weighted</th>\n",
              "      <th>f1_notweighted</th>\n",
              "      <th>datetime</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[1, 1, 1, 1, 1]</td>\n",
              "      <td>rep0_1_1_1_1_1_</td>\n",
              "      <td>[0.9456429481506348, 0.4860853850841522, 0.407...</td>\n",
              "      <td>[0.6949666738510132, 0.854449987411499, 0.8777...</td>\n",
              "      <td>[0.3900302052497864, 0.31284916400909424, 0.26...</td>\n",
              "      <td>[0.8893333077430725, 0.9082666635513306, 0.920...</td>\n",
              "      <td>[40, 32, 34, 27, 34, 14, 11, 18, 17, 12, 16, 1...</td>\n",
              "      <td>[43, 35, 20, 27, 16, 33, 27, 19, 19, 22, 16, 1...</td>\n",
              "      <td>...</td>\n",
              "      <td>[0.9263456090651558, 0.9348441926345609, 0.944...</td>\n",
              "      <td>[0.8569553805774278, 0.8976377952755905, 0.902...</td>\n",
              "      <td>[0.8590971272229823, 0.8549931600547196, 0.868...</td>\n",
              "      <td>[0.8608582574772432, 0.8842652795838751, 0.880...</td>\n",
              "      <td>[[[692, 0, 2, 0, 0, 8, 12, 1, 4, 0, 0, 821, 2,...</td>\n",
              "      <td>[0.8893333333333333, 0.9082666666666667, 0.920...</td>\n",
              "      <td>[0.8882316895423983, 0.9072518662116483, 0.919...</td>\n",
              "      <td>[0.8891085314112255, 0.9079025892372772, 0.920...</td>\n",
              "      <td>[[0.9434219495569189, 0.9469434832756631, 0.88...</td>\n",
              "      <td>2023-05-04 22:31:34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[1, 1, 1, 1, 1]</td>\n",
              "      <td>rep1_1_1_1_1_1_</td>\n",
              "      <td>[0.9782470464706421, 0.5124708414077759, 0.427...</td>\n",
              "      <td>[0.685366690158844, 0.843833327293396, 0.87231...</td>\n",
              "      <td>[0.4170288145542145, 0.32572802901268005, 0.28...</td>\n",
              "      <td>[0.8845333456993103, 0.9050666689872742, 0.917...</td>\n",
              "      <td>[64, 66, 46, 22, 37, 15, 9, 12, 9, 22, 12, 16,...</td>\n",
              "      <td>[32, 20, 21, 38, 21, 31, 46, 28, 27, 16, 26, 2...</td>\n",
              "      <td>...</td>\n",
              "      <td>[0.9305949008498584, 0.93342776203966, 0.93909...</td>\n",
              "      <td>[0.884514435695538, 0.8910761154855643, 0.9146...</td>\n",
              "      <td>[0.8248974008207934, 0.8714090287277702, 0.857...</td>\n",
              "      <td>[0.7958387516254877, 0.8218465539661899, 0.850...</td>\n",
              "      <td>[[[695, 0, 1, 0, 0, 7, 12, 1, 3, 0, 0, 825, 6,...</td>\n",
              "      <td>[0.8845333333333333, 0.9050666666666667, 0.917...</td>\n",
              "      <td>[0.8823194816144022, 0.9039453346304072, 0.916...</td>\n",
              "      <td>[0.8834613877897937, 0.9047057870312513, 0.917...</td>\n",
              "      <td>[[0.943652410047522, 0.9515570934256055, 0.878...</td>\n",
              "      <td>2023-05-04 22:34:24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[1, 1, 1, 1, 1]</td>\n",
              "      <td>rep2_1_1_1_1_1_</td>\n",
              "      <td>[0.9994550347328186, 0.5300266742706299, 0.438...</td>\n",
              "      <td>[0.6777166724205017, 0.8369500041007996, 0.867...</td>\n",
              "      <td>[0.4193570911884308, 0.33019378781318665, 0.28...</td>\n",
              "      <td>[0.887066662311554, 0.9058666825294495, 0.9170...</td>\n",
              "      <td>[36, 42, 41, 18, 31, 21, 10, 17, 12, 17, 25, 3...</td>\n",
              "      <td>[54, 31, 24, 42, 19, 29, 50, 28, 38, 32, 16, 1...</td>\n",
              "      <td>...</td>\n",
              "      <td>[0.93342776203966, 0.9376770538243626, 0.92776...</td>\n",
              "      <td>[0.863517060367454, 0.910761154855643, 0.90288...</td>\n",
              "      <td>[0.8604651162790697, 0.8549931600547196, 0.859...</td>\n",
              "      <td>[0.8621586475942783, 0.8660598179453837, 0.879...</td>\n",
              "      <td>[[[696, 0, 0, 0, 1, 7, 10, 1, 4, 0, 0, 833, 2,...</td>\n",
              "      <td>[0.8870666666666667, 0.9058666666666667, 0.917...</td>\n",
              "      <td>[0.8850595642681671, 0.9044252079409356, 0.915...</td>\n",
              "      <td>[0.88651391511855, 0.9055499596598852, 0.91678...</td>\n",
              "      <td>[[0.9475833900612662, 0.9602305475504322, 0.88...</td>\n",
              "      <td>2023-05-04 22:37:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[1, 1, 1, 1, 10]</td>\n",
              "      <td>rep0_1_1_1_1_10_</td>\n",
              "      <td>[0.9456429481506348, 0.486085444688797, 0.4073...</td>\n",
              "      <td>[0.6949666738510132, 0.854449987411499, 0.8777...</td>\n",
              "      <td>[0.3900302052497864, 0.31284916400909424, 0.26...</td>\n",
              "      <td>[0.8893333077430725, 0.9082666635513306, 0.920...</td>\n",
              "      <td>[40, 32, 34, 27, 34, 14, 11, 18, 17, 12, 16, 1...</td>\n",
              "      <td>[43, 35, 20, 27, 16, 33, 27, 19, 19, 22, 16, 1...</td>\n",
              "      <td>...</td>\n",
              "      <td>[0.9263456090651558, 0.9348441926345609, 0.944...</td>\n",
              "      <td>[0.8569553805774278, 0.8976377952755905, 0.902...</td>\n",
              "      <td>[0.8590971272229823, 0.8549931600547196, 0.868...</td>\n",
              "      <td>[0.8608582574772432, 0.8842652795838751, 0.880...</td>\n",
              "      <td>[[[692, 0, 2, 0, 0, 8, 12, 1, 4, 0, 0, 821, 2,...</td>\n",
              "      <td>[0.8893333333333333, 0.9082666666666667, 0.920...</td>\n",
              "      <td>[0.8882316895423983, 0.9072518662116483, 0.919...</td>\n",
              "      <td>[0.8891085314112255, 0.9079025892372772, 0.920...</td>\n",
              "      <td>[[0.9434219495569189, 0.9469434832756631, 0.88...</td>\n",
              "      <td>2023-05-04 22:40:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[1, 1, 1, 1, 10]</td>\n",
              "      <td>rep1_1_1_1_1_10_</td>\n",
              "      <td>[0.9782470464706421, 0.5124708414077759, 0.427...</td>\n",
              "      <td>[0.685366690158844, 0.843833327293396, 0.87231...</td>\n",
              "      <td>[0.41702884435653687, 0.32572802901268005, 0.2...</td>\n",
              "      <td>[0.8845333456993103, 0.9050666689872742, 0.917...</td>\n",
              "      <td>[64, 66, 46, 22, 37, 15, 9, 12, 9, 18, 13, 15,...</td>\n",
              "      <td>[32, 20, 21, 38, 21, 31, 46, 28, 27, 17, 26, 2...</td>\n",
              "      <td>...</td>\n",
              "      <td>[0.9305949008498584, 0.93342776203966, 0.93909...</td>\n",
              "      <td>[0.884514435695538, 0.8910761154855643, 0.9146...</td>\n",
              "      <td>[0.8248974008207934, 0.8714090287277702, 0.857...</td>\n",
              "      <td>[0.7958387516254877, 0.8218465539661899, 0.850...</td>\n",
              "      <td>[[[695, 0, 1, 0, 0, 7, 12, 1, 3, 0, 0, 825, 6,...</td>\n",
              "      <td>[0.8845333333333333, 0.9050666666666667, 0.917...</td>\n",
              "      <td>[0.8823194816144022, 0.9039453346304072, 0.916...</td>\n",
              "      <td>[0.8834613877897937, 0.9047057870312513, 0.917...</td>\n",
              "      <td>[[0.943652410047522, 0.9515570934256055, 0.878...</td>\n",
              "      <td>2023-05-04 22:42:49</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 26 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5dc5623f-5db8-4fc8-a1ec-875a09d55787')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5dc5623f-5db8-4fc8-a1ec-875a09d55787 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5dc5623f-5db8-4fc8-a1ec-875a09d55787');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "#code to save dataframe as pickle file\n",
        "# import pandas as pd\n",
        "\n",
        "# # Assuming your dataframe is called \"df\"\n",
        "\n",
        "# # Save dataframe as pickle file\n",
        "# df.to_pickle('dataframe.pkl')\n",
        "\n",
        "# # Download the pickle file in Google Colab\n",
        "# from google.colab import files\n",
        "# files.download('dataframe.pkl')  #prompts a download to My 'Downloads' on computer\n",
        "\n",
        "# import pickle \n",
        "\n",
        "# with open(str(mydrive_path) + \"/\" + str('dataframe.pkl'), 'wb') as f:\n",
        "#     pickle.dump(df, f)\n",
        "# print('Dataframe Pickle file saved to: ', mydrive_path)     #saves to My Google Drive without asking for permission"
      ],
      "metadata": {
        "id": "xW8I4jcyjjRj",
        "cellView": "form"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['lambdas'].value_counts()"
      ],
      "metadata": {
        "id": "x9LRH980GpSB",
        "outputId": "53ecce19-ed0c-4774-f3a9-bffc9710473f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 1, 1, 1]              3\n",
              "[1, 1, 1, 1, 10]             3\n",
              "[1, 1, 1, 1, 100]            3\n",
              "[1, 1, 1, 10, 1]             3\n",
              "[1, 1, 1, 100, 1]            3\n",
              "[1, 1, 10, 1, 1]             3\n",
              "[1, 1, 100, 1, 1]            3\n",
              "[1, 10, 1, 1, 1]             3\n",
              "[1, 100, 1, 1, 1]            3\n",
              "[10, 1, 1, 1, 1]             3\n",
              "[100, 1, 1, 1, 1]            3\n",
              "[10, 10, 10, 10, 10]         3\n",
              "[100, 100, 100, 100, 100]    3\n",
              "Name: lambdas, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have three replicates for each of the 11 runs, and the two other baselines with all 10s and all 100s.  We want to know, is there a differene when training the neural network with different weights at different times.  For example, is it better to give a lot of weight to the misclassification you do not want to happen while you train the neural network early on, or is it better to let the neural network train generally first and then increase the weight, and then let it to settle down?  Of course the answer \"depends\" on what you care about.  Unfortunately we're not sure what we really care about  😆   we care about Overall Accuracy, and OVerall rate of misclassifications, but we don't have a set percentage in mind.  We just know we want the overall accuracy to remain as high as possible, and the overall rate of misclassification to remain as low as possible.   It seems that you could create different multiple objective functions using these facts, but at the moment we are just exploring the art of the possible with Cost-of-Misclassifications. "
      ],
      "metadata": {
        "id": "0GUnv7PMQiQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So there's at least four schools of thought. \n",
        "\n",
        "\n",
        "1.   Train like normal (baseline costs are all 1) to get best overall accuracy\n",
        "2.   Train to misclassifications (baseline cost is high on misclass rate you care about)\n",
        "3.   Train like normal and then amp up the cost of misclassification.\n",
        "4.   Train to misclassifications (baseline cost is high on misclass rate you care about) and then lower the cost of mistakes to allow overall accuracy to recover. \n",
        "\n",
        "What do these four 'worlds' of training look like and how would you compare them?\n",
        "\n"
      ],
      "metadata": {
        "id": "eQxDZX7WRg58"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to filter our complicated dataframe to only show values for a given lambda combination (we define a target_array)\n"
      ],
      "metadata": {
        "id": "BLB0082Q7hpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the array to match\n",
        "# target_array = np.array([100, 100, 100, 100, 100])\n",
        "# target_array = np.array([1, 1, 1, 1, 1])\n",
        "target_array = np.array([10, 10, 10, 10, 10])\n",
        "\n",
        "# Filter rows where the \"lambdas\" column matches the target array\n",
        "filtered_df = df.loc[df['lambdas'].apply(lambda x: np.array_equal(x, target_array))]\n",
        "\n",
        "print(filtered_df.columns)"
      ],
      "metadata": {
        "id": "ojzf8Gd-P8DU",
        "outputId": "8239a9aa-0383-48ed-ac7f-e374d1ec0970",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['rep', 'bins', 'lambdas', 'final_saved_weights_string', 'loss',\n",
            "       'categorical_accuracy', 'val_loss', 'val_categorical_accuracy', '9T_4P',\n",
            "       '4T_9P', '0T_Acc', '1T_Acc', '2T_Acc', '3T_Acc', '4T_Acc', '5T_Acc',\n",
            "       '6T_Acc', '7T_Acc', '8T_Acc', '9T_Acc', 'cm_per_epoch', 'f1_micro',\n",
            "       'f1_macro', 'f1_weighted', 'f1_notweighted', 'datetime'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FilteredDataFrame now consists of just the Run with all 100s.  What do we know about these three runs?\n",
        "\n",
        "* What epoch would early stop normally stop if not Epoch 110?  It would be the epoch where early stop, given a patience level, does not detect a decrease in validation-error over a certain number of epochs.\n",
        "* What is the mean for all the replicates for the validation set for all recorded metrics?"
      ],
      "metadata": {
        "id": "sGy_i05QiF4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have a dataframe called 'filtered_df' with a column named 'val_loss'\n",
        "\n",
        "# Define the column name\n",
        "column_name = 'val_loss'\n",
        "\n",
        "# Initialize lists to store minimum indexes and minimum losses\n",
        "min_indexes = []\n",
        "min_losses = []\n",
        "reps = []\n",
        "\n",
        "\n",
        "\n",
        "# Initialize an array to store the values for mean line\n",
        "mean_values = np.zeros(110)  # Assuming there are 110 epochs\n",
        "\n",
        "\n",
        "# Iterate over each row in the dataframe\n",
        "for row_index, row in filtered_df.iterrows():\n",
        "    # Get the array of values from the current row and column\n",
        "    values = row[column_name]\n",
        "    rep_value = int(row['rep'])\n",
        "    color = plt.get_cmap('tab10')(rep_value % 10)\n",
        "\n",
        "    # Create an array for the x-axis (epochs)\n",
        "    epochs = np.arange(1, len(values) + 1)\n",
        "\n",
        "    # Plot the array of values\n",
        "    plt.plot(epochs, values, label=f'Rep {rep_value}', color=color)\n",
        "\n",
        "    # Find the index of the minimum 'val_loss' value\n",
        "    min_index = np.argmin(values)\n",
        "    min_loss = round(values[min_index], 4)\n",
        "\n",
        "    # Annotate the minimum value Epoch point on the graph\n",
        "    plt.annotate(f'{epochs[min_index]}', xy=(epochs[min_index],min_loss),\n",
        "                 xytext=(epochs[min_index]-0.5, 0.05), fontsize=10, color=color,\n",
        "                 #arrowprops=dict(facecolor='black', arrowstyle='->')\n",
        "                 )\n",
        "\n",
        "    # Add scatter plot markers for the minimum index and minimum loss\n",
        "    plt.scatter(epochs[min_index], values[min_index], color=color, marker='o')\n",
        "\n",
        "    \n",
        "    # Append the minimum index and minimum loss to the lists\n",
        "    min_indexes.append(epochs[min_index])\n",
        "    min_losses.append(min_loss)\n",
        "    reps.append(rep_value)\n",
        "\n",
        "    # Accumulate values for mean line\n",
        "    mean_values += values\n",
        "\n",
        "# Calculate the mean values\n",
        "mean_values /= len(filtered_df)\n",
        "\n",
        "# Plot the mean line\n",
        "plt.plot(epochs, mean_values, color='black', linestyle='--', label='Mean')\n",
        "\n",
        "\n",
        "# Find the index of the minimum value in mean_values\n",
        "mean_min_index = np.argmin(mean_values)\n",
        "mean_min_loss = round(mean_values[mean_min_index], 2)\n",
        "\n",
        "# Annotate the minimum value of the mean line on the graph\n",
        "plt.annotate(f'{epochs[mean_min_index]}', xy=(epochs[mean_min_index], mean_values[mean_min_index]),\n",
        "             xytext=(epochs[mean_min_index], 0.05),\n",
        "            #  arrowprops=dict(facecolor='black', arrowstyle='->')\n",
        "            )\n",
        "\n",
        "# Add scatter plot marker for the minimum value of the mean line\n",
        "plt.scatter(epochs[mean_min_index], mean_values[mean_min_index], color='black', marker='o',  zorder=10)\n",
        "# plt.scatter(epochs[mean_min_index], mean_values[mean_min_index], color='pink', edgecolor='black', marker='o', s=100, zorder=10)\n",
        "\n",
        "\n",
        "# Set the x-axis limits starting from 0\n",
        "plt.xlim(0)\n",
        "\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Validation Loss')\n",
        "plt.title('Validation Loss for Each Row in Column {}'.format(column_name))\n",
        "\n",
        "# Add legend\n",
        "plt.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "# Return the lists of minimum indexes and minimum losses\n",
        "print(f'The minimum validation loss occurs at the epochs: {min_indexes} for reps {reps}')\n",
        "# min_indexes, min_losses\n",
        "print(\"epochs[mean_min_index]: \", epochs[mean_min_index])\n",
        "print(\"mean_values[mean_min_index]: \",  mean_values[mean_min_index])\n",
        "print(\"mean_min_loss rounded: \", mean_min_loss)\n",
        "\n"
      ],
      "metadata": {
        "id": "YEuCb_GC9jOy",
        "outputId": "75cec819-385e-4c7f-bf00-fc37efe4e9a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The minimum validation loss occurs at the epochs: [96, 90, 87] for reps [0, 1, 2]\n",
            "epochs[mean_min_index]:  95\n",
            "mean_values[mean_min_index]:  0.13277714947859445\n",
            "mean_min_loss rounded:  0.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#What Do We Know after Running Analysis at Epoch 110"
      ],
      "metadata": {
        "id": "BOk-YKM0ANwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK so what do we know.  If we train the simple dense neural network on the MNIST dataset with a constant lambda value of 1 for all misclassifications, on average for three replicates, we get the following after 110 epochs: "
      ],
      "metadata": {
        "id": "en3rlFJ958V2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for column in df.columns:\n",
        "#     print(f\"Column: {column}\")\n",
        "#     print(f\"Type: {type(df[column][0])}\")\n",
        "#     print(f\"Dimension: {np.shape(df[column][0])}\")\n",
        "#     print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ujB2Q65TH4LM",
        "outputId": "4df6f5bd-0722-4fb2-8889-6dc19a3baa16"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column: rep\n",
            "Type: <class 'numpy.float64'>\n",
            "Dimension: ()\n",
            "\n",
            "Column: bins\n",
            "Type: <class 'numpy.float64'>\n",
            "Dimension: ()\n",
            "\n",
            "Column: lambdas\n",
            "Type: <class 'list'>\n",
            "Dimension: (5,)\n",
            "\n",
            "Column: final_saved_weights_string\n",
            "Type: <class 'str'>\n",
            "Dimension: ()\n",
            "\n",
            "Column: loss\n",
            "Type: <class 'list'>\n",
            "Dimension: (110,)\n",
            "\n",
            "Column: categorical_accuracy\n",
            "Type: <class 'list'>\n",
            "Dimension: (110,)\n",
            "\n",
            "Column: val_loss\n",
            "Type: <class 'list'>\n",
            "Dimension: (110,)\n",
            "\n",
            "Column: val_categorical_accuracy\n",
            "Type: <class 'list'>\n",
            "Dimension: (110,)\n",
            "\n",
            "Column: 9T_4P\n",
            "Type: <class 'list'>\n",
            "Dimension: (110,)\n",
            "\n",
            "Column: 4T_9P\n",
            "Type: <class 'list'>\n",
            "Dimension: (110,)\n",
            "\n",
            "Column: 0T_Acc\n",
            "Type: <class 'list'>\n",
            "Dimension: (110,)\n",
            "\n",
            "Column: 1T_Acc\n",
            "Type: <class 'list'>\n",
            "Dimension: (110,)\n",
            "\n",
            "Column: 2T_Acc\n",
            "Type: <class 'list'>\n",
            "Dimension: (110,)\n",
            "\n",
            "Column: 3T_Acc\n",
            "Type: <class 'list'>\n",
            "Dimension: (110,)\n",
            "\n",
            "Column: 4T_Acc\n",
            "Type: <class 'list'>\n",
            "Dimension: (110,)\n",
            "\n",
            "Column: 5T_Acc\n",
            "Type: <class 'list'>\n",
            "Dimension: (110,)\n",
            "\n",
            "Column: 6T_Acc\n",
            "Type: <class 'list'>\n",
            "Dimension: (110,)\n",
            "\n",
            "Column: 7T_Acc\n",
            "Type: <class 'list'>\n",
            "Dimension: (110,)\n",
            "\n",
            "Column: 8T_Acc\n",
            "Type: <class 'list'>\n",
            "Dimension: (110,)\n",
            "\n",
            "Column: 9T_Acc\n",
            "Type: <class 'list'>\n",
            "Dimension: (110,)\n",
            "\n",
            "Column: cm_per_epoch\n",
            "Type: <class 'list'>\n",
            "Dimension: (110, 1, 100)\n",
            "\n",
            "Column: f1_micro\n",
            "Type: <class 'list'>\n",
            "Dimension: (110,)\n",
            "\n",
            "Column: f1_macro\n",
            "Type: <class 'list'>\n",
            "Dimension: (110,)\n",
            "\n",
            "Column: f1_weighted\n",
            "Type: <class 'list'>\n",
            "Dimension: (110,)\n",
            "\n",
            "Column: f1_notweighted\n",
            "Type: <class 'list'>\n",
            "Dimension: (110, 10)\n",
            "\n",
            "Column: datetime\n",
            "Type: <class 'str'>\n",
            "Dimension: ()\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline: λ's are all 1 for each of the five bins of training.  Metrics are from Epoch 110"
      ],
      "metadata": {
        "id": "hNwzWmyXAhuk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jgILc0r34i4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_array = np.array([1, 1, 1, 1, 1])\n",
        "\n",
        "# Filter rows where the \"lambdas\" column matches the target array\n",
        "filtered_df = df.loc[df['lambdas'].apply(lambda x: np.array_equal(x, target_array))]\n",
        "\n",
        "\n",
        "epoch_to_study = 110\n",
        "mean_value = 0\n",
        "for rep in filtered_df.index:\n",
        "  print(\"rep \", rep, \", epoch \",epoch_to_study, \"value: \", filtered_df['val_loss'][rep][epoch_to_study-1])\n",
        "  mean_value += filtered_df['val_loss'][rep][epoch_to_study-1]\n",
        "\n",
        "mean_value /= len(filtered_df.index)\n",
        "print(\"Average over \", len(filtered_df.index),\"reps at epoch\", epoch_to_study, \"is\",  round(mean_value,3))\n",
        "  \n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbhONyMSALPo",
        "outputId": "4a24c500-37f4-4761-9fa3-ec55323bb214"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rep  0 , epoch  110 value:  0.11964289098978043\n",
            "rep  1 , epoch  110 value:  0.12336307018995285\n",
            "rep  2 , epoch  110 value:  0.13224121928215027\n",
            "Average over  3 reps at epoch 110 is 0.125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This is a newer version of get_data_by_epoch that uses a value to save time looking up the value of df[column][rep] \n",
        "#I also changed len < 109 because if the length is less than 110 epochs then i want to save the value \n",
        "# if equal to 109 then it just needs to save the epoch of interest\n",
        "#if the value isn't a list? hmm.. could that mean its an array?\n",
        "\n",
        "def get_data_by_epoch(filtered_df, epoch_to_study, rep):\n",
        "    row_data = {}\n",
        "\n",
        "    for column in filtered_df.columns:\n",
        "        value = filtered_df[column][rep]\n",
        "\n",
        "        if isinstance(value, list):\n",
        "            if len(value) < 109:\n",
        "                row_data[column] = value\n",
        "            else:\n",
        "                row_data[column] = value[epoch_to_study - 1]\n",
        "        else:\n",
        "            # print(column, \" isn't a list\") #prints \"rep\", \"bins\" \"final saved weights string\" and \"datetime\"\n",
        "            row_data[column] = value\n",
        "\n",
        "    df = pd.DataFrame.from_dict(row_data, orient=\"index\").transpose()\n",
        "    return df"
      ],
      "metadata": {
        "id": "tG6ljRfYFqUb"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "\n",
        "# Example usage\n",
        "epoch_to_study = 110\n",
        "\n",
        "df_dictionary = pd.DataFrame()\n",
        "\n",
        "for rep in filtered_df.index:\n",
        "  # print(rep)\n",
        "  row_data = get_data_by_epoch(filtered_df, epoch_to_study, rep)\n",
        "  df_dictionary = pd.concat([df_dictionary, row_data], ignore_index=True)\n",
        "\n",
        "print(df_dictionary)\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjJdjhb-XE6u",
        "outputId": "dae10459-23f9-4f64-907c-71c1673f2a76"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   rep bins          lambdas final_saved_weights_string      loss  \\\n",
            "0  0.0  5.0  [1, 1, 1, 1, 1]            rep0_1_1_1_1_1_  0.110624   \n",
            "1  1.0  5.0  [1, 1, 1, 1, 1]            rep1_1_1_1_1_1_  0.113079   \n",
            "2  2.0  5.0  [1, 1, 1, 1, 1]            rep2_1_1_1_1_1_  0.114868   \n",
            "\n",
            "  categorical_accuracy  val_loss val_categorical_accuracy 9T_4P 4T_9P  ...  \\\n",
            "0             0.965683  0.119643                 0.967867    18    11  ...   \n",
            "1              0.96485  0.123363                   0.9668     9    16  ...   \n",
            "2             0.964033  0.132241                 0.963867     4    40  ...   \n",
            "\n",
            "     6T_Acc    7T_Acc    8T_Acc    9T_Acc  \\\n",
            "0  0.970255  0.969816  0.948016  0.945384   \n",
            "1  0.974504  0.965879   0.93844  0.950585   \n",
            "2  0.968839  0.971129  0.937073  0.958388   \n",
            "\n",
            "                                        cm_per_epoch  f1_micro  f1_macro  \\\n",
            "0  [[710, 0, 1, 1, 0, 3, 1, 1, 2, 0, 0, 836, 2, 4...  0.967867   0.96766   \n",
            "1  [[710, 0, 2, 1, 1, 2, 1, 1, 1, 0, 0, 836, 3, 2...    0.9668  0.966562   \n",
            "2  [[711, 0, 2, 0, 0, 0, 1, 2, 2, 1, 0, 833, 4, 4...  0.963867  0.963722   \n",
            "\n",
            "  f1_weighted                                     f1_notweighted  \\\n",
            "0    0.967853  [0.9766162310866574, 0.9823736780258518, 0.972...   \n",
            "1     0.96676  [0.9752747252747253, 0.9817968291250734, 0.971...   \n",
            "2    0.963871  [0.9726402188782489, 0.982311320754717, 0.9744...   \n",
            "\n",
            "              datetime  \n",
            "0  2023-05-04 22:31:34  \n",
            "1  2023-05-04 22:34:24  \n",
            "2  2023-05-04 22:37:12  \n",
            "\n",
            "[3 rows x 26 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_elements(df, epoch_to_study):\n",
        "#     result = {}\n",
        "#     for column in df.columns:\n",
        "#         if isinstance(df[column].iloc[0], list):\n",
        "#             result[column] = df[column].apply(lambda x: x[:epoch_to_study])\n",
        "#         else:\n",
        "#             result[column] = df[column]\n",
        "#     return result\n",
        "\n",
        "def get_element(df, epoch_to_study):\n",
        "    result = {}\n",
        "    for column in df.columns:\n",
        "      value = df[column].iloc[0]\n",
        "      if isinstance(value, list):\n",
        "          # print(column, \" column item 0 has length... \",  len(value) )\n",
        "          if len(value) == 110:\n",
        "              # result[column] = value.apply(lambda x: x[epoch_to_study])\n",
        "              result[column] = df[column].apply(lambda x: x[epoch_to_study] if len(x) > epoch_to_study else x)\n",
        "\n",
        "          else:\n",
        "              result[column] = df[column]\n",
        "      else:\n",
        "          result[column] = df[column]\n",
        "    return result\n",
        "\n",
        "\n",
        "# def get_element(df, epoch_to_study):\n",
        "#     result = {}\n",
        "#     for column, values in df.items():\n",
        "#         value = values.iloc[0]\n",
        "#         if isinstance(value, list) and len(value) == 110:\n",
        "#             result[column] = [x[epoch_to_study] if len(x) > epoch_to_study else x for x in values]\n",
        "#         else:\n",
        "#             result[column] = values\n",
        "#     return result\n"
      ],
      "metadata": {
        "id": "KuT2JIceJZVk"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JYTSoqcO5PdQ"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# epoch_to_study = 109 #counting from 0, epoch 110 is 109 since I'm bound to forget this, I'll put it in the function\n",
        "# epoch_to_study_base_1 = 110\n",
        "\n",
        "\n",
        "epoch_to_study_base_1 = 5\n",
        "result = get_element(df, epoch_to_study_base_1 - 1)\n",
        "\n",
        "# Print the result\n",
        "for column in result:\n",
        "    print(f\"{column}:\\n{result[column]}\\n\")\n",
        "\n",
        "#Output the information seems incorrect for \"Epoch 0\", like validation_categorical_accuracy is already 0.967? \n",
        "# and how is it better than what was displayed for epoch 1?"
      ],
      "metadata": {
        "id": "dGtfWAMI0Gom",
        "outputId": "a6775abb-2aa8-48fc-e9e8-e5eb4fd4310e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rep:\n",
            "0     0.0\n",
            "1     1.0\n",
            "2     2.0\n",
            "3     0.0\n",
            "4     1.0\n",
            "5     2.0\n",
            "6     0.0\n",
            "7     1.0\n",
            "8     2.0\n",
            "9     0.0\n",
            "10    1.0\n",
            "11    2.0\n",
            "12    0.0\n",
            "13    1.0\n",
            "14    2.0\n",
            "15    0.0\n",
            "16    1.0\n",
            "17    2.0\n",
            "18    0.0\n",
            "19    1.0\n",
            "20    2.0\n",
            "21    0.0\n",
            "22    1.0\n",
            "23    2.0\n",
            "24    0.0\n",
            "25    1.0\n",
            "26    2.0\n",
            "27    0.0\n",
            "28    1.0\n",
            "29    2.0\n",
            "30    0.0\n",
            "31    1.0\n",
            "32    2.0\n",
            "33    0.0\n",
            "34    1.0\n",
            "35    2.0\n",
            "36    0.0\n",
            "37    1.0\n",
            "38    2.0\n",
            "Name: rep, dtype: float64\n",
            "\n",
            "bins:\n",
            "0     5.0\n",
            "1     5.0\n",
            "2     5.0\n",
            "3     5.0\n",
            "4     5.0\n",
            "5     5.0\n",
            "6     5.0\n",
            "7     5.0\n",
            "8     5.0\n",
            "9     5.0\n",
            "10    5.0\n",
            "11    5.0\n",
            "12    5.0\n",
            "13    5.0\n",
            "14    5.0\n",
            "15    5.0\n",
            "16    5.0\n",
            "17    5.0\n",
            "18    5.0\n",
            "19    5.0\n",
            "20    5.0\n",
            "21    5.0\n",
            "22    5.0\n",
            "23    5.0\n",
            "24    5.0\n",
            "25    5.0\n",
            "26    5.0\n",
            "27    5.0\n",
            "28    5.0\n",
            "29    5.0\n",
            "30    5.0\n",
            "31    5.0\n",
            "32    5.0\n",
            "33    5.0\n",
            "34    5.0\n",
            "35    5.0\n",
            "36    5.0\n",
            "37    5.0\n",
            "38    5.0\n",
            "Name: bins, dtype: float64\n",
            "\n",
            "lambdas:\n",
            "0               [1, 1, 1, 1, 1]\n",
            "1               [1, 1, 1, 1, 1]\n",
            "2               [1, 1, 1, 1, 1]\n",
            "3              [1, 1, 1, 1, 10]\n",
            "4              [1, 1, 1, 1, 10]\n",
            "5              [1, 1, 1, 1, 10]\n",
            "6             [1, 1, 1, 1, 100]\n",
            "7             [1, 1, 1, 1, 100]\n",
            "8             [1, 1, 1, 1, 100]\n",
            "9              [1, 1, 1, 10, 1]\n",
            "10             [1, 1, 1, 10, 1]\n",
            "11             [1, 1, 1, 10, 1]\n",
            "12            [1, 1, 1, 100, 1]\n",
            "13            [1, 1, 1, 100, 1]\n",
            "14            [1, 1, 1, 100, 1]\n",
            "15             [1, 1, 10, 1, 1]\n",
            "16             [1, 1, 10, 1, 1]\n",
            "17             [1, 1, 10, 1, 1]\n",
            "18            [1, 1, 100, 1, 1]\n",
            "19            [1, 1, 100, 1, 1]\n",
            "20            [1, 1, 100, 1, 1]\n",
            "21             [1, 10, 1, 1, 1]\n",
            "22             [1, 10, 1, 1, 1]\n",
            "23             [1, 10, 1, 1, 1]\n",
            "24            [1, 100, 1, 1, 1]\n",
            "25            [1, 100, 1, 1, 1]\n",
            "26            [1, 100, 1, 1, 1]\n",
            "27             [10, 1, 1, 1, 1]\n",
            "28             [10, 1, 1, 1, 1]\n",
            "29             [10, 1, 1, 1, 1]\n",
            "30            [100, 1, 1, 1, 1]\n",
            "31            [100, 1, 1, 1, 1]\n",
            "32            [100, 1, 1, 1, 1]\n",
            "33         [10, 10, 10, 10, 10]\n",
            "34         [10, 10, 10, 10, 10]\n",
            "35         [10, 10, 10, 10, 10]\n",
            "36    [100, 100, 100, 100, 100]\n",
            "37    [100, 100, 100, 100, 100]\n",
            "38    [100, 100, 100, 100, 100]\n",
            "Name: lambdas, dtype: object\n",
            "\n",
            "final_saved_weights_string:\n",
            "0               rep0_1_1_1_1_1_\n",
            "1               rep1_1_1_1_1_1_\n",
            "2               rep2_1_1_1_1_1_\n",
            "3              rep0_1_1_1_1_10_\n",
            "4              rep1_1_1_1_1_10_\n",
            "5              rep2_1_1_1_1_10_\n",
            "6             rep0_1_1_1_1_100_\n",
            "7             rep1_1_1_1_1_100_\n",
            "8             rep2_1_1_1_1_100_\n",
            "9              rep0_1_1_1_10_1_\n",
            "10             rep1_1_1_1_10_1_\n",
            "11             rep2_1_1_1_10_1_\n",
            "12            rep0_1_1_1_100_1_\n",
            "13            rep1_1_1_1_100_1_\n",
            "14            rep2_1_1_1_100_1_\n",
            "15             rep0_1_1_10_1_1_\n",
            "16             rep1_1_1_10_1_1_\n",
            "17             rep2_1_1_10_1_1_\n",
            "18            rep0_1_1_100_1_1_\n",
            "19            rep1_1_1_100_1_1_\n",
            "20            rep2_1_1_100_1_1_\n",
            "21             rep0_1_10_1_1_1_\n",
            "22             rep1_1_10_1_1_1_\n",
            "23             rep2_1_10_1_1_1_\n",
            "24            rep0_1_100_1_1_1_\n",
            "25            rep1_1_100_1_1_1_\n",
            "26            rep2_1_100_1_1_1_\n",
            "27             rep0_10_1_1_1_1_\n",
            "28             rep1_10_1_1_1_1_\n",
            "29             rep2_10_1_1_1_1_\n",
            "30            rep0_100_1_1_1_1_\n",
            "31            rep1_100_1_1_1_1_\n",
            "32            rep2_100_1_1_1_1_\n",
            "33         rep0_10_10_10_10_10_\n",
            "34         rep1_10_10_10_10_10_\n",
            "35         rep2_10_10_10_10_10_\n",
            "36    rep0_100_100_100_100_100_\n",
            "37    rep1_100_100_100_100_100_\n",
            "38    rep2_100_100_100_100_100_\n",
            "Name: final_saved_weights_string, dtype: object\n",
            "\n",
            "loss:\n",
            "0     0.326160\n",
            "1     0.346245\n",
            "2     0.350723\n",
            "3     0.326160\n",
            "4     0.346245\n",
            "5     0.350723\n",
            "6     0.326160\n",
            "7     0.346116\n",
            "8     0.350723\n",
            "9     0.326160\n",
            "10    0.346116\n",
            "11    0.350761\n",
            "12    0.326160\n",
            "13    0.346245\n",
            "14    0.350761\n",
            "15    0.326160\n",
            "16    0.346116\n",
            "17    0.350713\n",
            "18    0.326160\n",
            "19    0.346246\n",
            "20    0.350684\n",
            "21    0.326160\n",
            "22    0.346116\n",
            "23    0.350761\n",
            "24    0.326160\n",
            "25    0.346116\n",
            "26    0.350723\n",
            "27    0.348716\n",
            "28    0.361459\n",
            "29    0.375850\n",
            "30    0.433981\n",
            "31    0.491292\n",
            "32    0.484441\n",
            "33    0.349029\n",
            "34    0.361459\n",
            "35    0.375850\n",
            "36    0.473451\n",
            "37    0.490969\n",
            "38    0.484441\n",
            "Name: loss, dtype: float64\n",
            "\n",
            "categorical_accuracy:\n",
            "0     0.903317\n",
            "1     0.898600\n",
            "2     0.895783\n",
            "3     0.903317\n",
            "4     0.898600\n",
            "5     0.895783\n",
            "6     0.903317\n",
            "7     0.898783\n",
            "8     0.895783\n",
            "9     0.903317\n",
            "10    0.898783\n",
            "11    0.895883\n",
            "12    0.903317\n",
            "13    0.898600\n",
            "14    0.895883\n",
            "15    0.903317\n",
            "16    0.898783\n",
            "17    0.895867\n",
            "18    0.903317\n",
            "19    0.898583\n",
            "20    0.895933\n",
            "21    0.903317\n",
            "22    0.898783\n",
            "23    0.895883\n",
            "24    0.903317\n",
            "25    0.898783\n",
            "26    0.895783\n",
            "27    0.901850\n",
            "28    0.896783\n",
            "29    0.894450\n",
            "30    0.877417\n",
            "31    0.855667\n",
            "32    0.862617\n",
            "33    0.901683\n",
            "34    0.896783\n",
            "35    0.894450\n",
            "36    0.865100\n",
            "37    0.856950\n",
            "38    0.862617\n",
            "Name: categorical_accuracy, dtype: float64\n",
            "\n",
            "val_loss:\n",
            "0     0.221175\n",
            "1     0.227765\n",
            "2     0.233804\n",
            "3     0.221175\n",
            "4     0.227765\n",
            "5     0.233804\n",
            "6     0.221100\n",
            "7     0.227405\n",
            "8     0.233804\n",
            "9     0.221175\n",
            "10    0.227405\n",
            "11    0.233679\n",
            "12    0.221175\n",
            "13    0.227765\n",
            "14    0.233679\n",
            "15    0.221100\n",
            "16    0.227405\n",
            "17    0.233986\n",
            "18    0.221100\n",
            "19    0.227703\n",
            "20    0.233768\n",
            "21    0.221100\n",
            "22    0.227405\n",
            "23    0.233679\n",
            "24    0.221175\n",
            "25    0.227405\n",
            "26    0.233804\n",
            "27    0.237630\n",
            "28    0.242153\n",
            "29    0.251954\n",
            "30    0.395968\n",
            "31    0.313973\n",
            "32    0.325579\n",
            "33    0.237582\n",
            "34    0.242153\n",
            "35    0.251954\n",
            "36    0.345294\n",
            "37    0.318276\n",
            "38    0.325579\n",
            "Name: val_loss, dtype: float64\n",
            "\n",
            "val_categorical_accuracy:\n",
            "0     0.932533\n",
            "1     0.931467\n",
            "2     0.931067\n",
            "3     0.932533\n",
            "4     0.931467\n",
            "5     0.931067\n",
            "6     0.932533\n",
            "7     0.931733\n",
            "8     0.931067\n",
            "9     0.932533\n",
            "10    0.931733\n",
            "11    0.931600\n",
            "12    0.932533\n",
            "13    0.931467\n",
            "14    0.931600\n",
            "15    0.932533\n",
            "16    0.931733\n",
            "17    0.931200\n",
            "18    0.932533\n",
            "19    0.931467\n",
            "20    0.931600\n",
            "21    0.932533\n",
            "22    0.931733\n",
            "23    0.931600\n",
            "24    0.932533\n",
            "25    0.931733\n",
            "26    0.931067\n",
            "27    0.934533\n",
            "28    0.932800\n",
            "29    0.930933\n",
            "30    0.923867\n",
            "31    0.888933\n",
            "32    0.864000\n",
            "33    0.934667\n",
            "34    0.932800\n",
            "35    0.930933\n",
            "36    0.918800\n",
            "37    0.916267\n",
            "38    0.864000\n",
            "Name: val_categorical_accuracy, dtype: float64\n",
            "\n",
            "9T_4P:\n",
            "0     34\n",
            "1     37\n",
            "2     31\n",
            "3     34\n",
            "4     37\n",
            "5     31\n",
            "6     34\n",
            "7     37\n",
            "8     31\n",
            "9     34\n",
            "10    37\n",
            "11    31\n",
            "12    34\n",
            "13    37\n",
            "14    31\n",
            "15    34\n",
            "16    37\n",
            "17    31\n",
            "18    34\n",
            "19    37\n",
            "20    31\n",
            "21    34\n",
            "22    37\n",
            "23    31\n",
            "24    34\n",
            "25    37\n",
            "26    31\n",
            "27     8\n",
            "28    11\n",
            "29    10\n",
            "30     9\n",
            "31     0\n",
            "32     0\n",
            "33     8\n",
            "34    11\n",
            "35    10\n",
            "36     5\n",
            "37     2\n",
            "38     0\n",
            "Name: 9T_4P, dtype: int64\n",
            "\n",
            "4T_9P:\n",
            "0      16\n",
            "1      21\n",
            "2      19\n",
            "3      16\n",
            "4      21\n",
            "5      19\n",
            "6      16\n",
            "7      21\n",
            "8      19\n",
            "9      16\n",
            "10     21\n",
            "11     19\n",
            "12     16\n",
            "13     21\n",
            "14     19\n",
            "15     16\n",
            "16     21\n",
            "17     19\n",
            "18     16\n",
            "19     21\n",
            "20     19\n",
            "21     16\n",
            "22     21\n",
            "23     19\n",
            "24     16\n",
            "25     21\n",
            "26     19\n",
            "27     38\n",
            "28     48\n",
            "29     39\n",
            "30     84\n",
            "31    361\n",
            "32    546\n",
            "33     38\n",
            "34     48\n",
            "35     39\n",
            "36    134\n",
            "37    144\n",
            "38    546\n",
            "Name: 4T_9P, dtype: int64\n",
            "\n",
            "0T_Acc:\n",
            "0     0.979138\n",
            "1     0.979138\n",
            "2     0.977747\n",
            "3     0.979138\n",
            "4     0.979138\n",
            "5     0.977747\n",
            "6     0.979138\n",
            "7     0.979138\n",
            "8     0.977747\n",
            "9     0.979138\n",
            "10    0.979138\n",
            "11    0.979138\n",
            "12    0.979138\n",
            "13    0.979138\n",
            "14    0.979138\n",
            "15    0.979138\n",
            "16    0.979138\n",
            "17    0.976356\n",
            "18    0.979138\n",
            "19    0.979138\n",
            "20    0.976356\n",
            "21    0.979138\n",
            "22    0.979138\n",
            "23    0.979138\n",
            "24    0.979138\n",
            "25    0.979138\n",
            "26    0.977747\n",
            "27    0.980529\n",
            "28    0.972184\n",
            "29    0.979138\n",
            "30    0.972184\n",
            "31    0.977747\n",
            "32    0.981919\n",
            "33    0.979138\n",
            "34    0.972184\n",
            "35    0.979138\n",
            "36    0.973574\n",
            "37    0.977747\n",
            "38    0.981919\n",
            "Name: 0T_Acc, dtype: float64\n",
            "\n",
            "1T_Acc:\n",
            "0     0.979976\n",
            "1     0.979976\n",
            "2     0.979976\n",
            "3     0.979976\n",
            "4     0.979976\n",
            "5     0.979976\n",
            "6     0.979976\n",
            "7     0.982332\n",
            "8     0.979976\n",
            "9     0.979976\n",
            "10    0.982332\n",
            "11    0.979976\n",
            "12    0.979976\n",
            "13    0.979976\n",
            "14    0.979976\n",
            "15    0.979976\n",
            "16    0.982332\n",
            "17    0.979976\n",
            "18    0.979976\n",
            "19    0.979976\n",
            "20    0.979976\n",
            "21    0.979976\n",
            "22    0.982332\n",
            "23    0.979976\n",
            "24    0.979976\n",
            "25    0.982332\n",
            "26    0.979976\n",
            "27    0.981154\n",
            "28    0.981154\n",
            "29    0.981154\n",
            "30    0.978799\n",
            "31    0.979976\n",
            "32    0.979976\n",
            "33    0.981154\n",
            "34    0.981154\n",
            "35    0.981154\n",
            "36    0.979976\n",
            "37    0.981154\n",
            "38    0.979976\n",
            "Name: 1T_Acc, dtype: float64\n",
            "\n",
            "2T_Acc:\n",
            "0     0.934949\n",
            "1     0.934949\n",
            "2     0.940051\n",
            "3     0.934949\n",
            "4     0.934949\n",
            "5     0.940051\n",
            "6     0.934949\n",
            "7     0.933673\n",
            "8     0.940051\n",
            "9     0.934949\n",
            "10    0.933673\n",
            "11    0.940051\n",
            "12    0.934949\n",
            "13    0.934949\n",
            "14    0.940051\n",
            "15    0.934949\n",
            "16    0.933673\n",
            "17    0.941327\n",
            "18    0.934949\n",
            "19    0.934949\n",
            "20    0.941327\n",
            "21    0.934949\n",
            "22    0.933673\n",
            "23    0.940051\n",
            "24    0.934949\n",
            "25    0.933673\n",
            "26    0.940051\n",
            "27    0.931122\n",
            "28    0.938776\n",
            "29    0.933673\n",
            "30    0.919643\n",
            "31    0.931122\n",
            "32    0.932398\n",
            "33    0.932398\n",
            "34    0.938776\n",
            "35    0.933673\n",
            "36    0.924745\n",
            "37    0.932398\n",
            "38    0.932398\n",
            "Name: 2T_Acc, dtype: float64\n",
            "\n",
            "3T_Acc:\n",
            "0     0.931126\n",
            "1     0.928477\n",
            "2     0.903311\n",
            "3     0.931126\n",
            "4     0.928477\n",
            "5     0.903311\n",
            "6     0.931126\n",
            "7     0.925828\n",
            "8     0.903311\n",
            "9     0.931126\n",
            "10    0.925828\n",
            "11    0.904636\n",
            "12    0.931126\n",
            "13    0.928477\n",
            "14    0.904636\n",
            "15    0.931126\n",
            "16    0.925828\n",
            "17    0.903311\n",
            "18    0.931126\n",
            "19    0.928477\n",
            "20    0.905960\n",
            "21    0.931126\n",
            "22    0.925828\n",
            "23    0.904636\n",
            "24    0.931126\n",
            "25    0.925828\n",
            "26    0.903311\n",
            "27    0.931126\n",
            "28    0.927152\n",
            "29    0.907285\n",
            "30    0.927152\n",
            "31    0.921854\n",
            "32    0.907285\n",
            "33    0.931126\n",
            "34    0.927152\n",
            "35    0.907285\n",
            "36    0.912583\n",
            "37    0.927152\n",
            "38    0.907285\n",
            "Name: 3T_Acc, dtype: float64\n",
            "\n",
            "4T_Acc:\n",
            "0     0.943925\n",
            "1     0.938585\n",
            "2     0.942590\n",
            "3     0.943925\n",
            "4     0.938585\n",
            "5     0.942590\n",
            "6     0.943925\n",
            "7     0.938585\n",
            "8     0.942590\n",
            "9     0.943925\n",
            "10    0.938585\n",
            "11    0.942590\n",
            "12    0.943925\n",
            "13    0.938585\n",
            "14    0.942590\n",
            "15    0.943925\n",
            "16    0.938585\n",
            "17    0.942590\n",
            "18    0.943925\n",
            "19    0.938585\n",
            "20    0.942590\n",
            "21    0.943925\n",
            "22    0.938585\n",
            "23    0.942590\n",
            "24    0.943925\n",
            "25    0.938585\n",
            "26    0.942590\n",
            "27    0.915888\n",
            "28    0.903872\n",
            "29    0.914553\n",
            "30    0.859813\n",
            "31    0.477971\n",
            "32    0.225634\n",
            "33    0.915888\n",
            "34    0.903872\n",
            "35    0.914553\n",
            "36    0.791722\n",
            "37    0.773031\n",
            "38    0.225634\n",
            "Name: 4T_Acc, dtype: float64\n",
            "\n",
            "5T_Acc:\n",
            "0     0.902367\n",
            "1     0.893491\n",
            "2     0.893491\n",
            "3     0.902367\n",
            "4     0.893491\n",
            "5     0.893491\n",
            "6     0.902367\n",
            "7     0.893491\n",
            "8     0.893491\n",
            "9     0.902367\n",
            "10    0.893491\n",
            "11    0.893491\n",
            "12    0.902367\n",
            "13    0.893491\n",
            "14    0.893491\n",
            "15    0.902367\n",
            "16    0.893491\n",
            "17    0.894970\n",
            "18    0.902367\n",
            "19    0.893491\n",
            "20    0.893491\n",
            "21    0.902367\n",
            "22    0.893491\n",
            "23    0.893491\n",
            "24    0.902367\n",
            "25    0.893491\n",
            "26    0.893491\n",
            "27    0.900888\n",
            "28    0.893491\n",
            "29    0.889053\n",
            "30    0.894970\n",
            "31    0.881657\n",
            "32    0.887574\n",
            "33    0.900888\n",
            "34    0.893491\n",
            "35    0.889053\n",
            "36    0.892012\n",
            "37    0.877219\n",
            "38    0.887574\n",
            "Name: 5T_Acc, dtype: float64\n",
            "\n",
            "6T_Acc:\n",
            "0     0.953258\n",
            "1     0.950425\n",
            "2     0.956091\n",
            "3     0.953258\n",
            "4     0.950425\n",
            "5     0.956091\n",
            "6     0.953258\n",
            "7     0.950425\n",
            "8     0.956091\n",
            "9     0.953258\n",
            "10    0.950425\n",
            "11    0.956091\n",
            "12    0.953258\n",
            "13    0.950425\n",
            "14    0.956091\n",
            "15    0.953258\n",
            "16    0.950425\n",
            "17    0.954674\n",
            "18    0.953258\n",
            "19    0.950425\n",
            "20    0.956091\n",
            "21    0.953258\n",
            "22    0.950425\n",
            "23    0.956091\n",
            "24    0.953258\n",
            "25    0.950425\n",
            "26    0.956091\n",
            "27    0.954674\n",
            "28    0.951841\n",
            "29    0.954674\n",
            "30    0.946176\n",
            "31    0.944759\n",
            "32    0.953258\n",
            "33    0.953258\n",
            "34    0.951841\n",
            "35    0.954674\n",
            "36    0.956091\n",
            "37    0.943343\n",
            "38    0.953258\n",
            "Name: 6T_Acc, dtype: float64\n",
            "\n",
            "7T_Acc:\n",
            "0     0.895013\n",
            "1     0.904199\n",
            "2     0.906824\n",
            "3     0.895013\n",
            "4     0.904199\n",
            "5     0.906824\n",
            "6     0.895013\n",
            "7     0.904199\n",
            "8     0.906824\n",
            "9     0.895013\n",
            "10    0.904199\n",
            "11    0.906824\n",
            "12    0.895013\n",
            "13    0.904199\n",
            "14    0.906824\n",
            "15    0.895013\n",
            "16    0.904199\n",
            "17    0.905512\n",
            "18    0.895013\n",
            "19    0.904199\n",
            "20    0.906824\n",
            "21    0.895013\n",
            "22    0.904199\n",
            "23    0.906824\n",
            "24    0.895013\n",
            "25    0.904199\n",
            "26    0.906824\n",
            "27    0.908136\n",
            "28    0.904199\n",
            "29    0.906824\n",
            "30    0.900262\n",
            "31    0.902887\n",
            "32    0.909449\n",
            "33    0.908136\n",
            "34    0.904199\n",
            "35    0.906824\n",
            "36    0.912073\n",
            "37    0.896325\n",
            "38    0.909449\n",
            "Name: 7T_Acc, dtype: float64\n",
            "\n",
            "8T_Acc:\n",
            "0     0.912449\n",
            "1     0.911081\n",
            "2     0.908345\n",
            "3     0.912449\n",
            "4     0.911081\n",
            "5     0.908345\n",
            "6     0.912449\n",
            "7     0.915185\n",
            "8     0.908345\n",
            "9     0.912449\n",
            "10    0.915185\n",
            "11    0.909713\n",
            "12    0.912449\n",
            "13    0.911081\n",
            "14    0.909713\n",
            "15    0.912449\n",
            "16    0.915185\n",
            "17    0.909713\n",
            "18    0.912449\n",
            "19    0.911081\n",
            "20    0.909713\n",
            "21    0.912449\n",
            "22    0.915185\n",
            "23    0.909713\n",
            "24    0.912449\n",
            "25    0.915185\n",
            "26    0.908345\n",
            "27    0.915185\n",
            "28    0.926129\n",
            "29    0.904241\n",
            "30    0.908345\n",
            "31    0.927497\n",
            "32    0.908345\n",
            "33    0.917921\n",
            "34    0.926129\n",
            "35    0.904241\n",
            "36    0.912449\n",
            "37    0.906977\n",
            "38    0.908345\n",
            "Name: 8T_Acc, dtype: float64\n",
            "\n",
            "9T_Acc:\n",
            "0     0.888166\n",
            "1     0.888166\n",
            "2     0.895969\n",
            "3     0.888166\n",
            "4     0.888166\n",
            "5     0.895969\n",
            "6     0.888166\n",
            "7     0.888166\n",
            "8     0.895969\n",
            "9     0.888166\n",
            "10    0.888166\n",
            "11    0.897269\n",
            "12    0.888166\n",
            "13    0.888166\n",
            "14    0.897269\n",
            "15    0.888166\n",
            "16    0.888166\n",
            "17    0.897269\n",
            "18    0.888166\n",
            "19    0.888166\n",
            "20    0.897269\n",
            "21    0.888166\n",
            "22    0.888166\n",
            "23    0.897269\n",
            "24    0.888166\n",
            "25    0.888166\n",
            "26    0.895969\n",
            "27    0.920676\n",
            "28    0.921977\n",
            "29    0.931079\n",
            "30    0.924577\n",
            "31    0.934980\n",
            "32    0.945384\n",
            "33    0.920676\n",
            "34    0.921977\n",
            "35    0.931079\n",
            "36    0.925878\n",
            "37    0.937581\n",
            "38    0.945384\n",
            "Name: 9T_Acc, dtype: float64\n",
            "\n",
            "cm_per_epoch:\n",
            "0     [[704, 0, 1, 0, 0, 2, 9, 1, 2, 0, 0, 832, 3, 3...\n",
            "1     [[704, 0, 0, 0, 0, 2, 9, 1, 3, 0, 0, 832, 2, 4...\n",
            "2     [[703, 0, 2, 0, 1, 3, 5, 2, 3, 0, 0, 832, 2, 3...\n",
            "3     [[704, 0, 1, 0, 0, 2, 9, 1, 2, 0, 0, 832, 3, 3...\n",
            "4     [[704, 0, 0, 0, 0, 2, 9, 1, 3, 0, 0, 832, 2, 4...\n",
            "5     [[703, 0, 2, 0, 1, 3, 5, 2, 3, 0, 0, 832, 2, 3...\n",
            "6     [[704, 0, 1, 0, 0, 2, 9, 1, 2, 0, 0, 832, 3, 3...\n",
            "7     [[704, 0, 0, 0, 0, 2, 9, 1, 3, 0, 0, 834, 2, 3...\n",
            "8     [[703, 0, 2, 0, 1, 3, 5, 2, 3, 0, 0, 832, 2, 3...\n",
            "9     [[704, 0, 1, 0, 0, 2, 9, 1, 2, 0, 0, 832, 3, 3...\n",
            "10    [[704, 0, 0, 0, 0, 2, 9, 1, 3, 0, 0, 834, 2, 3...\n",
            "11    [[704, 0, 2, 0, 1, 3, 5, 1, 3, 0, 0, 832, 2, 3...\n",
            "12    [[704, 0, 1, 0, 0, 2, 9, 1, 2, 0, 0, 832, 3, 3...\n",
            "13    [[704, 0, 0, 0, 0, 2, 9, 1, 3, 0, 0, 832, 2, 4...\n",
            "14    [[704, 0, 2, 0, 1, 3, 5, 1, 3, 0, 0, 832, 2, 3...\n",
            "15    [[704, 0, 1, 0, 0, 2, 9, 1, 2, 0, 0, 832, 3, 3...\n",
            "16    [[704, 0, 0, 0, 0, 2, 9, 1, 3, 0, 0, 834, 2, 3...\n",
            "17    [[702, 0, 2, 0, 1, 3, 5, 2, 3, 1, 0, 832, 2, 3...\n",
            "18    [[704, 0, 1, 0, 0, 2, 9, 1, 2, 0, 0, 832, 3, 3...\n",
            "19    [[704, 0, 0, 0, 0, 2, 9, 1, 3, 0, 0, 832, 2, 4...\n",
            "20    [[702, 0, 2, 0, 1, 3, 5, 2, 3, 1, 0, 832, 3, 3...\n",
            "21    [[704, 0, 1, 0, 0, 2, 9, 1, 2, 0, 0, 832, 3, 3...\n",
            "22    [[704, 0, 0, 0, 0, 2, 9, 1, 3, 0, 0, 834, 2, 3...\n",
            "23    [[704, 0, 2, 0, 1, 3, 5, 1, 3, 0, 0, 832, 2, 3...\n",
            "24    [[704, 0, 1, 0, 0, 2, 9, 1, 2, 0, 0, 832, 3, 3...\n",
            "25    [[704, 0, 0, 0, 0, 2, 9, 1, 3, 0, 0, 834, 2, 3...\n",
            "26    [[703, 0, 2, 0, 1, 3, 5, 2, 3, 0, 0, 832, 2, 3...\n",
            "27    [[705, 0, 1, 0, 0, 2, 8, 1, 2, 0, 0, 833, 3, 3...\n",
            "28    [[699, 0, 0, 0, 0, 4, 11, 2, 3, 0, 0, 833, 3, ...\n",
            "29    [[704, 0, 2, 0, 1, 3, 5, 1, 2, 1, 0, 833, 4, 1...\n",
            "30    [[699, 0, 3, 0, 1, 3, 10, 1, 2, 0, 0, 831, 4, ...\n",
            "31    [[703, 0, 1, 0, 0, 4, 7, 1, 3, 0, 0, 832, 4, 2...\n",
            "32    [[706, 0, 2, 0, 0, 2, 5, 1, 3, 0, 0, 832, 2, 3...\n",
            "33    [[704, 0, 1, 0, 0, 3, 8, 1, 2, 0, 0, 833, 3, 3...\n",
            "34    [[699, 0, 0, 0, 0, 4, 11, 2, 3, 0, 0, 833, 3, ...\n",
            "35    [[704, 0, 2, 0, 1, 3, 5, 1, 2, 1, 0, 833, 4, 1...\n",
            "36    [[700, 0, 1, 0, 1, 2, 11, 1, 2, 1, 0, 832, 3, ...\n",
            "37    [[703, 0, 1, 0, 1, 3, 8, 1, 2, 0, 0, 833, 3, 4...\n",
            "38    [[706, 0, 2, 0, 0, 2, 5, 1, 3, 0, 0, 832, 2, 3...\n",
            "Name: cm_per_epoch, dtype: object\n",
            "\n",
            "f1_micro:\n",
            "0     0.932533\n",
            "1     0.931467\n",
            "2     0.931067\n",
            "3     0.932533\n",
            "4     0.931467\n",
            "5     0.931067\n",
            "6     0.932533\n",
            "7     0.931733\n",
            "8     0.931067\n",
            "9     0.932533\n",
            "10    0.931733\n",
            "11    0.931600\n",
            "12    0.932533\n",
            "13    0.931467\n",
            "14    0.931600\n",
            "15    0.932533\n",
            "16    0.931733\n",
            "17    0.931200\n",
            "18    0.932533\n",
            "19    0.931467\n",
            "20    0.931600\n",
            "21    0.932533\n",
            "22    0.931733\n",
            "23    0.931600\n",
            "24    0.932533\n",
            "25    0.931733\n",
            "26    0.931067\n",
            "27    0.934533\n",
            "28    0.932800\n",
            "29    0.930933\n",
            "30    0.923867\n",
            "31    0.888933\n",
            "32    0.864000\n",
            "33    0.934667\n",
            "34    0.932800\n",
            "35    0.930933\n",
            "36    0.918800\n",
            "37    0.916267\n",
            "38    0.864000\n",
            "Name: f1_micro, dtype: float64\n",
            "\n",
            "f1_macro:\n",
            "0     0.931973\n",
            "1     0.930788\n",
            "2     0.930185\n",
            "3     0.931973\n",
            "4     0.930788\n",
            "5     0.930185\n",
            "6     0.931973\n",
            "7     0.931046\n",
            "8     0.930185\n",
            "9     0.931973\n",
            "10    0.931046\n",
            "11    0.930750\n",
            "12    0.931973\n",
            "13    0.930788\n",
            "14    0.930750\n",
            "15    0.931973\n",
            "16    0.931046\n",
            "17    0.930346\n",
            "18    0.931973\n",
            "19    0.930783\n",
            "20    0.930755\n",
            "21    0.931973\n",
            "22    0.931046\n",
            "23    0.930750\n",
            "24    0.931973\n",
            "25    0.931046\n",
            "26    0.930185\n",
            "27    0.934061\n",
            "28    0.932164\n",
            "29    0.930091\n",
            "30    0.923471\n",
            "31    0.885313\n",
            "32    0.851197\n",
            "33    0.934168\n",
            "34    0.932164\n",
            "35    0.930091\n",
            "36    0.918354\n",
            "37    0.915489\n",
            "38    0.851197\n",
            "Name: f1_macro, dtype: float64\n",
            "\n",
            "f1_weighted:\n",
            "0     0.932334\n",
            "1     0.931288\n",
            "2     0.930917\n",
            "3     0.932334\n",
            "4     0.931288\n",
            "5     0.930917\n",
            "6     0.932334\n",
            "7     0.931547\n",
            "8     0.930917\n",
            "9     0.932334\n",
            "10    0.931547\n",
            "11    0.931446\n",
            "12    0.932334\n",
            "13    0.931288\n",
            "14    0.931446\n",
            "15    0.932334\n",
            "16    0.931547\n",
            "17    0.931058\n",
            "18    0.932334\n",
            "19    0.931287\n",
            "20    0.931450\n",
            "21    0.932334\n",
            "22    0.931547\n",
            "23    0.931446\n",
            "24    0.932334\n",
            "25    0.931547\n",
            "26    0.930917\n",
            "27    0.934406\n",
            "28    0.932691\n",
            "29    0.930818\n",
            "30    0.923846\n",
            "31    0.885601\n",
            "32    0.851468\n",
            "33    0.934543\n",
            "34    0.932691\n",
            "35    0.930818\n",
            "36    0.918714\n",
            "37    0.916073\n",
            "38    0.851468\n",
            "Name: f1_weighted, dtype: float64\n",
            "\n",
            "f1_notweighted:\n",
            "0     [0.9610921501706483, 0.9702623906705539, 0.925...\n",
            "1     [0.9604365620736698, 0.970828471411902, 0.9313...\n",
            "2     [0.963013698630137, 0.9742388758782202, 0.9311...\n",
            "3     [0.9610921501706483, 0.9702623906705539, 0.925...\n",
            "4     [0.9604365620736698, 0.970828471411902, 0.9313...\n",
            "5     [0.963013698630137, 0.9742388758782202, 0.9311...\n",
            "6     [0.9610921501706483, 0.9702623906705539, 0.925...\n",
            "7     [0.9610921501706483, 0.9714618520675595, 0.930...\n",
            "8     [0.963013698630137, 0.9742388758782202, 0.9311...\n",
            "9     [0.9610921501706483, 0.9702623906705539, 0.925...\n",
            "10    [0.9610921501706483, 0.9714618520675595, 0.930...\n",
            "11    [0.9630642954856361, 0.97366881217086, 0.93055...\n",
            "12    [0.9610921501706483, 0.9702623906705539, 0.925...\n",
            "13    [0.9604365620736698, 0.970828471411902, 0.9313...\n",
            "14    [0.9630642954856361, 0.97366881217086, 0.93055...\n",
            "15    [0.9610921501706483, 0.9702623906705539, 0.925...\n",
            "16    [0.9610921501706483, 0.9714618520675595, 0.930...\n",
            "17    [0.9623029472241261, 0.97366881217086, 0.93123...\n",
            "18    [0.9610921501706483, 0.9702623906705539, 0.925...\n",
            "19    [0.9604365620736698, 0.970828471411902, 0.9313...\n",
            "20    [0.9623029472241261, 0.97366881217086, 0.93064...\n",
            "21    [0.9610921501706483, 0.9702623906705539, 0.925...\n",
            "22    [0.9610921501706483, 0.9714618520675595, 0.930...\n",
            "23    [0.9630642954856361, 0.97366881217086, 0.93055...\n",
            "24    [0.9610921501706483, 0.9702623906705539, 0.925...\n",
            "25    [0.9610921501706483, 0.9714618520675595, 0.930...\n",
            "26    [0.963013698630137, 0.9742388758782202, 0.9311...\n",
            "27    [0.9657534246575341, 0.9702970297029703, 0.924...\n",
            "28    [0.9628099173553719, 0.9731308411214954, 0.931...\n",
            "29    [0.9617486338797815, 0.9742690058479532, 0.927...\n",
            "30    [0.9614855570839064, 0.9674039580908033, 0.921...\n",
            "31    [0.9623545516769336, 0.970828471411902, 0.9275...\n",
            "32    [0.9625085207907293, 0.9753810082063307, 0.928...\n",
            "33    [0.9650445510623716, 0.9702970297029703, 0.925...\n",
            "34    [0.9628099173553719, 0.9731308411214954, 0.931...\n",
            "35    [0.9617486338797815, 0.9742690058479532, 0.927...\n",
            "36    [0.9628610729023384, 0.9696969696969696, 0.921...\n",
            "37    [0.963013698630137, 0.9719953325554259, 0.9294...\n",
            "38    [0.9625085207907293, 0.9753810082063307, 0.928...\n",
            "Name: f1_notweighted, dtype: object\n",
            "\n",
            "datetime:\n",
            "0     2023-05-04 22:31:34\n",
            "1     2023-05-04 22:34:24\n",
            "2     2023-05-04 22:37:12\n",
            "3     2023-05-04 22:40:01\n",
            "4     2023-05-04 22:42:49\n",
            "5     2023-05-04 22:45:38\n",
            "6     2023-05-04 22:48:27\n",
            "7     2023-05-04 22:51:23\n",
            "8     2023-05-04 22:54:12\n",
            "9     2023-05-04 22:57:01\n",
            "10    2023-05-04 22:59:49\n",
            "11    2023-05-04 23:02:40\n",
            "12    2023-05-04 23:05:31\n",
            "13    2023-05-04 23:08:22\n",
            "14    2023-05-04 23:11:10\n",
            "15    2023-05-04 23:13:56\n",
            "16    2023-05-04 23:16:45\n",
            "17    2023-05-04 23:19:31\n",
            "18    2023-05-04 23:22:20\n",
            "19    2023-05-04 23:25:06\n",
            "20    2023-05-04 23:27:56\n",
            "21    2023-05-04 23:30:44\n",
            "22    2023-05-04 23:33:31\n",
            "23    2023-05-04 23:36:21\n",
            "24    2023-05-04 23:39:09\n",
            "25    2023-05-04 23:42:00\n",
            "26    2023-05-04 23:44:50\n",
            "27    2023-05-04 23:47:38\n",
            "28    2023-05-04 23:50:27\n",
            "29    2023-05-04 23:53:15\n",
            "30    2023-05-04 23:56:04\n",
            "31    2023-05-04 23:58:52\n",
            "32    2023-05-05 00:01:41\n",
            "33    2023-05-09 17:28:41\n",
            "34    2023-05-09 17:33:00\n",
            "35    2023-05-09 17:38:41\n",
            "36    2023-05-09 17:43:49\n",
            "37    2023-05-09 17:49:47\n",
            "38    2023-05-09 17:55:59\n",
            "Name: datetime, dtype: object\n",
            "\n"
          ]
        }
      ]
    }
  ]
}